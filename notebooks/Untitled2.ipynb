{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x = [-.5,.5,.5,.5,.5]\r\n",
    "y = tf.nn.relu(x)\r\n",
    "print(y)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0.  0.5 0.5 0.5 0.5], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(tf.reduce_mean(y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0.4, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# This function to generate evidence is used for the first example\r\n",
    "def relu_evidence(logits):\r\n",
    "    return tf.nn.relu(logits)\r\n",
    "\r\n",
    "# This one usually works better and used for the second and third examples\r\n",
    "# For general settings and different datasets, you may try this one first\r\n",
    "def exp_evidence(logits): \r\n",
    "    return tf.exp(tf.clip_by_value(logits,-10,10))\r\n",
    "\r\n",
    "# This one is another alternative and \r\n",
    "# usually behaves better than the relu_evidence \r\n",
    "def softplus_evidence(logits):\r\n",
    "    return tf.nn.softplus(logits)\r\n",
    "K=10\r\n",
    "def KL(alpha):\r\n",
    "    # print(\"K:\",K)\r\n",
    "    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\r\n",
    "    S_alpha = tf.cast(tf.reduce_sum(alpha,axis=1,keepdims=True),tf.float32)\r\n",
    "    \r\n",
    "    print(\"lgama\",tf.compat.v1.lgamma(S_alpha) )\r\n",
    "    S_beta = tf.reduce_sum(beta,axis=1,keepdims=True)\r\n",
    "    lnB = tf.compat.v1.lgamma(S_alpha) - tf.reduce_sum(tf.compat.v1.lgamma(alpha),axis=1,keepdims=True)\r\n",
    "    lnB_uni = tf.reduce_sum(tf.compat.v1.lgamma(beta),axis=1,keepdims=True) - tf.compat.v1.lgamma(S_beta)\r\n",
    "    \r\n",
    "    dg0 = tf.compat.v1.digamma(S_alpha)\r\n",
    "    dg1 = tf.compat.v1.digamma(alpha)\r\n",
    "    \r\n",
    "    kl = tf.reduce_sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\r\n",
    "    # print(\"kl\", kl)\r\n",
    "    return kl\r\n",
    "def mse_loss_global(labels, outputs): \r\n",
    "#         labels = tf.one_hot(tf.cast(labels, tf.int32), 10)\r\n",
    "# #         print(\"onehot\",labels)\r\n",
    "#         labels = tf.cast(labels, dtype=tf.float32)\r\n",
    "#         try:\r\n",
    "#             labels= tf.squeeze(labels,[1])\r\n",
    "#         except:\r\n",
    "#                 print(\"loss labels can't be squeezed\")\r\n",
    "        print(\"global Loss\")\r\n",
    "        evidence = softplus_evidence(outputs)\r\n",
    "        print(\"evid_relu\",evidence)\r\n",
    "        alpha = evidence + 1\r\n",
    "        alpha = tf.cast(alpha, tf.float32)\r\n",
    "        print(\"alpha\",alpha)\r\n",
    "        S = tf.reduce_sum(alpha, axis=1, keepdims=True) \r\n",
    "        E = alpha - 1\r\n",
    "        m = alpha / S\r\n",
    "#         print(\"m\",m)\r\n",
    "#         print(\"global\", labels)\r\n",
    "        A = tf.reduce_sum((labels-m)**2, axis=1, keepdims=True) \r\n",
    "        B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True) \r\n",
    "\r\n",
    "        annealing_coef = tf.minimum(1.0,tf.cast(1,tf.float32))\r\n",
    "#         annealing_coef = 1\r\n",
    "        alp = E*(1-labels) + 1 \r\n",
    "        print(\"alp\", alp)\r\n",
    "        C =  annealing_coef * KL(alp)\r\n",
    "#         print(alpha)\r\n",
    "        # C = keras_kl(labels, alpha)\r\n",
    "        return (A + B) + C\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "print(tf.nn.softplus(0.408539593))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0.9181366, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import numpy as np\r\n",
    "labels =  np.array([[0., 0., 0., 0., 0., 0., 0. ,0. ,1., 0.]])\r\n",
    "# outputs = np.array([[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]])\r\n",
    "outputs = np.array([[3,3,4,0,1,3,3,3,3,3]])\r\n",
    "\r\n",
    "\r\n",
    "# # outputs = np.array([[-6,-3,3,-4,-6,-3,-3,-3,-3,-3]])\r\n",
    "# print(\"mse\", mse_loss_global(labels,outputs))\r\n",
    "# print(\"evidence\",tf.reduce_sum(relu_evidence(outputs)))\r\n",
    "\r\n",
    "# print(\"keras_mse\",tf.keras.losses.mean_squared_error(labels,relu_evidence(outputs)))\r\n",
    "# print(\"keras_mae\",tf.keras.losses.mean_absolute_error(labels,(outputs)))\r\n",
    "# print(\"keras_crossE\",tf.keras.losses.categorical_crossentropy(labels,tf.cast(outputs,tf.float32),True))\r\n",
    "# print(\"softmax\", tf.nn.softmax(tf.cast(outputs,tf.float32)),\"label:\",tf.argmax(outputs,1))\r\n",
    "\r\n",
    "\r\n",
    "outputs = np.array([[-0.867575228, 0.408539593, -5.3238, -4.99858856, -4.85228109, -6.52761173, -6.87370205, -6.45510149, 0.890240133 ,-2.260957]])\r\n",
    "# outputs = np.array([[1.2,0,1.1,1,0,0,0,0,0,0]])\r\n",
    "outputs = tf.cast(outputs,tf.float32)\r\n",
    "print(\"mse\", mse_loss_global(labels,outputs))\r\n",
    "print(\"KL\",KL(softplus_evidence(outputs))+1)\r\n",
    "print(\"KL2\",KL((outputs))*.1)\r\n",
    "print(\"keras KL\",tf.keras.losses.kl_divergence(labels,outputs))\r\n",
    "\r\n",
    "\r\n",
    "print(\"evidence\",tf.reduce_sum(softplus_evidence(outputs)))\r\n",
    "\r\n",
    "print(\"keras_mse\",tf.keras.losses.mean_squared_error(labels,softplus_evidence(outputs)+1))\r\n",
    "# print(\"keras_mae\",tf.keras.losses.mean_absolute_error(labels,(outputs)))\r\n",
    "print(\"keras_crossE\",tf.keras.losses.categorical_crossentropy(labels,tf.cast(outputs,tf.float32),True))\r\n",
    "print(\"softmax\", tf.nn.softmax(tf.cast(outputs,tf.float32)), \"label:\",tf.argmax(outputs,1))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "global Loss\n",
      "evid_relu tf.Tensor(\n",
      "[[3.5063478e-01 9.1813660e-01 4.8623555e-03 6.7248014e-03 7.7801961e-03\n",
      "  1.4614260e-03 1.0341047e-03 1.5712447e-03 1.2342249e+00 9.9166982e-02]], shape=(1, 10), dtype=float32)\n",
      "alpha tf.Tensor(\n",
      "[[1.3506348 1.9181366 1.0048623 1.0067248 1.0077802 1.0014614 1.0010341\n",
      "  1.0015713 2.2342248 1.099167 ]], shape=(1, 10), dtype=float32)\n",
      "alp tf.Tensor(\n",
      "[[1.3506348 1.9181366 1.0048623 1.0067248 1.0077802 1.0014614 1.0010341\n",
      "  1.0015713 1.        1.099167 ]], shape=(1, 10), dtype=float32)\n",
      "lgama tf.Tensor([[16.032024]], shape=(1, 1), dtype=float32)\n",
      "mse tf.Tensor([[1.1837213]], shape=(1, 1), dtype=float32)\n",
      "lgama tf.Tensor([[0.37678945]], shape=(1, 1), dtype=float32)\n",
      "KL tf.Tensor([[2735.4424]], shape=(1, 1), dtype=float32)\n",
      "lgama tf.Tensor([[-96.82233]], shape=(1, 1), dtype=float32)\n",
      "KL2 tf.Tensor([[402.2756]], shape=(1, 1), dtype=float32)\n",
      "keras KL tf.Tensor([0.11626254], shape=(1,), dtype=float32)\n",
      "evidence tf.Tensor(2.6255975, shape=(), dtype=float32)\n",
      "keras_mse tf.Tensor([1.4281945], shape=(1,), dtype=float32)\n",
      "keras_crossE tf.Tensor([0.61117923], shape=(1,), dtype=float32)\n",
      "softmax tf.Tensor(\n",
      "[[9.3574755e-02 3.3524966e-01 1.0860376e-03 1.5034273e-03 1.7402952e-03\n",
      "  3.2586377e-04 2.3053186e-04 3.5036998e-04 5.4271048e-01 2.3228474e-02]], shape=(1, 10), dtype=float32) label: tf.Tensor([8], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "outputs = np.array([[-6,-3,3,-4,-6,-3,-3,-3,-3,-3]])\r\n",
    "outputs = np.array([[0,0,0,0,0,0,0,0,0,0]])\r\n",
    "outputs = tf.cast(outputs,tf.float32)\r\n",
    "print(\"evidence\",tf.reduce_sum(softplus_evidence(outputs)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "evidence tf.Tensor(6.931472, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import numpy as np\r\n",
    "x = np.array([.01,1,6])\r\n",
    "y = np.array([0,0,1])\r\n",
    "print(tf.keras.losses.mean_squared_error(y,x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(8.6667, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "out = y - x\r\n",
    "print(out)\r\n",
    "out = tf.reduce_mean(out*out)\r\n",
    "out"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.01 -1.   -5.  ]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=8.6667>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('py36': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "interpreter": {
   "hash": "7483e188a462fd4248cd8d23b24bc727a5fe7a35e4044aa57ebcc92f8fe9e445"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}