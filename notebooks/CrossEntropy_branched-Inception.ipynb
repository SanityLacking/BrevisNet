{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook example for building a BrevisNet model using Inceptionv3 as a base.\n",
    "\n",
    "Things to keep in mind when running this example: \n",
    "1. That the dataset size matches the model input size\n",
    "2. That the names of the joining layer points matches what you expect\n",
    "3. That your computer has enough memory to build the model and train as well as shuffle and load the dataset. In testing I've found that if the kernel gets interupted and then continued, the ram and gpu memory allocation can get wonky when performing the fit command.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "import branchingdnn as branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_ds, test_ds, validation_ds) = branching.dataset.prepare.dataset(tf.keras.datasets.cifar10.load_data(),32,5000,22500,(227,227),include_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcEntropy(y_hat):\n",
    "        #entropy is the sum of y * log(y) for all possible labels.\n",
    "        if isinstance(y_hat, list):\n",
    "            y_hat = np.array(y_hat)\n",
    "        sum_entropy = 0\n",
    "        if y_hat.ndim >1:\n",
    "            return list(map(calcEntropy,y_hat))\n",
    "        for i in range(len(y_hat)):\n",
    "            if y_hat[i] != 0: # log of zero is undefined, see MacKay's book \"Information Theory, Inference, and Learning Algorithms\"  for more info on this workaround reasoning.\n",
    "                entropy =y_hat[i] * math.log(y_hat[i],2)\n",
    "                sum_entropy +=  entropy\n",
    "\n",
    "        return -sum_entropy\n",
    "    \n",
    "def calcEntropy_Tensors(y_hat):\n",
    "        #entropy is the sum of y * log(y) for all possible labels.\n",
    "        #doesn't deal with cases of log(0)\n",
    "        rank = tf.rank(y_hat)\n",
    "        def calc_E(y_hat):\n",
    "            results = tf.clip_by_value((tf.math.log(y_hat)/tf.math.log(tf.constant(2, dtype=y_hat.dtype))), -1e12, 1e12)\n",
    "#             results = tf.clip_by_value(results, -1e12, 1e12)\n",
    "#             print(\"res \", results)\n",
    "            return tf.reduce_sum(y_hat * results)\n",
    "\n",
    "        sumEntropies = (tf.map_fn(calc_E,tf.cast(y_hat,'float')))\n",
    "        \n",
    "        if rank == 1:\n",
    "            sumEntropies = tf.reduce_sum(sumEntropies)\n",
    "        return -sumEntropies\n",
    "    \n",
    "def calcEntropy_Tensors2(y_hat):\n",
    "    #entropy is the sum of y * log(y) for all possible labels.\n",
    "    #doesn't deal with cases of log(0)\n",
    "#     num = tf.math.log(y_hat)\n",
    "# #     print(\"num\",num)\n",
    "#     dem = tf.math.log(tf.constant(2, dtype=y_hat.dtype))\n",
    "# #     print(\"dem\",dem)\n",
    "#     E = num / dem\n",
    "# #     print(\"E\",E)\n",
    "#     P = y_hat * E\n",
    "# #     print(\"p\",P)\n",
    "#     mean = tf.reduce_mean(tf.boolean_mask(P, tf.math.is_finite(P)))\n",
    "#     print(\"mean\",mean)\n",
    "#     sumEntropies = mean\n",
    "    val = y_hat * tf.math.log(y_hat)/tf.math.log(tf.constant(2, dtype=y_hat.dtype))\n",
    "    sumEntropies =  tf.reduce_mean(tf.boolean_mask(val,tf.math.is_finite(val)))\n",
    "    return -sumEntropies\n",
    "    \n",
    "# This function to generate evidence is used for the first example\n",
    "def relu_evidence(logits):\n",
    "    return tf.nn.relu(logits)\n",
    "\n",
    "# This one usually works better and used for the second and third examples\n",
    "# For general settings and different datasets, you may try this one first\n",
    "def exp_evidence(logits): \n",
    "    return tf.exp(tf.clip_by_value(logits,-10,10))\n",
    "\n",
    "# This one is another alternative and \n",
    "# usually behaves better than the relu_evidence \n",
    "def softplus_evidence(logits):\n",
    "    return tf.nn.softplus(logits)\n",
    "\n",
    "def KL(alpha):\n",
    "    # print(\"K:\",K)\n",
    "    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\n",
    "    S_alpha = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "    S_beta = tf.reduce_sum(beta,axis=1,keepdims=True)\n",
    "    lnB = tf.compat.v1.lgamma(S_alpha) - tf.reduce_sum(tf.compat.v1.lgamma(alpha),axis=1,keepdims=True)\n",
    "    lnB_uni = tf.reduce_sum(tf.compat.v1.lgamma(beta),axis=1,keepdims=True) - tf.compat.v1.lgamma(S_beta)\n",
    "\n",
    "    dg0 = tf.compat.v1.digamma(S_alpha)\n",
    "    dg1 = tf.compat.v1.digamma(alpha)\n",
    "\n",
    "    kl = tf.reduce_sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
    "    # print(\"kl\", kl)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyEndpoint(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_outputs, name=None, **kwargs):\n",
    "            super(CrossEntropyEndpoint, self).__init__(name=name)\n",
    "            self.num_outputs = num_outputs\n",
    "#             self.kl = tf.keras.losses.KLDivergence()\n",
    "            self.loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "#             self.loss_fn = tf.keras.losses.categorical_crossentropy\n",
    "            self.evidence = softplus_evidence\n",
    "#             self.evidence = tf.compat.v1.distributions.Dirichlet\n",
    "            self.temperature = 10\n",
    "            self.lmb = 0.005\n",
    "        def build(self, input_shape):\n",
    "            self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]), self.num_outputs])\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config().copy()\n",
    "            config.update({\n",
    "                'num_outputs': self.num_outputs,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "\n",
    "        def call(self, inputs, labels,learning_rate=1):\n",
    "            outputs = tf.matmul(inputs,self.kernel)\n",
    "            softmax = tf.nn.softmax(outputs)\n",
    "            evidence = self.evidence (outputs)\n",
    "            alpha = evidence + 1\n",
    "            u = self.num_outputs / tf.reduce_sum(alpha, axis=1, keepdims=True) #uncertainty\n",
    "          \n",
    "            # prob = alpha/tf.reduce_sum(alpha, 1, keepdims=True) \n",
    "            pred = tf.argmax(outputs,1)\n",
    "            truth = tf.argmax(labels,1)\n",
    "            match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\n",
    "            # total_evidence = tf.reduce_sum(evidence,1, keepdims=True)\n",
    "            mean_succ = tf.reduce_sum(tf.reduce_sum(evidence,1, keepdims=True)*match) / tf.reduce_sum(match+1e-20)\n",
    "            mean_fail = tf.reduce_sum(tf.reduce_sum(tf.reduce_sum(evidence,1, keepdims=True)*(1-match)) / (tf.reduce_sum(tf.abs(1-match))+1e-20) )\n",
    "            \n",
    "            self.add_metric(evidence, name=self.name+\"_evidence\",aggregation='mean')\n",
    "            self.add_metric(mean_succ, name=self.name+\"_mean_ev_succ\",aggregation='mean')\n",
    "            self.add_metric(mean_fail, name=self.name+\"_mean_ev_fail\",aggregation='mean')\n",
    "            \n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(annealing_rate=1, momentum=1, decay=1, global_loss=False):\n",
    "    #create a wrapper function that returns a function\n",
    "    temperature = 1\n",
    "    Classes = 10\n",
    "    keras_kl = tf.keras.losses.KLDivergence()\n",
    "    annealing_rate = annealing_rate\n",
    "    momentum_rate = momentum\n",
    "    decay_rate = decay\n",
    "\n",
    "    def cross_entropy_evidence(labels, outputs): \n",
    "        softmax = tf.nn.softmax(outputs)\n",
    "        # activated_outputs =tf.keras.activations.sigmoid(softmax)\n",
    "        evidence = softplus_evidence(outputs)\n",
    "        alpha = evidence + 1\n",
    "        S = tf.reduce_sum(alpha, axis=1, keepdims=True) \n",
    "        E = alpha - 1\n",
    "        m = alpha / S\n",
    "        A = tf.reduce_sum((labels-m)**2, axis=1, keepdims=True) \n",
    "        B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True) \n",
    "\n",
    "        annealing_coef = tf.minimum(1.0,tf.cast(annealing_rate,tf.float32))\n",
    "#         annealing_coef = 1\n",
    "        alp = E*(1-labels) + 1 \n",
    "        C =  annealing_coef * KL(alp)\n",
    "        # C = keras_kl(labels,evidence)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(labels, softmax)\n",
    "        pred = tf.argmax(outputs,1)\n",
    "        truth = tf.argmax(labels,1)\n",
    "        match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\n",
    "        return loss + C\n",
    "        # return (A + B) + C\n",
    "\n",
    "    return  cross_entropy_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "def loss_function(annealing_rate=1, momentum=1, decay=1, global_loss=False):\n",
    "    #create a wrapper function that returns a function\n",
    "    temperature = 1\n",
    "    Classes = 10\n",
    "    keras_kl = tf.keras.losses.KLDivergence()\n",
    "    annealing_rate = annealing_rate\n",
    "    momentum_rate = momentum\n",
    "    decay_rate = decay\n",
    "    def cross_entropy_evidence(labels, outputs): \n",
    "        softmax = tf.nn.softmax(outputs)\n",
    "        # activated_outputs =tf.keras.activations.sigmoid(softmax)\n",
    "        evidence = softplus_evidence(outputs)\n",
    "        alpha = evidence + 1\n",
    "        S = tf.reduce_sum(alpha, axis=1, keepdims=True) \n",
    "        E = alpha - 1\n",
    "        m = alpha / S\n",
    "        A = tf.reduce_sum((labels-m)**2, axis=1, keepdims=True) \n",
    "        B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True) \n",
    "\n",
    "        annealing_coef = tf.minimum(1.0,tf.cast(annealing_rate,tf.float32))\n",
    "#         annealing_coef = 1\n",
    "        alp = E*(1-labels) + 1 \n",
    "        # print(\"alp\", alp)\n",
    "        C =  annealing_coef * KL(alp)\n",
    "        # C = keras_kl(labels,evidence)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(labels, softmax)\n",
    "        pred = tf.argmax(outputs,1)\n",
    "        truth = tf.argmax(labels,1)\n",
    "        match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\n",
    "        return loss + C\n",
    "        # return (A + B) + C\n",
    "    return  cross_entropy_evidence\n",
    "\n",
    "# outputs =[]\n",
    "# targets = tf.keras.Input(shape=(10,),name='targets')\n",
    "# inputs = tf.keras.Input(shape=(227,227,3))\n",
    "# x = tf.keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),padding=\"same\")(x)\n",
    "\n",
    "\n",
    "# branchLayer = tf.keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "# branchLayer = tf.keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))(branchLayer)\n",
    "# branchLayer = tf.keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))(branchLayer)\n",
    "# # branchLayer = tf.keras.layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer)\n",
    "# # branch_output_1 = (CrossEntropyEndpoint(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer,targets))\n",
    "# branch_output_1 = tf.keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer)\n",
    "\n",
    "# x = tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),padding=\"same\")(x)\n",
    "\n",
    "\n",
    "# branchLayer = tf.keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "# branchLayer = tf.keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))(branchLayer)\n",
    "# branchLayer = tf.keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))(branchLayer)\n",
    "# # branchLayer = tf.keras.layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer)\n",
    "# # branch_output_2 = (CrossEntropyEndpoint(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer,targets))\n",
    "# branch_output_2 = tf.keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer)\n",
    "\n",
    "# x = tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),padding=\"same\")(x)\n",
    "# x = tf.keras.layers.Flatten()(x)\n",
    "# x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
    "\n",
    "\n",
    "\n",
    "# branchLayer = tf.keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "# branchLayer = tf.keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))(branchLayer)\n",
    "# branchLayer = tf.keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))(branchLayer)\n",
    "# # branchLayer = tf.keras.layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer)\n",
    "# branch_output_3 = tf.keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer)\n",
    "# # branch_output_3 = (CrossEntropyEndpoint(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer,targets))\n",
    "\n",
    "# x = tf.keras.layers.Dropout(0.5)(x)\n",
    "# x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
    "# x = tf.keras.layers.Dropout(0.5)(x)\n",
    "# output = tf.keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"endpoint\"))(x)\n",
    "# # output = CrossEntropyEndpoint(10, name=tf.compat.v1.get_default_graph().unique_name(\"endpoint\"))(x, targets)\n",
    "\n",
    "# model = tf.keras.Model(inputs=[inputs,targets], outputs=[output,branch_output_1,branch_output_2,branch_output_3], name=\"alexnet_branched_entropy\")\n",
    "# loss_fn = loss_function()\n",
    "# model.compile( loss=\"categorical_crossentropy\", optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# root_logdir = os.path.join(os.curdir, \"logs\\\\fit\\\\\")\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(\"alexNetv6_entropy_branched_scratch_40.hdf5\", monitor='val_loss',verbose=1,save_best_only=True, mode='auto')\n",
    "# def get_run_logdir():\n",
    "#     run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "#     return os.path.join(root_logdir, run_id)\n",
    "\n",
    "# run_logdir = get_run_logdir()\n",
    "# tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "# model.fit(train_ds,\n",
    "#         epochs=40,\n",
    "#         validation_data=validation_ds,\n",
    "#         validation_freq=1,\n",
    "#         # batch_size=1,\n",
    "#         verbose=1,\n",
    "#         callbacks=[tensorboard_cb,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment Dataset\n",
      "targetsis : True\n",
      "adding targets to inputs\n",
      "trainSize 45000\n",
      "testSize 10000\n"
     ]
    }
   ],
   "source": [
    "dataset = branching.dataset.prepare.dataset(tf.keras.datasets.cifar10.load_data(),32,5000,22500,(224,224), include_targets=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_ds, validation_ds, test_ds) = dataset\n",
    "# print(train_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets already present?  False\n",
      "added targets\n",
      "Matching Branchpoint by id number\n",
      "Matching Branchpoint by name\n",
      "add Branch to branch point  mixed0\n",
      "add Branch to branch point  mixed1\n",
      "add Branch to branch point  mixed6\n",
      "<branchingdnn.core.BranchingDnn.branched_model object at 0x000001AF2D3D1978>\n"
     ]
    }
   ],
   "source": [
    "# model  = tf.keras.models.load_model('alexNetv6_evidence_branched_contin_30.hdf5', custom_objects={\"CrossEntropyEndpoint\":CrossEntropyEndpoint,\"cross_entropy_evidence\":loss_function()})\n",
    "\n",
    "loss_fn = loss_function()\n",
    "brevis = (branching.core.branched_model(modelName=\"../models/inception_finetuned.hdf5\",saveName=\"inception_entropy_flat\",transfer=True,customOptions=\"\")\n",
    "            .add_branches(branching.branches.branch.newBranch_flatten_incept,[\"mixed0\",\"mixed1\",\"mixed6\"], target_input=True)\n",
    "            .set_dataset(dataset)\n",
    "            \n",
    "            )\n",
    "# brevis.evaluate(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_branched\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 111, 111, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 111, 111, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 111, 111, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 109, 109, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 109, 109, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 109, 109, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 109, 109, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 109, 109, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 109, 109, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 54, 54, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 54, 54, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 54, 54, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 54, 54, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 52, 52, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 52, 52, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 52, 52, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 25, 25, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 25, 25, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 25, 25, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 25, 25, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 25, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 25, 25, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 25, 25, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 25, 25, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 25, 25, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 25, 25, 64)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25, 25, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 25, 25, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 25, 25, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 25, 25, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 25, 25, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 25, 25, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 25, 25, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 25, 25, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 25, 25, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 25, 25, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 25, 25, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 25, 25, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 25, 25, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 25, 25, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 25, 25, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 25, 25, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 25, 25, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 25, 25, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 25, 25, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 25, 25, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 25, 25, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 25, 25, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 25, 25, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 25, 25, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 25, 25, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 25, 25, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 25, 25, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 25, 25, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 25, 25, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 25, 25, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 25, 25, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 25, 25, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 25, 25, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 25, 25, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 25, 25, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 25, 25, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 25, 25, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 25, 25, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 25, 25, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 25, 25, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 25, 25, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 25, 25, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 25, 25, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 25, 25, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 25, 25, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 25, 25, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 25, 25, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 25, 25, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 12, 12, 96)   82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 12, 12, 384)  1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 12, 12, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 12, 12, 384)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 12, 12, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 12, 12, 128)  384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 12, 12, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 12, 12, 128)  114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 12, 12, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 12, 12, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 12, 12, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 12, 12, 128)  384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 12, 12, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 12, 12, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 12, 12, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 12, 12, 128)  114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 12, 12, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 12, 12, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 12, 12, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 12, 12, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 12, 12, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 12, 12, 192)  172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 12, 12, 192)  172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 12, 12, 192)  576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 12, 12, 192)  576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 12, 12, 192)  576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 12, 12, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 12, 12, 192)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 12, 12, 192)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 12, 12, 192)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 12, 12, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 12, 12, 160)  480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 12, 12, 160)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 12, 12, 160)  179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 12, 12, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 12, 12, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 12, 12, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 12, 12, 160)  480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 12, 12, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 12, 12, 160)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 12, 12, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 12, 12, 160)  179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 12, 12, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 12, 12, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 12, 12, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 12, 12, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 12, 12, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 12, 12, 192)  215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 12, 12, 192)  215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 12, 12, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 12, 12, 192)  576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 12, 12, 192)  576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 12, 12, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 12, 12, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 12, 12, 192)  0           batch_normalization_43[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 12, 12, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 12, 12, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 12, 12, 160)  480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 12, 12, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 12, 12, 160)  179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 12, 12, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 12, 12, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 12, 12, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 12, 12, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 12, 12, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 12, 12, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 12, 12, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 12, 12, 160)  179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 12, 12, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 12, 12, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 12, 12, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 12, 12, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 12, 12, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 12, 12, 192)  215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 12, 12, 192)  215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 12, 12, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 12, 12, 192)  576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 12, 12, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 12, 12, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 12, 12, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 12, 12, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 12, 12, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 12, 12, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 12, 12, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 12, 12, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 12, 12, 192)  258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 12, 12, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 12, 12, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 12, 12, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 12, 12, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 12, 12, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 12, 12, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 12, 12, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 12, 12, 192)  258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 12, 12, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 12, 12, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 12, 12, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 12, 12, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 12, 12, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 12, 12, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 12, 12, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 12, 12, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 12, 12, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 12, 12, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 12, 12, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 12, 12, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 12, 12, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 12, 12, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 12, 12, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 12, 12, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 12, 12, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 12, 12, 192)  258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 12, 12, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 12, 12, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 12, 12, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 12, 12, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 12, 12, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 12, 12, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 12, 12, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 5, 5, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 5, 5, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 5, 5, 320)    960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 5, 5, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 5, 5, 320)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 5, 5, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 5, 5, 448)    1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 5, 5, 448)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 5, 5, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 5, 5, 384)    1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 5, 5, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 5, 5, 384)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 5, 5, 384)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 5, 5, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 5, 5, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 5, 5, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 5, 5, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 5, 5, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 5, 5, 320)    960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 5, 5, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 5, 5, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 5, 5, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 5, 5, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 5, 5, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 5, 5, 320)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5, 5, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 5, 5, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 5, 5, 448)    1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 5, 5, 448)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 5, 5, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 5, 5, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 5, 5, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 5, 5, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 5, 5, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 5, 5, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 5, 5, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 5, 5, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 5, 5, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 5, 5, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 5, 5, 320)    960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 5, 5, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 5, 5, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 5, 5, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 5, 5, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 5, 5, 192)    576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 5, 5, 320)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5, 5, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 5, 5, 192)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "branch_avgPool_3 (GlobalAverage (None, 256)          0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "branch_avgPool_4 (GlobalAverage (None, 288)          0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "branch_avgPool_5 (GlobalAverage (None, 768)          0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "branch_flatten_3 (Flatten)      (None, 256)          0           branch_avgPool_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch_flatten_4 (Flatten)      (None, 288)          0           branch_avgPool_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch_flatten_5 (Flatten)      (None, 768)          0           branch_avgPool_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "branch124_3 (Dense)             (None, 124)          31868       branch_flatten_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch124_4 (Dense)             (None, 124)          35836       branch_flatten_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch124_5 (Dense)             (None, 124)          95356       branch_flatten_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         2098176     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "branch64_3 (Dense)              (None, 64)           8000        branch124_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "branch64_4 (Dense)              (None, 64)           8000        branch124_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "branch64_5 (Dense)              (None, 64)           8000        branch124_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          524800      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "branch_output (Dense)           (None, 10)           650         branch64_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "branch_output_1 (Dense)         (None, 10)           650         branch64_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "branch_output_2 (Dense)         (None, 10)           650         branch64_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "targets (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 10)           5130        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "branch_softmax_3 (Softmax)      (None, 10)           0           branch_output[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "branch_softmax_4 (Softmax)      (None, 10)           0           branch_output_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "branch_softmax_5 (Softmax)      (None, 10)           0           branch_output_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 24,619,900\n",
      "Trainable params: 24,585,468\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "brevis.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Main Layers  and branch layers training to true\n",
      "\n",
      "customOption: Other\n",
      "https://app.neptune.ai/cailen01/branchingDNN/e/BRAN-374\n",
      "Remember to stop your run once youve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/12\n",
      "1406/1406 [==============================] - 468s 322ms/step - loss: 5.3854 - classification_loss: 0.0625 - branch_softmax_3_loss: 1.9018 - branch_softmax_4_loss: 1.9078 - branch_softmax_5_loss: 1.5133 - classification_accuracy: 0.9791 - branch_softmax_3_accuracy: 0.3014 - branch_softmax_4_accuracy: 0.2989 - branch_softmax_5_accuracy: 0.5482 - val_loss: 3.2439 - val_classification_loss: 0.0648 - val_branch_softmax_3_loss: 1.4355 - val_branch_softmax_4_loss: 1.3936 - val_branch_softmax_5_loss: 0.3500 - val_classification_accuracy: 0.9794 - val_branch_softmax_3_accuracy: 0.4688 - val_branch_softmax_4_accuracy: 0.4800 - val_branch_softmax_5_accuracy: 0.8912\n",
      "\n",
      "Epoch 00001: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 2/12\n",
      "1406/1406 [==============================] - 462s 318ms/step - loss: 3.1282 - classification_loss: 0.0609 - branch_softmax_3_loss: 1.4136 - branch_softmax_4_loss: 1.3117 - branch_softmax_5_loss: 0.3421 - classification_accuracy: 0.9797 - branch_softmax_3_accuracy: 0.4797 - branch_softmax_4_accuracy: 0.5150 - branch_softmax_5_accuracy: 0.8889 - val_loss: 3.3953 - val_classification_loss: 0.0900 - val_branch_softmax_3_loss: 1.6232 - val_branch_softmax_4_loss: 1.4313 - val_branch_softmax_5_loss: 0.2508 - val_classification_accuracy: 0.9720 - val_branch_softmax_3_accuracy: 0.4191 - val_branch_softmax_4_accuracy: 0.4850 - val_branch_softmax_5_accuracy: 0.9167\n",
      "\n",
      "Epoch 00002: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 3/12\n",
      "1406/1406 [==============================] - 463s 319ms/step - loss: 2.6004 - classification_loss: 0.0384 - branch_softmax_3_loss: 1.2556 - branch_softmax_4_loss: 1.1075 - branch_softmax_5_loss: 0.1988 - classification_accuracy: 0.9876 - branch_softmax_3_accuracy: 0.5455 - branch_softmax_4_accuracy: 0.5992 - branch_softmax_5_accuracy: 0.9353 - val_loss: 2.6592 - val_classification_loss: 0.0813 - val_branch_softmax_3_loss: 1.2840 - val_branch_softmax_4_loss: 1.0852 - val_branch_softmax_5_loss: 0.2087 - val_classification_accuracy: 0.9742 - val_branch_softmax_3_accuracy: 0.5258 - val_branch_softmax_4_accuracy: 0.6032 - val_branch_softmax_5_accuracy: 0.9289\n",
      "\n",
      "Epoch 00003: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 4/12\n",
      "1406/1406 [==============================] - 471s 324ms/step - loss: 2.2763 - classification_loss: 0.0234 - branch_softmax_3_loss: 1.1344 - branch_softmax_4_loss: 0.9797 - branch_softmax_5_loss: 0.1387 - classification_accuracy: 0.9925 - branch_softmax_3_accuracy: 0.5909 - branch_softmax_4_accuracy: 0.6477 - branch_softmax_5_accuracy: 0.9543 - val_loss: 3.0744 - val_classification_loss: 0.0887 - val_branch_softmax_3_loss: 1.5731 - val_branch_softmax_4_loss: 1.2072 - val_branch_softmax_5_loss: 0.2054 - val_classification_accuracy: 0.9732 - val_branch_softmax_3_accuracy: 0.4663 - val_branch_softmax_4_accuracy: 0.5709 - val_branch_softmax_5_accuracy: 0.9327\n",
      "\n",
      "Epoch 00004: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 5/12\n",
      "1406/1406 [==============================] - 509s 350ms/step - loss: 2.0319 - classification_loss: 0.0172 - branch_softmax_3_loss: 1.0360 - branch_softmax_4_loss: 0.8884 - branch_softmax_5_loss: 0.0903 - classification_accuracy: 0.9947 - branch_softmax_3_accuracy: 0.6249 - branch_softmax_4_accuracy: 0.6780 - branch_softmax_5_accuracy: 0.9711 - val_loss: 3.0720 - val_classification_loss: 0.1060 - val_branch_softmax_3_loss: 1.4816 - val_branch_softmax_4_loss: 1.2849 - val_branch_softmax_5_loss: 0.1996 - val_classification_accuracy: 0.9685 - val_branch_softmax_3_accuracy: 0.4844 - val_branch_softmax_4_accuracy: 0.5517 - val_branch_softmax_5_accuracy: 0.9341\n",
      "\n",
      "Epoch 00005: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 6/12\n",
      "1406/1406 [==============================] - 582s 402ms/step - loss: 1.8692 - classification_loss: 0.0135 - branch_softmax_3_loss: 0.9672 - branch_softmax_4_loss: 0.8240 - branch_softmax_5_loss: 0.0645 - classification_accuracy: 0.9961 - branch_softmax_3_accuracy: 0.6521 - branch_softmax_4_accuracy: 0.7064 - branch_softmax_5_accuracy: 0.9797 - val_loss: 2.9173 - val_classification_loss: 0.0998 - val_branch_softmax_3_loss: 1.5162 - val_branch_softmax_4_loss: 1.0985 - val_branch_softmax_5_loss: 0.2028 - val_classification_accuracy: 0.9752 - val_branch_softmax_3_accuracy: 0.5148 - val_branch_softmax_4_accuracy: 0.6230 - val_branch_softmax_5_accuracy: 0.9371\n",
      "\n",
      "Epoch 00006: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 7/12\n",
      "1406/1406 [==============================] - 592s 408ms/step - loss: 1.7598 - classification_loss: 0.0116 - branch_softmax_3_loss: 0.9175 - branch_softmax_4_loss: 0.7805 - branch_softmax_5_loss: 0.0502 - classification_accuracy: 0.9961 - branch_softmax_3_accuracy: 0.6694 - branch_softmax_4_accuracy: 0.7177 - branch_softmax_5_accuracy: 0.9841 - val_loss: 2.4579 - val_classification_loss: 0.1050 - val_branch_softmax_3_loss: 1.1947 - val_branch_softmax_4_loss: 0.9703 - val_branch_softmax_5_loss: 0.1879 - val_classification_accuracy: 0.9722 - val_branch_softmax_3_accuracy: 0.5982 - val_branch_softmax_4_accuracy: 0.6743 - val_branch_softmax_5_accuracy: 0.9409\n",
      "\n",
      "Epoch 00007: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 8/12\n",
      "1406/1406 [==============================] - 607s 419ms/step - loss: 1.6242 - classification_loss: 0.0088 - branch_softmax_3_loss: 0.8557 - branch_softmax_4_loss: 0.7233 - branch_softmax_5_loss: 0.0363 - classification_accuracy: 0.9974 - branch_softmax_3_accuracy: 0.6933 - branch_softmax_4_accuracy: 0.7409 - branch_softmax_5_accuracy: 0.9888 - val_loss: 2.1720 - val_classification_loss: 0.1177 - val_branch_softmax_3_loss: 0.9718 - val_branch_softmax_4_loss: 0.8803 - val_branch_softmax_5_loss: 0.2022 - val_classification_accuracy: 0.9700 - val_branch_softmax_3_accuracy: 0.6524 - val_branch_softmax_4_accuracy: 0.6877 - val_branch_softmax_5_accuracy: 0.9405\n",
      "\n",
      "Epoch 00008: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 9/12\n",
      " 675/1406 [=============>................] - ETA: 5:09 - loss: 1.5650 - classification_loss: 0.0097 - branch_softmax_3_loss: 0.8296 - branch_softmax_4_loss: 0.6917 - branch_softmax_5_loss: 0.0341 - classification_accuracy: 0.9964 - branch_softmax_3_accuracy: 0.7000 - branch_softmax_4_accuracy: 0.7484 - branch_softmax_5_accuracy: 0.9895"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-451067eabd42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrevis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainTransfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransfer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustomOptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\MachineLearning\\Keras_branchynet\\branchingdnn\\core.py\u001b[0m in \u001b[0;36mtrainTransfer\u001b[1;34m(self, numEpocs, loss, optimizer, transfer, customOptions)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mtrainTransfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumEpocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransfer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcustomOptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbranchingdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainModelTransfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumEpocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaveName\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransfer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransfer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcustomOptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustomOptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MachineLearning\\Keras_branchynet\\branchingdnn\\models\\train.py\u001b[0m in \u001b[0;36mtrainModelTransfer\u001b[1;34m(model, dataset, loss, optimizer, resetBranches, epocs, save, transfer, saveName, customOptions, tags)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;31m# batch_size=1,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             callbacks=[tensorboard_cb,checkpoint,neptune_cbk])\n\u001b[0m\u001b[0;32m    137\u001b[0m                         \u001b[1;31m# callbacks=[tensorboard_cb,checkpoint])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    803\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;34m\"\"\"Runs a training execution with one step.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mstep_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m       outputs = reduce_per_replica(\n\u001b[0;32m    797\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1257\u001b[0m       fn = autograph.tf_convert(\n\u001b[0;32m   1258\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m-> 1259\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2729\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2730\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2732\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3415\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3416\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3417\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3419\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    789\u001b[0m         \u001b[1;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    756\u001b[0m           y, y_pred, sample_weight, regularization_losses=self.losses)\n\u001b[0;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py\u001b[0m in \u001b[0;36mupdate_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmetric_obj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m           \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m         \u001b[0mmetric_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mweighted_metric_obj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweighted_metric_objs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# update_op will be None in eager execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mcontrol_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mag_update_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_update_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdef_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    665\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    348\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Allowlisted %s: from cache'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m     return super(MeanMetricWrapper, self).update_state(\n\u001b[1;32m--> 620\u001b[1;33m         matches, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[1;34m(self, values, sample_weight)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWEIGHTED_MEAN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m         \u001b[0mnum_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mnum_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(x, dtype, name)\u001b[0m\n\u001b[0;32m    964\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 1823\u001b[1;33m         _ctx, \"Cast\", name, x, \"DstT\", DstT, \"Truncate\", Truncate)\n\u001b[0m\u001b[0;32m   1824\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = brevis.trainTransfer(12, loss=\"categorical_crossentropy\", optimizer=tf.optimizers.SGD(), transfer=False, customOptions=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Main Layers  and branch layers training to true\n",
      "Model: \"model_branched\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 111, 111, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 111, 111, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 111, 111, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 109, 109, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 109, 109, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 109, 109, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 109, 109, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 109, 109, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 109, 109, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 54, 54, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 54, 54, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 54, 54, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 54, 54, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 52, 52, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 52, 52, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 52, 52, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 25, 25, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 25, 25, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 25, 25, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 25, 25, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 25, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 25, 25, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 25, 25, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 25, 25, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 25, 25, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 25, 25, 64)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25, 25, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 25, 25, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 25, 25, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 25, 25, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 25, 25, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 25, 25, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 25, 25, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 25, 25, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 25, 25, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 25, 25, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 25, 25, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 25, 25, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 25, 25, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 25, 25, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 25, 25, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 25, 25, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 25, 25, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 25, 25, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 25, 25, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 25, 25, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 25, 25, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 25, 25, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 25, 25, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 25, 25, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 25, 25, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 25, 25, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 25, 25, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 25, 25, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 25, 25, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 25, 25, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 25, 25, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 25, 25, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 25, 25, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 25, 25, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 25, 25, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 25, 25, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 25, 25, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 25, 25, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 25, 25, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 25, 25, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 25, 25, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 25, 25, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 25, 25, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 25, 25, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 25, 25, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 25, 25, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 25, 25, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 25, 25, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 12, 12, 96)   82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 12, 12, 384)  1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 12, 12, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 12, 12, 384)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 12, 12, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 12, 12, 128)  384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 12, 12, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 12, 12, 128)  114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 12, 12, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 12, 12, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 12, 12, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 12, 12, 128)  384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 12, 12, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 12, 12, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 12, 12, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 12, 12, 128)  114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 12, 12, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 12, 12, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 12, 12, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 12, 12, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 12, 12, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 12, 12, 192)  172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 12, 12, 192)  172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 12, 12, 192)  576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 12, 12, 192)  576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 12, 12, 192)  576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 12, 12, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 12, 12, 192)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 12, 12, 192)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 12, 12, 192)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 12, 12, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 12, 12, 160)  480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 12, 12, 160)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 12, 12, 160)  179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 12, 12, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 12, 12, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 12, 12, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 12, 12, 160)  480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 12, 12, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 12, 12, 160)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 12, 12, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 12, 12, 160)  179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 12, 12, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 12, 12, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 12, 12, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 12, 12, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 12, 12, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 12, 12, 192)  215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 12, 12, 192)  215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 12, 12, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 12, 12, 192)  576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 12, 12, 192)  576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 12, 12, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 12, 12, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 12, 12, 192)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 12, 12, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 12, 12, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 12, 12, 160)  480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 12, 12, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 12, 12, 160)  179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 12, 12, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 12, 12, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 12, 12, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 12, 12, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 12, 12, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 12, 12, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 12, 12, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 12, 12, 160)  179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 12, 12, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 12, 12, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 12, 12, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 12, 12, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 12, 12, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 12, 12, 192)  215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 12, 12, 192)  215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 12, 12, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 12, 12, 192)  576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 12, 12, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 12, 12, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 12, 12, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 12, 12, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 12, 12, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 12, 12, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 12, 12, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 12, 12, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 12, 12, 192)  258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 12, 12, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 12, 12, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 12, 12, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 12, 12, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 12, 12, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 12, 12, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 12, 12, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 12, 12, 192)  258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 12, 12, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 12, 12, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 12, 12, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 12, 12, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 12, 12, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 12, 12, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 12, 12, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 12, 12, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 12, 12, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 12, 12, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 12, 12, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 12, 12, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 12, 12, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 12, 12, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 12, 12, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 12, 12, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 12, 12, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 12, 12, 192)  258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 12, 12, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 12, 12, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 12, 12, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 12, 12, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 12, 12, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 12, 12, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 12, 12, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 5, 5, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 5, 5, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 5, 5, 320)    960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 5, 5, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 5, 5, 320)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 5, 5, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 5, 5, 448)    1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 5, 5, 448)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 5, 5, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 5, 5, 384)    1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 5, 5, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 5, 5, 384)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 5, 5, 384)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 5, 5, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 5, 5, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 5, 5, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 5, 5, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 5, 5, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 5, 5, 320)    960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 5, 5, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 5, 5, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 5, 5, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 5, 5, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 5, 5, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 5, 5, 320)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5, 5, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 5, 5, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 5, 5, 448)    1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 5, 5, 448)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 5, 5, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 5, 5, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 5, 5, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 5, 5, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 5, 5, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 5, 5, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 5, 5, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 5, 5, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 5, 5, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 5, 5, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 5, 5, 320)    960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 5, 5, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 5, 5, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 5, 5, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 5, 5, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 5, 5, 192)    576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 5, 5, 320)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5, 5, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 5, 5, 192)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "branch_flatten (Flatten)        (None, 160000)       0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "branch_flatten_1 (Flatten)      (None, 180000)       0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "branch_flatten_2 (Flatten)      (None, 110592)       0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "branch124 (Dense)               (None, 1024)         163841024   branch_flatten[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "branch124_1 (Dense)             (None, 1024)         184321024   branch_flatten_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch124_2 (Dense)             (None, 1024)         113247232   branch_flatten_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         2098176     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "branch64 (Dense)                (None, 512)          524800      branch124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "branch64_1 (Dense)              (None, 512)          524800      branch124_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "branch64_2 (Dense)              (None, 512)          524800      branch124_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          524800      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "branch_output (Dense)           (None, 10)           5130        branch64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "branch_output_1 (Dense)         (None, 10)           5130        branch64_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "branch_output_2 (Dense)         (None, 10)           5130        branch64_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "targets (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 10)           5130        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "branch_softmax (Softmax)        (None, 10)           0           branch_output[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "branch_softmax_1 (Softmax)      (None, 10)           0           branch_output_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "branch_softmax_2 (Softmax)      (None, 10)           0           branch_output_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 487,429,960\n",
      "Trainable params: 487,395,528\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "customOption: Other\n",
      "https://app.neptune.ai/cailen01/branchingDNN/e/BRAN-338\n",
      "Remember to stop your run once youve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/12\n",
      "1406/1406 [==============================] - 537s 368ms/step - loss: 11.0903 - classification_loss: 2.1802 - branch_softmax_loss: 3.1945 - branch_softmax_1_loss: 3.1898 - branch_softmax_2_loss: 2.5258 - classification_accuracy: 0.9890 - branch_softmax_accuracy: 0.0963 - branch_softmax_1_accuracy: 0.1010 - branch_softmax_2_accuracy: 0.6927 - val_loss: 10.8150 - val_classification_loss: 2.1664 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2722 - val_classification_accuracy: 0.9974 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9050\n",
      "\n",
      "Epoch 00001: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 2/12\n",
      "1406/1406 [==============================] - 538s 372ms/step - loss: 10.8050 - classification_loss: 2.1718 - branch_softmax_loss: 3.1933 - branch_softmax_1_loss: 3.1909 - branch_softmax_2_loss: 2.2490 - classification_accuracy: 0.9937 - branch_softmax_accuracy: 0.0980 - branch_softmax_1_accuracy: 0.1001 - branch_softmax_2_accuracy: 0.9292 - val_loss: 10.7806 - val_classification_loss: 2.1670 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2372 - val_classification_accuracy: 0.9968 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9373\n",
      "\n",
      "Epoch 00002: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 3/12\n",
      "1406/1406 [==============================] - 535s 371ms/step - loss: 10.7657 - classification_loss: 2.1687 - branch_softmax_loss: 3.1896 - branch_softmax_1_loss: 3.1933 - branch_softmax_2_loss: 2.2141 - classification_accuracy: 0.9950 - branch_softmax_accuracy: 0.1012 - branch_softmax_1_accuracy: 0.0980 - branch_softmax_2_accuracy: 0.9567 - val_loss: 10.7854 - val_classification_loss: 2.1672 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2418 - val_classification_accuracy: 0.9960 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9317\n",
      "\n",
      "Epoch 00003: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 4/12\n",
      "1406/1406 [==============================] - 533s 370ms/step - loss: 10.7432 - classification_loss: 2.1664 - branch_softmax_loss: 3.1909 - branch_softmax_1_loss: 3.1918 - branch_softmax_2_loss: 2.1941 - classification_accuracy: 0.9969 - branch_softmax_accuracy: 0.1001 - branch_softmax_1_accuracy: 0.0994 - branch_softmax_2_accuracy: 0.9740 - val_loss: 10.7771 - val_classification_loss: 2.1667 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2340 - val_classification_accuracy: 0.9966 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9391\n",
      "\n",
      "Epoch 00004: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 5/12\n",
      "1406/1406 [==============================] - 533s 370ms/step - loss: 10.7349 - classification_loss: 2.1649 - branch_softmax_loss: 3.1917 - branch_softmax_1_loss: 3.1933 - branch_softmax_2_loss: 2.1850 - classification_accuracy: 0.9978 - branch_softmax_accuracy: 0.0994 - branch_softmax_1_accuracy: 0.0981 - branch_softmax_2_accuracy: 0.9822 - val_loss: 10.7725 - val_classification_loss: 2.1672 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2289 - val_classification_accuracy: 0.9958 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9431\n",
      "\n",
      "Epoch 00005: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 6/12\n",
      "1406/1406 [==============================] - 536s 371ms/step - loss: 10.7252 - classification_loss: 2.1642 - branch_softmax_loss: 3.1906 - branch_softmax_1_loss: 3.1912 - branch_softmax_2_loss: 2.1791 - classification_accuracy: 0.9982 - branch_softmax_accuracy: 0.1004 - branch_softmax_1_accuracy: 0.0998 - branch_softmax_2_accuracy: 0.9861 - val_loss: 10.7709 - val_classification_loss: 2.1658 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2286 - val_classification_accuracy: 0.9966 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 7/12\n",
      "1406/1406 [==============================] - 538s 373ms/step - loss: 10.7202 - classification_loss: 2.1635 - branch_softmax_loss: 3.1918 - branch_softmax_1_loss: 3.1910 - branch_softmax_2_loss: 2.1740 - classification_accuracy: 0.9988 - branch_softmax_accuracy: 0.0994 - branch_softmax_1_accuracy: 0.1000 - branch_softmax_2_accuracy: 0.9905 - val_loss: 10.7615 - val_classification_loss: 2.1657 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2194 - val_classification_accuracy: 0.9966 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9503\n",
      "\n",
      "Epoch 00007: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 8/12\n",
      "1406/1406 [==============================] - 540s 374ms/step - loss: 10.7181 - classification_loss: 2.1632 - branch_softmax_loss: 3.1916 - branch_softmax_1_loss: 3.1926 - branch_softmax_2_loss: 2.1708 - classification_accuracy: 0.9987 - branch_softmax_accuracy: 0.0995 - branch_softmax_1_accuracy: 0.0987 - branch_softmax_2_accuracy: 0.9929 - val_loss: 10.7632 - val_classification_loss: 2.1664 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2203 - val_classification_accuracy: 0.9964 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9501\n",
      "\n",
      "Epoch 00008: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 9/12\n",
      "1406/1406 [==============================] - 539s 374ms/step - loss: 10.7157 - classification_loss: 2.1630 - branch_softmax_loss: 3.1924 - branch_softmax_1_loss: 3.1906 - branch_softmax_2_loss: 2.1698 - classification_accuracy: 0.9989 - branch_softmax_accuracy: 0.0988 - branch_softmax_1_accuracy: 0.1004 - branch_softmax_2_accuracy: 0.9936 - val_loss: 10.7599 - val_classification_loss: 2.1661 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2174 - val_classification_accuracy: 0.9968 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9523\n",
      "\n",
      "Epoch 00009: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 10/12\n",
      "1406/1406 [==============================] - 542s 376ms/step - loss: 10.7142 - classification_loss: 2.1634 - branch_softmax_loss: 3.1894 - branch_softmax_1_loss: 3.1920 - branch_softmax_2_loss: 2.1694 - classification_accuracy: 0.9984 - branch_softmax_accuracy: 0.1014 - branch_softmax_1_accuracy: 0.0991 - branch_softmax_2_accuracy: 0.9936 - val_loss: 10.7615 - val_classification_loss: 2.1669 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2181 - val_classification_accuracy: 0.9954 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9507\n",
      "\n",
      "Epoch 00010: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 11/12\n",
      "1406/1406 [==============================] - 564s 391ms/step - loss: 10.7130 - classification_loss: 2.1625 - branch_softmax_loss: 3.1914 - branch_softmax_1_loss: 3.1909 - branch_softmax_2_loss: 2.1682 - classification_accuracy: 0.9994 - branch_softmax_accuracy: 0.0997 - branch_softmax_1_accuracy: 0.1001 - branch_softmax_2_accuracy: 0.9944 - val_loss: 10.7588 - val_classification_loss: 2.1670 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2154 - val_classification_accuracy: 0.9952 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9537\n",
      "\n",
      "Epoch 00011: saving model to models\\inception_entropy_flat.hdf5\n",
      "Epoch 12/12\n",
      "1406/1406 [==============================] - 574s 398ms/step - loss: 10.7121 - classification_loss: 2.1624 - branch_softmax_loss: 3.1915 - branch_softmax_1_loss: 3.1906 - branch_softmax_2_loss: 2.1677 - classification_accuracy: 0.9993 - branch_softmax_accuracy: 0.0996 - branch_softmax_1_accuracy: 0.1004 - branch_softmax_2_accuracy: 0.9949 - val_loss: 10.7578 - val_classification_loss: 2.1666 - val_branch_softmax_loss: 3.1897 - val_branch_softmax_1_loss: 3.1867 - val_branch_softmax_2_loss: 2.2149 - val_classification_accuracy: 0.9958 - val_branch_softmax_accuracy: 0.1012 - val_branch_softmax_1_accuracy: 0.1038 - val_branch_softmax_2_accuracy: 0.9543\n",
      "\n",
      "Epoch 00012: saving model to models\\inception_entropy_flat.hdf5\n",
      "<tensorflow.python.keras.callbacks.History object at 0x00000141A1DA2470>\n",
      "312/312 - 38s - loss: 10.8198 - classification_loss: 2.2003 - branch_softmax_loss: 3.1911 - branch_softmax_1_loss: 3.1908 - branch_softmax_2_loss: 2.2376 - classification_accuracy: 0.9666 - branch_softmax_accuracy: 0.1000 - branch_softmax_1_accuracy: 0.1002 - branch_softmax_2_accuracy: 0.9333\n",
      "overall loss: 10.819804191589355\n"
     ]
    }
   ],
   "source": [
    "model = brevis.trainTransfer(12, loss=loss_fn, optimizer=tf.optimizers.SGD(), transfer=False, customOptions=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = tf.keras.models.load_model('models/inception_entropy_flat.hdf5', custom_objects={\"CrossEntropyEndpoint\":CrossEntropyEndpoint,\"cross_entropy_evidence\":loss_function()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 0 of 10000\r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b5b8c402bc14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m#                 print(\"overlap\",pOverlap[j][i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;31m#             print(result)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "evidence=False\n",
    "num_outputs=4\n",
    "predictions = []\n",
    "labels = []\n",
    "pClass = []\n",
    "predictions=[]\n",
    "pEvidence = []\n",
    "pUncertainty=[]\n",
    "pOverlap=[]\n",
    "\n",
    "Outputs = pd.DataFrame()\n",
    "pAcc=[]\n",
    "for i in range(num_outputs):\n",
    "    pClass.append([])\n",
    "    predictions.append([])\n",
    "    pEvidence.append([])\n",
    "    pUncertainty.append([])\n",
    "    pAcc.append([])\n",
    "    pOverlap.append([])\n",
    "    # pOutputs.append([])\n",
    "for i, (x,y) in enumerate(test_ds):\n",
    "        if i > 10:\n",
    "            break\n",
    "        print(\"prediction: {} of {}\".format(i,len(test_ds)),end='\\r')\n",
    "        if evidence: \n",
    "            result = model.model.test_on_batch(x,y)\n",
    "#             print(result)\n",
    "            for j in range(num_outputs):\n",
    "#                 print(\"output\",j)\n",
    "                pClass[j].append(tf.argmax(y[0]).numpy())\n",
    "#                 print(\"class\",pClass[j][i])\n",
    "                pAcc[j].append(result[j+(num_outputs+1)])  \n",
    "#                 print(\"acc\",pAcc[j][i])\n",
    "                if j ==0:\n",
    "                    pEvidence[j].append(0)\n",
    "                else:\n",
    "#                     print(\"evid Number\",((num_outputs * 2)+1), \" \", ((j-1)*3))\n",
    "                    pEvidence[j].append(result[((num_outputs * 2) + 1)+((j-1)*3)])\n",
    "#                 print(\"evid\",pEvidence[j][i])\n",
    "\n",
    "                pOverlap[j].append(pAcc[0][i] - pAcc[j][i])\n",
    "#                 print(\"overlap\",pOverlap[j][i])\n",
    "        else:\n",
    "            result = model.model.predict(x)[0]\n",
    "#             print(result)\n",
    "\n",
    "            for j in range(num_outputs):\n",
    "                pClass[j].append(tf.argmax(y[0]).numpy())\n",
    "                # print(pClass[j])\n",
    "                # print(result)\n",
    "                prediction = np.argmax(result[j])\n",
    "                if prediction == pClass[j][i]:\n",
    "                    pAcc[j].append(1)  \n",
    "                else:\n",
    "                    pAcc[j].append(0)  \n",
    "                print(branching.utils.calcEntropy_Tensors(result[j]).numpy())\n",
    "                pEvidence[j].append(branching.utils.calcEntropy_Tensors(result[j]).numpy()[0])\n",
    "\n",
    "                pOverlap[j].append(pAcc[0][i] - pAcc[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectEvidence_branches(model,test_ds, evidence=True):\n",
    "    num_outputs = len(model.outputs) # the number of output layers for the purpose of providing labels\n",
    "    print(\"outputs\",num_outputs)\n",
    "#     train_ds, test_ds, validation_ds = (dataset)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    pClass = []\n",
    "    predictions=[]\n",
    "    pEvidence = []\n",
    "    pUncertainty=[]\n",
    "    pOverlap=[]\n",
    "\n",
    "    Outputs = pd.DataFrame()\n",
    "    pAcc=[]\n",
    "    for i in range(num_outputs):\n",
    "        pClass.append([])\n",
    "        predictions.append([])\n",
    "        pEvidence.append([])\n",
    "        pUncertainty.append([])\n",
    "        pAcc.append([])\n",
    "        pOverlap.append([])\n",
    "        # pOutputs.append([])\n",
    "\n",
    "    for i, (x,y) in enumerate(test_ds):\n",
    "        # if i > 10:\n",
    "            # break\n",
    "        print(\"prediction: {} of {}\".format(i,len(test_ds)),end='\\r')\n",
    "        if evidence: \n",
    "            result = model.test_on_batch(x,y)\n",
    "            if i < 1:\n",
    "                print(result)\n",
    "#             print(result)\n",
    "            for j in range(num_outputs):\n",
    "#                 print(\"output\",j)\n",
    "                pClass[j].append(tf.argmax(y[0]).numpy())\n",
    "#                 print(\"class\",pClass[j][i])\n",
    "                pAcc[j].append(result[j+(num_outputs+1)])  \n",
    "#                 print(\"acc\",pAcc[j][i])\n",
    "                if j ==0:\n",
    "                    pEvidence[j].append(0)\n",
    "                else:\n",
    "#                     print(\"evid Number\",((num_outputs * 2)+1), \" \", ((j-1)*3))\n",
    "                    pEvidence[j].append(result[((num_outputs * 2) + 1)+((j-1)*3)])\n",
    "#                 print(\"evid\",pEvidence[j][i])\n",
    "\n",
    "                pOverlap[j].append(pAcc[0][i] - pAcc[j][i])\n",
    "#                 print(\"overlap\",pOverlap[j][i])\n",
    "        else:\n",
    "            result = model.predict(x)[0]\n",
    "            if i < 5:\n",
    "                print(result)\n",
    "    \n",
    "            for j in range(num_outputs):\n",
    "                pClass[j].append(tf.argmax(y[0]).numpy())\n",
    "                # print(pClass[j])\n",
    "                # print(result)\n",
    "                prediction = np.argmax(result[j])\n",
    "                if prediction == pClass[j][i]:\n",
    "                    pAcc[j].append(1)  \n",
    "                else:\n",
    "                    pAcc[j].append(0)  \n",
    "                # print(branching.utils.calcEntropy_Tensors(result[j]).numpy())\n",
    "                pEvidence[j].append(branching.utils.calcEntropy_Tensors(result[j]).numpy()[0])\n",
    "\n",
    "                pOverlap[j].append(pAcc[0][i] - pAcc[j][i])\n",
    "        '''\n",
    "        overlap\n",
    "        if zero, both match, if else they don't match\n",
    "        TT 1-1 =0\n",
    "        TF 1-0 =1\n",
    "\n",
    "        FT 0-1 = -1\n",
    "        FF 0-0 =0\n",
    "        \n",
    "        '''\n",
    "    Outputs=[]\n",
    "    for j in range(num_outputs):\n",
    "        Predictions = pd.DataFrame({\"label\":pClass[j],\"evidence\":pEvidence[j],\"Acc\":pAcc[j], \"overlap\":pOverlap[j]})\n",
    "        Outputs.append(Predictions)\n",
    "    return Outputs\n",
    "\n",
    "def displayEvidence(branch_predictions, output_names=[\"main_exit\",\"branch_1\",\"branch_2\",\"branch_3\"], Evidence = True):\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    fig = plt.figure()\n",
    "    series=[]\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.tight_layout()\n",
    "    Outputs=pd.DataFrame()\n",
    "    for i, Predictions in enumerate(branch_predictions):\n",
    "        Predictions[\"Acc\"]=Predictions[\"Acc\"].astype('bool') ##sometime the predictions can come back with 0.5 acc, this should be rounded to 1.\n",
    "        # Predictions[\"evidence\"]=Predictions[\"evidence\"].()[0]\n",
    "        acc = Predictions[\"Acc\"].value_counts()\n",
    "        # print(acc)\n",
    "        # print((acc.loc[True] , acc.loc[False]))\n",
    "        mean = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].mean().iloc[0]\n",
    "        std = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].std().iloc[0]\n",
    "        \n",
    "        correct_rows = Predictions.loc[Predictions['Acc'] == True]\n",
    "        incorrect_rows = Predictions.loc[Predictions['Acc'] == False]\n",
    "        if Evidence:\n",
    "            E_threshold = mean + std\n",
    "            Incorrects_overlap = Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] > E_threshold) & (Predictions[\"overlap\"] == 0)].count().iloc[0]\n",
    "            Outputs = Outputs.append(pd.DataFrame({\"Branch Name\":output_names[i],\n",
    "                    \"Accuracy\":(acc.loc[True] /  (acc.loc[True] + acc.loc[False])),\n",
    "                    \"E_Threshold\":E_threshold,\n",
    "                    # \"Overlap_Threshold\":non_overlapping_incorrects_threshold,\n",
    "                    \"acceptance_rate\":Predictions.loc[(Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0]/(Predictions.count().iloc[0]),\n",
    "                    \"accepted_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"accepted_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"accepted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0])/ Predictions.loc[(Predictions[\"evidence\"] > E_threshold)].count()[0],\n",
    "                    \"overlap_adjusted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0] + Incorrects_overlap) / Predictions.loc[(Predictions[\"evidence\"] > E_threshold)].count()[0],\n",
    "                    \"rejected_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"rejected_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"Incorrects_overlap\":Incorrects_overlap,\n",
    "                    },index=[i]))\n",
    "        else:\n",
    "            print(\"mean\",mean , \" std\",std)\n",
    "            E_threshold = mean - std\n",
    "            Incorrects_overlap = Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] < E_threshold) & (Predictions[\"overlap\"] == 0)].count().iloc[0]\n",
    "            # if i ==1 or i == 0 :\n",
    "                # print(Predictions)\n",
    "                # print(Predictions.loc[ (Predictions['Acc'] == True)  & (Predictions[\"overlap\"] == 0) ])\n",
    "            # print(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold) ].count())\n",
    "            Outputs = Outputs.append(pd.DataFrame({\"Branch Name\":output_names[i],\n",
    "                    \"Accuracy\":(acc.loc[True] /  (acc.loc[True] + acc.loc[False])),\n",
    "                    \"E_Threshold\":E_threshold,\n",
    "                    # \"Overlap_Threshold\":non_overlapping_incorrects_threshold,\n",
    "                    \"acceptance_rate\":Predictions.loc[(Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0]/(Predictions.count().iloc[0]),\n",
    "                    \"accepted_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"accepted_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"accepted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0])/ Predictions.loc[(Predictions[\"evidence\"] < E_threshold)].count()[0],\n",
    "                    \"overlap_adjusted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0] + Incorrects_overlap) / Predictions.loc[(Predictions[\"evidence\"] < E_threshold)].count()[0],\n",
    "                    \"rejected_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"rejected_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"Incorrects_overlap\":Incorrects_overlap,\n",
    "                    },index=[i]))\n",
    "        axs[round(int(i/2)), round(i%2)]\n",
    "    # fig, axs = plt.subplots(1, 2)\n",
    "        # axs[round(int(i/2)), round(i%2)].suptitle('Horizontally stacked subplots')\n",
    "        axs[round(int(i/2)), round(i%2)].scatter(correct_rows['label'],correct_rows['evidence'],c ='r',marker='+')\n",
    "        axs[round(int(i/2)), round(i%2)].scatter(incorrect_rows['label']+.3,incorrect_rows['evidence'],c ='k',marker='x')\n",
    "        axs[round(int(i/2)), round(i%2)].plot(np.repeat(E_threshold,11),'b--')\n",
    "        axs[round(int(i/2)), round(i%2)].title.set_text(\"evidence\")\n",
    "    \n",
    "    for ax in fig.axes:\n",
    "            # ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "            axLine, axLabel = ax.get_legend_handles_labels()\n",
    "            lines=(axLine)\n",
    "            labels=(axLabel)\n",
    "    fig.text(0.5, 0.01, 'Items Exit at Branch', ha='center', va='center')\n",
    "    fig.text(0.01, 0.5, 'Accuracy %', ha='center', va='center', rotation='vertical')\n",
    "    # fig.legend(lines, labels,bbox_to_anchor=(1., 1), loc=2,borderaxespad=0.,frameon=True)\n",
    "    # fig.set_size_inches(10, 10)\n",
    "    plt.show()\n",
    "    return Outputs\n",
    "\n",
    "\n",
    "def evidenceHistogram(branch_predictions, output_names=[\"main_exit\",\"branch_1\",\"branch_2\",\"branch_3\"], Evidence = True):\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    fig = plt.figure()\n",
    "    series=[]\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.tight_layout()\n",
    "    Outputs=pd.DataFrame()\n",
    "    for i, Predictions in enumerate(branch_predictions):\n",
    "        Predictions[\"Acc\"]=Predictions[\"Acc\"].astype('bool')\n",
    "        # Predictions[\"evidence\"]=Predictions[\"evidence\"].()[0]\n",
    "        acc = Predictions[\"Acc\"].value_counts()\n",
    "        # print(acc)\n",
    "        # print((acc.loc[True] , acc.loc[False]))\n",
    "        mean = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].mean().iloc[0]\n",
    "        std = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].std().iloc[0]\n",
    "        \n",
    "        correct_rows = Predictions.loc[Predictions['Acc'] == True]\n",
    "        incorrect_rows = Predictions.loc[Predictions['Acc'] == False]\n",
    "        if Evidence:\n",
    "            # E_threshold = mean + std + einsumfunc\n",
    "\n",
    "            Accepted = Predictions.loc[(Predictions[\"evidence\"] <= E_threshold)]\n",
    "            Rejected = Predictions.loc[(Predictions[\"evidence\"] > E_threshold)]\n",
    "            Incorrects_overlap = Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] > E_threshold) & (Predictions[\"overlap\"] == 0)].count().iloc[0]\n",
    "            Outputs = Outputs.append(pd.DataFrame({\"Branch Name\":output_names[i],\n",
    "                    \"Accuracy\":(acc.loc[True] /  (acc.loc[True] + acc.loc[False])),\n",
    "                    \"E_Threshold\":E_threshold,\n",
    "                    # \"Overlap_Threshold\":non_overlapping_incorrects_threshold,\n",
    "                    \"acceptance_rate\":Predictions.loc[(Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0]/(Predictions.count().iloc[0]),\n",
    "                    \"accepted_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"accepted_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"accepted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0])/ Predictions.loc[(Predictions[\"evidence\"] > E_threshold)].count()[0],\n",
    "                    \"overlap_adjusted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0] + Incorrects_overlap) / Predictions.loc[(Predictions[\"evidence\"] > E_threshold)].count()[0],\n",
    "                    \"T_F\":Accepted.loc[Accepted['overlap']==1],\n",
    "                    \"F_T\":Accepted.loc[Accepted['overlap']==-1],\n",
    "                    \"Incorrects_overlap\":Incorrects_overlap,\n",
    "                    },index=[i]))\n",
    "        else:\n",
    "            print(\"mean\",mean , \" std\",std)\n",
    "            E_threshold = mean - std\n",
    "            Incorrects_overlap = Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] < E_threshold) & (Predictions[\"overlap\"] == 0)].count().iloc[0]\n",
    "            # if i ==1 or i == 0 :\n",
    "                # print(Predictions)\n",
    "                # print(Predictions.loc[ (Predictions['Acc'] == True)  & (Predictions[\"overlap\"] == 0) ])\n",
    "            # print(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold) ].count())\n",
    "           \n",
    "            Outputs = Outputs.append(pd.DataFrame({\"Branch Name\":output_names[i],\n",
    "                    \"Accuracy\":(acc.loc[True] /  (acc.loc[True] + acc.loc[False])),\n",
    "                    \"E_Threshold\":E_threshold,\n",
    "                    # \"Overlap_Threshold\":non_overlapping_incorrects_threshold,\n",
    "                    \"acceptance_rate\":Predictions.loc[(Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0]/(Predictions.count().iloc[0]),\n",
    "                    \"accepted_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"accepted_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"accepted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0])/ Predictions.loc[(Predictions[\"evidence\"] < E_threshold)].count()[0],\n",
    "                    \"overlap_adjusted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0] + Incorrects_overlap) / Predictions.loc[(Predictions[\"evidence\"] < E_threshold)].count()[0],\n",
    "                    \"rejected_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"rejected_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                    \"Incorrects_overlap\":Incorrects_overlap,\n",
    "                    },index=[i]))\n",
    "        axs[round(int(i/2)), round(i%2)]\n",
    "    # fig, axs = plt.subplots(1, 2)\n",
    "        # axs[round(int(i/2)), round(i%2)].suptitle('Horizontally stacked subplots')\n",
    "        axs[round(int(i/2)), round(i%2)].scatter(correct_rows['label'],correct_rows['evidence'],c ='r',marker='+')\n",
    "        axs[round(int(i/2)), round(i%2)].scatter(incorrect_rows['label']+.3,incorrect_rows['evidence'],c ='k',marker='x')\n",
    "        axs[round(int(i/2)), round(i%2)].plot(np.repeat(E_threshold,11),'b--')\n",
    "        axs[round(int(i/2)), round(i%2)].title.set_text(\"evidence\")\n",
    "    \n",
    "    for ax in fig.axes:\n",
    "            # ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "            axLine, axLabel = ax.get_legend_handles_labels()\n",
    "            lines=(axLine)\n",
    "            labels=(axLabel)\n",
    "    fig.text(0.5, 0.01, 'Items Exit at Branch', ha='center', va='center')\n",
    "    fig.text(0.01, 0.5, 'Accuracy %', ha='center', va='center', rotation='vertical')\n",
    "    # fig.legend(lines, labels,bbox_to_anchor=(1., 1), loc=2,borderaxespad=0.,frameon=True)\n",
    "    # fig.set_size_inches(10, 10)\n",
    "    plt.show()\n",
    "    return Outputs\n",
    "\n",
    "def displayEvidence_cascade(branch_predictions, thresholds=None, output_names=[\"branch_1\",\"branch_2\",\"branch_3\",\"Main_Exit\"], Evidence = True):\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    fig = plt.figure()\n",
    "    series=[]\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.tight_layout()\n",
    "    Outputs=pd.DataFrame()\n",
    "    #lets reorder the predictions so that the final layer is at the end\n",
    "    # _branch_predictions.copy()\n",
    "    _branch_predictions = branch_predictions.copy()\n",
    "    # print(_branch_predictions)\n",
    "    _branch_predictions.append(_branch_predictions.pop(0))\n",
    "    # print(_branch_predictions)\n",
    "    rollOver_indices = pd.Index([])\n",
    "    for i, Predictions in enumerate(_branch_predictions):\n",
    "        #check if rollover is active, if so, select only the predictions whose indexes match the rollover list\n",
    "        # print(rollOver_indices)\n",
    "        test_acc = Predictions[\"Acc\"].astype('bool').value_counts()\n",
    "        test_accuracy = (test_acc.loc[True] /  (test_acc.loc[True] + test_acc.loc[False]))\n",
    "        if len(rollOver_indices)>0:\n",
    "            print(\"rollover enabled, {} predictions provided\".format(len(rollOver_indices)))\n",
    "            Predictions = Predictions.iloc[rollOver_indices]\n",
    "        # print(Predictions.shape)\n",
    "        Predictions[\"Acc\"]=Predictions[\"Acc\"].astype('bool')\n",
    "        # Predictions[\"evidence\"]=Predictions[\"evidence\"].()[0]\n",
    "        acc = Predictions[\"Acc\"].value_counts()\n",
    "        # print(acc)\n",
    "        # print((acc.loc[True] , acc.loc[False]))\n",
    "        _Incorrects_missed = Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"overlap\"] == 1)] #all the predictions that the main exit got true and the branch got wrong\n",
    "        if len(_Incorrects_missed) > 0 :\n",
    "            mean = _Incorrects_missed.groupby(\"Acc\")[\"evidence\"].mean().iloc[0]\n",
    "            std = _Incorrects_missed.groupby(\"Acc\")[\"evidence\"].std().iloc[0]\n",
    "        else:\n",
    "            mean = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].mean().iloc[0]\n",
    "            std = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].std().iloc[0]\n",
    "\n",
    "        print(\"mean\",mean , \" std\",std)\n",
    "        \n",
    "        correct_rows = Predictions.loc[Predictions['Acc'] == True]\n",
    "        incorrect_rows = Predictions.loc[Predictions['Acc'] == False]\n",
    "        \n",
    "        E_threshold = -1 #-1 is null for threshold\n",
    "        if thresholds is not None:\n",
    "            try:\n",
    "                E_threshold = thresholds[i]\n",
    "            except:\n",
    "                print(\"threshold not supplied for branch {}, using test data\".format(i))\n",
    "                \n",
    "        if Evidence:\n",
    "            if E_threshold ==-1:\n",
    "                E_threshold = mean + std\n",
    "            Accepted = Predictions.loc[(Predictions[\"evidence\"] >= E_threshold)]\n",
    "            Rejected = Predictions.loc[(Predictions[\"evidence\"] < E_threshold)]\n",
    "        else: \n",
    "            if E_threshold ==-1:\n",
    "                E_threshold = mean - std\n",
    "            Accepted = Predictions.loc[(Predictions[\"evidence\"] <= E_threshold)]\n",
    "            Rejected = Predictions.loc[(Predictions[\"evidence\"] > E_threshold)]\n",
    "        \n",
    "        rollOver_indices = Rejected.index\n",
    "        Incorrects_overlap = Accepted.loc[(Accepted['Acc'] == False) & (Accepted[\"overlap\"] == 0)].count().iloc[0]\n",
    "        Outputs = Outputs.append(pd.DataFrame({\"Branch Name\":output_names[i],\n",
    "                \"Predictions\": len(Predictions.index),\n",
    "                \"test_accuracy\": test_accuracy,\n",
    "                \"Accuracy\":(acc.loc[True] /  (acc.loc[True] + acc.loc[False])),\n",
    "                \"E_Threshold\":E_threshold,\n",
    "                # \"Overlap_Threshold\":non_overlapping_incorrects_threshold,\n",
    "                \"acceptance_rate\":Accepted.shape[0]/(Predictions.shape[0]),\n",
    "                \"accepted_correct\":Accepted.loc[(Predictions['Acc'] == True)].shape[0],\n",
    "                \"accepted_incorrect\":Accepted.loc[(Predictions['Acc'] == False)].shape[0],\n",
    "                \"accepted_accuracy\":(Accepted.loc[(Accepted['Acc'] == True)].shape[0])/ Accepted.shape[0],\n",
    "                \"overlap_adjusted_accuracy\":(Accepted.loc[(Accepted['Acc'] == True)].count()[0] + Incorrects_overlap) / Predictions.loc[(Predictions[\"evidence\"] >E_threshold)].count()[0],\n",
    "                \"M(T) B(F)\":Accepted.loc[(Accepted[\"overlap\"] == 1)].count().iloc[0],\n",
    "                \"M(F) B(T)\":Accepted.loc[(Accepted[\"overlap\"] ==-1)].count().iloc[0],\n",
    "                \"M(F) B(F) overlap\":Incorrects_overlap,\n",
    "                },index=[i]))\n",
    "#         print(\"TT\",Accepted.loc[(Accepted[\"Acc\"] ==True) & (Accepted[\"overlap\"] == 0)])\n",
    "#         print(\"TF\",Accepted.loc[(Accepted[\"overlap\"] == 1)])\n",
    "#         print(\"FT\",Accepted.loc[(Accepted[\"overlap\"] == -1)])\n",
    "#         print(\"FF\",Accepted.loc[(Accepted[\"Acc\"] ==False) & (Accepted[\"overlap\"] == 0)])\n",
    "        axs[round(int(i/2)), round(i%2)]\n",
    "    # fig, axs = plt.subplots(1, 2)\n",
    "        # axs[round(int(i/2)), round(i%2)].suptitle('Horizontally stacked subplots')\n",
    "        axs[round(int(i/2)), round(i%2)].scatter(correct_rows['label'],correct_rows['evidence'],c ='r',marker='+')\n",
    "        axs[round(int(i/2)), round(i%2)].scatter(incorrect_rows['label']+.3,incorrect_rows['evidence'],c ='k',marker='x')\n",
    "        axs[round(int(i/2)), round(i%2)].plot(np.repeat(E_threshold,11),'b--')\n",
    "        axs[round(int(i/2)), round(i%2)].title.set_text(output_names[i])\n",
    "    \n",
    "    for ax in fig.axes:\n",
    "            # ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "            axLine, axLabel = ax.get_legend_handles_labels()\n",
    "            lines=(axLine)\n",
    "            labels=(axLabel)\n",
    "    fig.text(0.5, 0.01, 'Items Exit at Branch', ha='center', va='center')\n",
    "    fig.text(0.01, 0.5, 'Accuracy %', ha='center', va='center', rotation='vertical')\n",
    "    # fig.legend(lines, labels,bbox_to_anchor=(1., 1), loc=2,borderaxespad=0.,frameon=True)\n",
    "    # fig.set_size_inches(10, 10)\n",
    "    plt.show()\n",
    "    return Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs 4\n",
      "[array([[5.5684535e-07, 4.6966561e-06, 5.3179706e-06, 3.1496069e-04,\n",
      "        1.4501819e-05, 1.4229490e-04, 9.9950600e-01, 9.4235902e-06,\n",
      "        1.9849954e-07, 2.0549053e-06]], dtype=float32), array([[0.00193528, 0.01514434, 0.09984089, 0.4260399 , 0.07646628,\n",
      "        0.03612326, 0.32816356, 0.01460762, 0.00069581, 0.00098305]],\n",
      "      dtype=float32), array([[1.5724291e-03, 3.7399880e-03, 1.3275781e-01, 2.7834660e-01,\n",
      "        4.7946125e-02, 4.1528028e-02, 4.8741278e-01, 5.9362501e-03,\n",
      "        2.0601177e-04, 5.5394945e-04]], dtype=float32), array([[3.1160820e-05, 8.0869514e-03, 2.6975173e-04, 3.8660962e-02,\n",
      "        2.8688402e-04, 3.4446423e-03, 9.3835872e-01, 6.1817761e-03,\n",
      "        2.5252098e-06, 4.6766205e-03]], dtype=float32)]\n",
      "[array([[1.4774101e-06, 2.1898386e-05, 5.6386234e-05, 1.0813593e-06,\n",
      "        3.3908036e-06, 3.4003176e-06, 3.0622210e-05, 3.7369569e-05,\n",
      "        1.1058371e-06, 9.9984324e-01]], dtype=float32), array([[3.0495101e-03, 9.4021958e-01, 5.3408188e-05, 1.4042790e-03,\n",
      "        3.4931571e-05, 4.4608681e-05, 5.7946960e-05, 3.2042728e-05,\n",
      "        1.3130379e-02, 4.1973237e-02]], dtype=float32), array([[9.3630265e-04, 9.8008597e-01, 3.4087018e-06, 1.1418985e-04,\n",
      "        1.7050443e-06, 3.5680428e-06, 1.1129403e-05, 7.0376859e-06,\n",
      "        5.0417532e-04, 1.8332412e-02]], dtype=float32), array([[3.4241136e-08, 5.0329636e-06, 4.1699127e-11, 1.3161286e-08,\n",
      "        8.4172998e-12, 1.0092411e-09, 3.1313519e-07, 8.3455030e-09,\n",
      "        6.7070536e-09, 9.9999464e-01]], dtype=float32)]\n",
      "[array([[2.6602203e-05, 4.0485446e-07, 2.6658481e-07, 4.7386500e-08,\n",
      "        1.3971693e-08, 1.2261793e-07, 2.3530727e-06, 1.8094015e-06,\n",
      "        1.5839699e-07, 9.9996829e-01]], dtype=float32), array([[1.9837444e-01, 4.2420998e-01, 3.8499606e-03, 5.4773535e-03,\n",
      "        4.2294958e-03, 4.0510140e-04, 8.2496144e-03, 3.9705527e-03,\n",
      "        1.3439655e-01, 2.1683697e-01]], dtype=float32), array([[2.0753486e-01, 4.8326787e-01, 1.6784094e-03, 3.7803364e-04,\n",
      "        1.3879886e-03, 8.5515770e-05, 3.8574364e-03, 2.4467385e-03,\n",
      "        8.8028209e-03, 2.9056031e-01]], dtype=float32), array([[8.07060424e-05, 3.23493732e-05, 2.04392208e-07, 8.71063548e-06,\n",
      "        1.67648193e-06, 5.35265769e-07, 8.76135236e-05, 4.86673998e-05,\n",
      "        1.05364115e-05, 9.99728978e-01]], dtype=float32)]\n",
      "[array([[3.7690888e-06, 3.0345424e-07, 1.6587819e-07, 8.5294278e-07,\n",
      "        9.9999118e-01, 5.0181001e-07, 9.8061480e-07, 5.1077512e-07,\n",
      "        1.0913290e-06, 5.9877618e-07]], dtype=float32), array([[8.8745798e-04, 7.9356483e-04, 6.0812909e-02, 3.6732297e-02,\n",
      "        8.0536056e-01, 7.4936785e-03, 6.3549563e-02, 2.2107806e-02,\n",
      "        7.0078153e-04, 1.5614345e-03]], dtype=float32), array([[6.54235424e-04, 3.05800961e-04, 4.54581603e-02, 1.22058326e-02,\n",
      "        8.56313229e-01, 6.04708074e-03, 6.05547465e-02, 1.72384214e-02,\n",
      "        1.11774425e-04, 1.11071626e-03]], dtype=float32), array([[6.3572383e-09, 2.1151884e-12, 4.7108828e-09, 1.1041553e-06,\n",
      "        9.9999309e-01, 4.1485009e-08, 1.2899690e-08, 5.7848797e-06,\n",
      "        9.4801104e-11, 5.7740737e-09]], dtype=float32)]\n",
      "[array([[5.3572317e-06, 9.9997687e-01, 5.5197556e-06, 7.5813801e-08,\n",
      "        5.8375008e-08, 2.2744318e-06, 4.2299098e-07, 8.1404903e-07,\n",
      "        4.5766228e-06, 4.0463656e-06]], dtype=float32), array([[5.3833774e-04, 9.9821150e-01, 4.9449687e-07, 7.7293407e-06,\n",
      "        3.3020359e-07, 1.2369568e-07, 2.1593280e-06, 2.6514797e-07,\n",
      "        1.5235196e-04, 1.0867460e-03]], dtype=float32), array([[7.8957264e-06, 9.9956852e-01, 2.3172381e-10, 5.8417651e-08,\n",
      "        2.1934806e-11, 1.1247964e-09, 4.2410284e-09, 8.8454322e-10,\n",
      "        2.1587382e-06, 4.2145254e-04]], dtype=float32), array([[1.9244038e-07, 9.9996245e-01, 3.8429047e-07, 1.4101674e-08,\n",
      "        1.4647297e-11, 2.0852158e-06, 3.0444921e-06, 1.3352452e-05,\n",
      "        8.5972829e-07, 1.7571263e-05]], dtype=float32)]\n",
      "prediction: 4999 of 5000\r"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "K= 10 # number of classes\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels,10)\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels,10)\n",
    "shuffle_size = 22500\n",
    "batch_size=1\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "def augment_images(image, label,input_size=(224,224), channel_first = False):\n",
    "            image = tf.image.resize(image,input_size)\n",
    "            if channel_first:\n",
    "                image = tf.transpose(image, [2, 0, 1])\n",
    "            return image, label\n",
    "test_ds_size = len(list(test_ds))\n",
    "test_ds = (test_ds.map(augment_images))\n",
    "t_target = tf.data.Dataset.from_tensor_slices((test_labels))\n",
    "test_ds = tf.data.Dataset.zip((test_ds,t_target))\n",
    "test_ds = (test_ds.batch(batch_size=1, drop_remainder=True))\n",
    "\n",
    "validation_size = 5000\n",
    "validation_images, validation_labels = train_images[:validation_size], train_labels[:validation_size] #get the first 5k training samples as validation set\n",
    "shuffle_size = 22500\n",
    "batch_size=1\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "validation_ds_size = len(list(validation_ds))\n",
    "validation_ds = (validation_ds.map(augment_images))\n",
    "v_target = tf.data.Dataset.from_tensor_slices((validation_labels))\n",
    "validation_ds = tf.data.Dataset.zip((validation_ds,v_target))\n",
    "validation_ds = (validation_ds.batch(batch_size=1, drop_remainder=True))\n",
    "\n",
    "\n",
    "validation_Outputs = collectEvidence_branches(model,validation_ds, evidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs 4\n",
      "[3.8278400897979736, 2.1615138053894043, 0.7436450719833374, 0.9130330681800842, 0.009648040868341923, 1.0, 1.0, 0.0, 1.0, 0.004500063136219978, 0.04500063136219978, 0.0, 0.0017666143830865622, 0.0, 0.01766614429652691, 0.040615107864141464, 0.40615108609199524, 0.0]\n",
      "[2.349536657333374, 2.1611812114715576, 0.16609691083431244, 0.02175804041326046, 0.0005003453698009253, 1.0, 1.0, 1.0, 1.0, 0.04471367597579956, 0.4471367597579956, 0.0, 0.05418643355369568, 0.5418643355369568, 0.0, 0.3364811837673187, 3.364811897277832, 0.0]\n",
      "[2.2055184841156006, 2.1611809730529785, 0.04156159982085228, 0.0025523852091282606, 0.00022373080719262362, 1.0, 1.0, 1.0, 1.0, 0.042857151478528976, 0.42857152223587036, 0.0, 0.11905646324157715, 1.1905646324157715, 0.0, 0.15414826571941376, 1.54148268699646, 0.0]\n",
      "[4.686412334442139, 2.1612510681152344, 0.9760501384735107, 1.544885277748108, 0.004225746728479862, 1.0, 0.0, 0.0, 1.0, 0.00574686611071229, 0.0, 0.057468660175800323, 0.0043826173059642315, 0.0, 0.04382617399096489, 0.04075207933783531, 0.4075208008289337, 0.0]\n",
      "[2.2005934715270996, 2.161133289337158, 0.016285192221403122, 0.02285274863243103, 0.00032240914879366755, 1.0, 1.0, 1.0, 1.0, 0.049255214631557465, 0.49255213141441345, 0.0, 0.012774916365742683, 0.12774915993213654, 0.0, 0.16000743210315704, 1.600074291229248, 0.0]\n",
      "prediction: 9999 of 10000\r"
     ]
    }
   ],
   "source": [
    "# (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# # print(y_train)\n",
    "# K= 10 # number of classes\n",
    "# test_labels = tf.keras.utils.to_categorical(test_labels,10)\n",
    "# shuffle_size = 22500\n",
    "# batch_size=1\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "\n",
    "# def augment_images(image, label,input_size=(224,224), channel_first = False):\n",
    "#             image = tf.image.resize(image,input_size)\n",
    "#             if channel_first:\n",
    "#                 image = tf.transpose(image, [2, 0, 1])\n",
    "#             return image, label\n",
    "# test_ds_size = len(list(test_ds))\n",
    "# test_ds = (test_ds.map(augment_images))\n",
    "# t_target = tf.data.Dataset.from_tensor_slices((test_labels))\n",
    "# test_ds = tf.data.Dataset.zip((test_ds,t_target))\n",
    "# test_ds = (test_ds.batch(batch_size=1, drop_remainder=True))\n",
    "# # Predictions = collectEvidence_branches(model,test_ds)\n",
    "# Outputs = collectEvidence_branches(model,test_ds, evidence=False)\n",
    "# # print(Outputs)\n",
    "test_Outputs = collectEvidence_branches(model,test_ds, evidence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      label  evidence  Acc  overlap\n",
      "0         6         0  1.0      0.0\n",
      "1         9         0  1.0      0.0\n",
      "2         9         0  1.0      0.0\n",
      "3         4         0  1.0      0.0\n",
      "4         1         0  1.0      0.0\n",
      "5         1         0  1.0      0.0\n",
      "6         2         0  1.0      0.0\n",
      "7         7         0  1.0      0.0\n",
      "8         8         0  1.0      0.0\n",
      "9         3         0  1.0      0.0\n",
      "10        4         0  1.0      0.0\n",
      "11        7         0  1.0      0.0\n",
      "12        7         0  1.0      0.0\n",
      "13        2         0  1.0      0.0\n",
      "14        9         0  1.0      0.0\n",
      "15        9         0  1.0      0.0\n",
      "16        9         0  1.0      0.0\n",
      "17        3         0  0.0      0.0\n",
      "18        2         0  1.0      0.0\n",
      "19        6         0  0.0      0.0\n",
      "20        4         0  1.0      0.0\n",
      "21        3         0  1.0      0.0\n",
      "22        6         0  1.0      0.0\n",
      "23        6         0  1.0      0.0\n",
      "24        2         0  1.0      0.0\n",
      "25        6         0  1.0      0.0\n",
      "26        3         0  1.0      0.0\n",
      "27        5         0  0.0      0.0\n",
      "28        4         0  1.0      0.0\n",
      "29        0         0  1.0      0.0\n",
      "...     ...       ...  ...      ...\n",
      "4970      9         0  1.0      0.0\n",
      "4971      2         0  1.0      0.0\n",
      "4972      8         0  0.0      0.0\n",
      "4973      4         0  0.0      0.0\n",
      "4974      8         0  1.0      0.0\n",
      "4975      8         0  1.0      0.0\n",
      "4976      2         0  1.0      0.0\n",
      "4977      1         0  1.0      0.0\n",
      "4978      4         0  1.0      0.0\n",
      "4979      8         0  1.0      0.0\n",
      "4980      6         0  1.0      0.0\n",
      "4981      7         0  1.0      0.0\n",
      "4982      3         0  1.0      0.0\n",
      "4983      1         0  1.0      0.0\n",
      "4984      3         0  1.0      0.0\n",
      "4985      4         0  1.0      0.0\n",
      "4986      9         0  1.0      0.0\n",
      "4987      4         0  0.0      0.0\n",
      "4988      8         0  1.0      0.0\n",
      "4989      4         0  1.0      0.0\n",
      "4990      5         0  1.0      0.0\n",
      "4991      0         0  1.0      0.0\n",
      "4992      9         0  1.0      0.0\n",
      "4993      1         0  1.0      0.0\n",
      "4994      3         0  1.0      0.0\n",
      "4995      8         0  1.0      0.0\n",
      "4996      7         0  1.0      0.0\n",
      "4997      5         0  1.0      0.0\n",
      "4998      4         0  1.0      0.0\n",
      "4999      6         0  1.0      0.0\n",
      "\n",
      "[5000 rows x 4 columns],       label  evidence  Acc  overlap\n",
      "0         6  0.023063  1.0      0.0\n",
      "1         9  0.015931  0.0      1.0\n",
      "2         9  0.003655  0.0      1.0\n",
      "3         4  0.008445  1.0      0.0\n",
      "4         1  0.116842  1.0      0.0\n",
      "5         1  0.013419  1.0      0.0\n",
      "6         2  0.005573  1.0      0.0\n",
      "7         7  0.051110  1.0      0.0\n",
      "8         8  0.005470  0.0      1.0\n",
      "9         3  0.001102  1.0      0.0\n",
      "10        4  0.005114  1.0      0.0\n",
      "11        7  0.019020  1.0      0.0\n",
      "12        7  0.020087  1.0      0.0\n",
      "13        2  0.001633  1.0      0.0\n",
      "14        9  0.003992  1.0      0.0\n",
      "15        9  0.024025  1.0      0.0\n",
      "16        9  0.017852  1.0      0.0\n",
      "17        3  0.006399  1.0     -1.0\n",
      "18        2  0.007063  1.0      0.0\n",
      "19        6  0.012174  0.0      0.0\n",
      "20        4  0.002150  0.0      1.0\n",
      "21        3  0.108341  0.0      1.0\n",
      "22        6  0.077460  1.0      0.0\n",
      "23        6  0.113525  1.0      0.0\n",
      "24        2  0.035643  0.0      1.0\n",
      "25        6  0.007738  1.0      0.0\n",
      "26        3  0.008695  1.0      0.0\n",
      "27        5  0.008807  0.0      0.0\n",
      "28        4  0.004735  1.0      0.0\n",
      "29        0  0.004276  0.0      1.0\n",
      "...     ...       ...  ...      ...\n",
      "4970      9  0.009102  0.0      1.0\n",
      "4971      2  0.002886  1.0      0.0\n",
      "4972      8  0.044152  0.0      0.0\n",
      "4973      4  0.008479  0.0      0.0\n",
      "4974      8  0.005181  1.0      0.0\n",
      "4975      8  0.024254  1.0      0.0\n",
      "4976      2  0.005327  1.0      0.0\n",
      "4977      1  0.006496  1.0      0.0\n",
      "4978      4  0.005063  0.0      1.0\n",
      "4979      8  0.079362  1.0      0.0\n",
      "4980      6  0.021845  1.0      0.0\n",
      "4981      7  0.004804  0.0      1.0\n",
      "4982      3  0.011525  0.0      1.0\n",
      "4983      1  0.080464  1.0      0.0\n",
      "4984      3  0.001345  1.0      0.0\n",
      "4985      4  0.000958  0.0      1.0\n",
      "4986      9  0.003109  1.0      0.0\n",
      "4987      4  0.020522  0.0      0.0\n",
      "4988      8  0.069668  1.0      0.0\n",
      "4989      4  0.009601  1.0      0.0\n",
      "4990      5  0.059901  1.0      0.0\n",
      "4991      0  0.015134  0.0      1.0\n",
      "4992      9  0.030163  1.0      0.0\n",
      "4993      1  0.188674  1.0      0.0\n",
      "4994      3  0.021290  0.0      1.0\n",
      "4995      8  0.021056  1.0      0.0\n",
      "4996      7  0.003161  0.0      1.0\n",
      "4997      5  0.004254  0.0      1.0\n",
      "4998      4  0.009524  1.0      0.0\n",
      "4999      6  0.083319  1.0      0.0\n",
      "\n",
      "[5000 rows x 4 columns],       label  evidence  Acc  overlap\n",
      "0         6  0.026424  1.0      0.0\n",
      "1         9  0.006753  0.0      1.0\n",
      "2         9  0.002504  0.0      1.0\n",
      "3         4  0.005098  1.0      0.0\n",
      "4         1  0.220072  1.0      0.0\n",
      "5         1  0.020718  1.0      0.0\n",
      "6         2  0.003101  1.0      0.0\n",
      "7         7  0.031881  1.0      0.0\n",
      "8         8  0.012258  1.0      0.0\n",
      "9         3  0.000427  1.0      0.0\n",
      "10        4  0.003769  1.0      0.0\n",
      "11        7  0.006154  1.0      0.0\n",
      "12        7  0.012641  1.0      0.0\n",
      "13        2  0.001234  1.0      0.0\n",
      "14        9  0.008947  1.0      0.0\n",
      "15        9  0.048700  1.0      0.0\n",
      "16        9  0.007916  1.0      0.0\n",
      "17        3  0.002899  1.0     -1.0\n",
      "18        2  0.003211  1.0      0.0\n",
      "19        6  0.001503  0.0      0.0\n",
      "20        4  0.001251  1.0      0.0\n",
      "21        3  0.000216  1.0      0.0\n",
      "22        6  0.037946  1.0      0.0\n",
      "23        6  0.051903  1.0      0.0\n",
      "24        2  0.006230  0.0      1.0\n",
      "25        6  0.002229  1.0      0.0\n",
      "26        3  0.004507  1.0      0.0\n",
      "27        5  0.002318  0.0      0.0\n",
      "28        4  0.001707  1.0      0.0\n",
      "29        0  0.001089  0.0      1.0\n",
      "...     ...       ...  ...      ...\n",
      "4970      9  0.003049  0.0      1.0\n",
      "4971      2  0.001486  1.0      0.0\n",
      "4972      8  0.031500  0.0      0.0\n",
      "4973      4  0.003432  0.0      0.0\n",
      "4974      8  0.004064  1.0      0.0\n",
      "4975      8  0.048642  1.0      0.0\n",
      "4976      2  0.002603  1.0      0.0\n",
      "4977      1  0.005776  1.0      0.0\n",
      "4978      4  0.003326  1.0      0.0\n",
      "4979      8  0.191457  1.0      0.0\n",
      "4980      6  0.005843  1.0      0.0\n",
      "4981      7  0.002848  0.0      1.0\n",
      "4982      3  0.001057  1.0      0.0\n",
      "4983      1  0.093281  1.0      0.0\n",
      "4984      3  0.000928  0.0      1.0\n",
      "4985      4  0.001396  0.0      1.0\n",
      "4986      9  0.001218  1.0      0.0\n",
      "4987      4  0.012550  0.0      0.0\n",
      "4988      8  0.228593  1.0      0.0\n",
      "4989      4  0.004738  1.0      0.0\n",
      "4990      5  0.032502  1.0      0.0\n",
      "4991      0  0.005169  0.0      1.0\n",
      "4992      9  0.037958  1.0      0.0\n",
      "4993      1  0.228017  1.0      0.0\n",
      "4994      3  0.022021  0.0      1.0\n",
      "4995      8  0.012915  1.0      0.0\n",
      "4996      7  0.000950  0.0      1.0\n",
      "4997      5  0.001062  0.0      1.0\n",
      "4998      4  0.005531  1.0      0.0\n",
      "4999      6  0.050585  1.0      0.0\n",
      "\n",
      "[5000 rows x 4 columns],       label  evidence  Acc  overlap\n",
      "0         6  0.193162  1.0      0.0\n",
      "1         9  0.127964  1.0      0.0\n",
      "2         9  0.311889  1.0      0.0\n",
      "3         4  0.363909  1.0      0.0\n",
      "4         1  0.461766  1.0      0.0\n",
      "5         1  0.275224  1.0      0.0\n",
      "6         2  0.361236  1.0      0.0\n",
      "7         7  0.476218  1.0      0.0\n",
      "8         8  0.234325  1.0      0.0\n",
      "9         3  0.196553  1.0      0.0\n",
      "10        4  0.269072  1.0      0.0\n",
      "11        7  0.064079  1.0      0.0\n",
      "12        7  0.014034  1.0      0.0\n",
      "13        2  0.099289  1.0      0.0\n",
      "14        9  0.225170  1.0      0.0\n",
      "15        9  0.204665  1.0      0.0\n",
      "16        9  0.014706  1.0      0.0\n",
      "17        3  0.127956  0.0      0.0\n",
      "18        2  0.225297  1.0      0.0\n",
      "19        6  0.004274  0.0      0.0\n",
      "20        4  0.358755  1.0      0.0\n",
      "21        3  0.001857  0.0      1.0\n",
      "22        6  0.276718  1.0      0.0\n",
      "23        6  0.221599  1.0      0.0\n",
      "24        2  0.815383  1.0      0.0\n",
      "25        6  0.000033  1.0      0.0\n",
      "26        3  0.043892  1.0      0.0\n",
      "27        5  0.000961  1.0     -1.0\n",
      "28        4  0.205234  1.0      0.0\n",
      "29        0  0.010737  1.0      0.0\n",
      "...     ...       ...  ...      ...\n",
      "4970      9  0.042785  1.0      0.0\n",
      "4971      2  0.012996  1.0      0.0\n",
      "4972      8  0.000752  0.0      0.0\n",
      "4973      4  0.000067  0.0      0.0\n",
      "4974      8  0.159554  1.0      0.0\n",
      "4975      8  0.548161  1.0      0.0\n",
      "4976      2  0.324061  1.0      0.0\n",
      "4977      1  0.047693  1.0      0.0\n",
      "4978      4  0.013435  1.0      0.0\n",
      "4979      8  0.615193  1.0      0.0\n",
      "4980      6  0.011986  1.0      0.0\n",
      "4981      7  0.082076  1.0      0.0\n",
      "4982      3  0.000454  1.0      0.0\n",
      "4983      1  0.282609  1.0      0.0\n",
      "4984      3  0.001548  1.0      0.0\n",
      "4985      4  0.005102  1.0      0.0\n",
      "4986      9  0.029168  1.0      0.0\n",
      "4987      4  0.013390  0.0      0.0\n",
      "4988      8  0.799241  1.0      0.0\n",
      "4989      4  0.511263  1.0      0.0\n",
      "4990      5  0.567692  1.0      0.0\n",
      "4991      0  0.058470  1.0      0.0\n",
      "4992      9  0.506891  1.0      0.0\n",
      "4993      1  0.429840  1.0      0.0\n",
      "4994      3  0.035118  1.0      0.0\n",
      "4995      8  0.637654  1.0      0.0\n",
      "4996      7  0.000315  1.0      0.0\n",
      "4997      5  0.020549  1.0      0.0\n",
      "4998      4  0.004930  1.0      0.0\n",
      "4999      6  0.118792  1.0      0.0\n",
      "\n",
      "[5000 rows x 4 columns]]\n"
     ]
    }
   ],
   "source": [
    "print(validation_Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.008826165158586549  std 0.009926359594913487\n",
      "rollover enabled, 3417 predictions provided\n",
      "mean 0.0033567113425893615  std 0.0034878633028697383\n",
      "rollover enabled, 2515 predictions provided\n",
      "mean 0.009153881367855315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  std 0.02159220240737837\n",
      "rollover enabled, 1116 predictions provided\n",
      "mean 0  std 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:308: RuntimeWarning: divide by zero encountered in longlong_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Branch Name</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>E_Threshold</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>accepted_correct</th>\n",
       "      <th>accepted_incorrect</th>\n",
       "      <th>accepted_accuracy</th>\n",
       "      <th>overlap_adjusted_accuracy</th>\n",
       "      <th>M(T) B(F)</th>\n",
       "      <th>M(F) B(T)</th>\n",
       "      <th>M(F) B(F) overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>branch_1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.7264</td>\n",
       "      <td>0.726400</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.316600</td>\n",
       "      <td>1449</td>\n",
       "      <td>134</td>\n",
       "      <td>0.915351</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>124</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>branch_2</td>\n",
       "      <td>3417</td>\n",
       "      <td>0.7514</td>\n",
       "      <td>0.671349</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.263974</td>\n",
       "      <td>764</td>\n",
       "      <td>138</td>\n",
       "      <td>0.847007</td>\n",
       "      <td>0.858093</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>branch_3</td>\n",
       "      <td>2515</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.897813</td>\n",
       "      <td>0.030746</td>\n",
       "      <td>0.556262</td>\n",
       "      <td>1364</td>\n",
       "      <td>35</td>\n",
       "      <td>0.974982</td>\n",
       "      <td>0.989993</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Main_Exit</td>\n",
       "      <td>1116</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>0.906810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1012</td>\n",
       "      <td>104</td>\n",
       "      <td>0.906810</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Branch Name  Predictions  test_accuracy  Accuracy  E_Threshold  \\\n",
       "0    branch_1         5000         0.7264  0.726400     0.018753   \n",
       "1    branch_2         3417         0.7514  0.671349     0.006845   \n",
       "2    branch_3         2515         0.9352  0.897813     0.030746   \n",
       "3   Main_Exit         1116         0.9692  0.906810     0.000000   \n",
       "\n",
       "   acceptance_rate  accepted_correct  accepted_incorrect  accepted_accuracy  \\\n",
       "0         0.316600              1449                 134           0.915351   \n",
       "1         0.263974               764                 138           0.847007   \n",
       "2         0.556262              1364                  35           0.974982   \n",
       "3         1.000000              1012                 104           0.906810   \n",
       "\n",
       "   overlap_adjusted_accuracy  M(T) B(F)  M(F) B(T)  M(F) B(F) overlap  \n",
       "0                   0.921668        124          3                 10  \n",
       "1                   0.858093        128          5                 10  \n",
       "2                   0.989993         14          1                 21  \n",
       "3                        inf          0          0                104  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displayEvidence_cascade(validation_Outputs, Evidence = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.008858946200560366  std 0.010717257193091079\n",
      "rollover enabled, 6726 predictions provided\n",
      "mean 0.0033306535034468357  std 0.0037912910896845944\n",
      "rollover enabled, 5032 predictions provided\n",
      "mean 0.010589637787124727  std 0.031884579800933975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollover enabled, 2279 predictions provided\n",
      "mean 0  std 0.0\n",
      "threshold not supplied for branch 3, using test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:308: RuntimeWarning: divide by zero encountered in longlong_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAEvCAYAAAAdGSXnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsnXl4FFXWh3+dzgZJIAQEHJggoIQlAgEH3CABxAVBQBEiHwHFEWQNARmEURIhE6KjrIKiKCqMEkRBBEWMAoHIIoGwSBKQXQQSiIEsZOu+3x/dVXRV19Kd3qqT8z5PP8mtW6fvqdt169Q5d9MxxhgIgiAIwkvw8bQCBEEQBGEPZLgIgiAIr4IMF0EQBOFVkOEiCIIgvAoyXARBEIRXQYaLIAiC8CrIcBEEQRBeBRkujbJ//34MHDjQrWV+/fXXGD9+fI3llyxZgnnz5jlRI4JwDG9qR7t378bTTz+NwYMHY+jQodizZ48LtKsd+HpaAcL7uXLlClJSUpCRkYGnn37a0+oQhNdRXFyMV155BWvXrsU999yD3NxcjBo1Cjt37kRwcLCn1dMcZLg0TFlZGaZOnYrz58+jQYMGmDdvHlauXImioiJcvHgRMTExGDZsGObNm4fS0lIUFBSgffv2WLx4MQICAnDvvfdi3LhxyMzMRH5+Pv75z39i5MiRAICVK1di48aN8PX1RatWrZCamgoAKCgowLhx43D58mXo9Xq88847aNu2raKeGzZsQI8ePdC2bVvcuHHD5fVCEPbgDe2oqqoKiYmJuOeeewAAd999Nxhj+Ouvv8hwScEITbJv3z7Wvn17lpWVxRhjbN26dWzYsGFs1qxZbMyYMfx5qampbNOmTYwxxiorK9nAgQPZtm3bGGOMtWvXjq1Zs4YxxtixY8dYZGQkKy8vZ+np6ezRRx9lRUVFjDHGUlJS2IoVK9hXX33F7rvvPnbu3DnGGGPz589ns2fPtlnnpUuXsjfeeMPhaycIZ+GN7Ygxxt555x329NNPO3TttRnq49IwERER6NatGwBg6NChOH78OIqLi9G9e3f+nJkzZyIsLAwffvghkpKSkJ+fj7KyMj6/X79+AIBOnTqhsrISZWVl2Lt3Lx5//HE0bNgQADB79mxMmDABANC5c2e0atUKANChQwcUFha65VoJwlV4Uzuqrq5GcnIytm3bhmXLljl+8bUUChVqGB8f4XuFTqeDr68v6tevzx+bPn06DAYDnnjiCcTExODy5ctgFusmBwQE8LIAwBiDXq/n0wBw8+ZN3Lx5EwDg63v7ltDpdILvIghvxFva0Y0bNzB16lQwxpCWloZGjRrV4GrrBuRxaZi8vDzk5OQAANLS0tC9e3fUq1dPcM6ePXswadIkDBgwAABw5MgRGAwGxe998MEH8eOPP6KkpAQAsGzZMnzyySfOvwCC0ADe0I4MBgPGjRuHli1b4uOPPyajpQJ5XBqmTZs2ePfdd3Hx4kU0btwYqampVuGDhIQETJo0CfXr10dwcDD+8Y9/4MKFC4rfGx0djd9//x3PPfccAFNH8Pz587F9+3aXXQtBeApvaEfff/89srOzUVZWhmeeeYY//tZbbyEiIsLu76vt6BjFggiCIAgvgjwuQpWUlBTs379fMm/27Nm4//773awRQXgf1I6cB3lcBEEQhFdBgzMIgiAIr8IrQoVZWVmeVoEgbMJybpAWobZEeAtKbckrDBegfBE5OTno0KGDG7WxHa3qplW9AO3qpqaXtxgFb2xLWtUL0K5uWtULcLwtUaiQIAiC8CrIcBEEQRBeBRkugiAIwqsgw0UQBEF4FS43XEajEXPnzsWIESMQFxeH8+fPS57zz3/+E1988YWr1SEIgiC8HJcbrvT0dFRWViItLQ0zZszgN1qzZPHixbQBIUEQBGETLjdcWVlZ6NWrFwCga9euOH78uCB/27Zt0Ol06N27t6tVIQiCIGoBLp/HVVJSIth6Wq/Xo7q6Gr6+vjh58iS2bNmCpUuXYvny5Yrfw21LIEV5eblivhzhY8YAAC58+qndsrZSU91cjVb1ArSrm1b1Ioi6hssNV3BwMEpLS/m00WjkN1nbtGkTrl69ijFjxuDSpUvw8/NDixYtJL0vpclqNZ5oZ95IzpWT9LQ6CVCregHa1a22TEAmCG/H5YarW7du2LFjBwYMGIDs7Gy0a9eOz/vXv/7F/79s2TI0adLEPSHDmBjT3127hOmdO11fNkEQBOEQLjdc/fv3R2ZmJmJjY8EYQ0pKClavXo3w8HD069fP1cUTBEEQYrz8Zd3lhsvHxwfz5s0THGvbtq3VeVOmTHG1KgRBEEQtwGsW2SUIgiAcpJZ0k9RNw8X9SF76oxEEQdRl6qbh4sjO9rQGBEEQ7qOWvLTXbcPVtaunNSAIgiDspG4arloS5yUIgqgRXv6so9XhCYIgCK+ibnpctSTOSxAEURchj4sgCILwKshwEQRBEF4FGS6CIIi6RkzM7a4SL6Ru9nHRqEKCIAivpW4aLoIgiLpILXlpJ8NFEF6C0WhEUlIS8vLy4O/vj+TkZLRq1YrP/+STT7B161YAQHR0NCZPnuwpVQnCpZDhIggvIT09HZWVlUhLS0N2djZSU1Px3nvvAQAuXryIzZs348svv4ROp8PIkSPxyCOPoH379h7WmtAUtWQqEBkugvASsrKy0KtXLwBA165dcfz4cT6vefPmWLVqFfR6PQCguroaAQEBHtGTIFyNyw2XWnjjf//7H77++mvodDpMmjQJffr0cbVKwJ49ymmC0CAlJSUIDg7m03q9HtXV1fD19YWfnx/CwsLAGMNbb72Fjh07onXr1pLfk5OTI1tGeXm5Yr6n0KpegHZ1U9TL7KnDQ3o7WmcuN1xK4Y3CwkJ8/vnn2LRpEyoqKvDkk08iJiYGOp3OtUoZDMppgtAgwcHBKC0t5dNGoxG+vrebcEVFBebMmYOgoCAkJibKfk+HDh1k83JychTzPYVW9QK0q5tW9QLUdcvKylKUd/k8LqXwRlhYGL755hv4+fnh2rVraNCggeuNFgDo9aaPXJogNEi3bt2QkZEBAMjOzka7du34PMYYJk6ciIiICMybN48PGRJEbcTlHpdSeAMAfH19sXbtWixbtgxxcXGy3+PM8EZ4t24AgKBffwUAlJrTF1zgNntlGMHDaFU3T+vVv39/ZGZmIjY2FowxpKSkYPXq1QgPD4fRaMSBAwdQWVmJ3bt3AwCmT5+OqKgoj+lLEK7C5YZLLbwBAKNGjcLw4cPx0ksvYd++fbj//vutvsep4Y0DB0x/zd5dkDntCqdaq+66VvUCtKubo+ENR/Hx8cG8efMEx9q2bcv/f+zYMZeWTxBaweWhQqXwxpkzZzB58mQwxuDn5wd/f3/4+LhhFarQUNNHLk0QBEFoFpd7XErhjX79+qF9+/YYMWIEdDodevXqhR49erhaJYIgCMKLcbnhUgtvTJ482f0z/IuKTH+5kCWXJgiCIDRP3ZyAzM0a54bBe/kscoIgCLvw8mcebWtCEARBeBV10+Pi3jK4UKGXvnUQBEHYRS1ZHZ48LoIgCMKrqJseFzf0nevj4tI0SIMgiNpMLVkdnjwugiAIwquomx5XSYlymiAIgtAsddNw0erwBEEQXkvdNFwEQRB1ERpVSBAEQRDuhwwXQRAE4VXUzVAht8ke17dFm+4RBFEXyM5WTnsJddNw0eAMgiDqIl27mv5yfVxc2suom4aLIAiiLkIeF0EQBOFV3LihnPYSXG64jEYjkpKSkJeXB39/fyQnJ6NVq1Z8/ieffIKtW7cCAKKjo92/NxdhP146hJYg6jy1pH/f5aMK09PTUVlZibS0NMyYMQOpqal83sWLF7F582asW7cOaWlp2LNnD3Jzc12tEkEQRN0kONj0kUt7CS73uLKystCrVy8AQNeuXXH8+HE+r3nz5li1ahX0ZqtfXV2NgIAAV6tE1JRaMnmRIAjvxuWGq6SkBMEWFl2v16O6uhq+vr7w8/NDWFgYGGN466230LFjR7Ru3Vrye3JycmTLKC8vV8wX0978V2f+y8x/c+34DluxVzd3URO9wsvKAABB5nSpOX3ByddXm+qMIAjn43LDFRwcjNLSUj5tNBrh63u72IqKCsyZMwdBQUFITEyU/Z4OHTrI5uXk5Cjmq8EZMEe+Qw5HdXMVNdLrwAHTX/PvF2ROO/vqvLXOsrKy3KgNQdSAWjIc3uV9XN26dUNGRgYAIDs7G+3atePzGGOYOHEiIiIiMG/ePD5kSBCEZ2GMKaYJLyU7WzgEXpz2ElzucfXv3x+ZmZmIjY0FYwwpKSlYvXo1wsPDYTQaceDAAVRWVmL37t0AgOnTpyMqKsrVahE1gTbgrBMkJSWhqKgIixYtgk6nA2MMCQkJCA0NRVJSkqfVIwjXGy4fHx/MmzdPcKxt27b8/8eOHXO1CgRB2AhjDEVFRViyZAkAYNGiRUhISMCSJUsQHx8Pxhh0Op3KtxCapZbsRUgTkAnb4Twrro+SPK1ah06nw6JFiwAAS5Ys4Q1YfHw874ERXkwtWe6OVocn7Mdg8MwNHxNzewh+HcRoNGLu3LkYMWIE4uLicP78eatzCgsL8eijj6KioqLG5VgaLw4yWrUEvV446Vic9hLIcDmRWt+hLTYcXm5IvO33UprMDwC7d+/G2LFjce3aNYfK4fq0LElISNB8/RB1BzJcTiIpKUnQuLnG/+6773pYMyeya9ftYbRSaVfBGUiuPCcYTLnfS8uDD5Qm8wOm/uTVq1cjlBs0UwO4euD6tIxGI+Lj47FkyRIyXoRmoD4uJ6DUoR0XF0cd2hrDWwcgKE3mB4CHHnrIpu9RmkRdUVGBqqoqxMXFYdy4ccjNzcW4ceNQWFiIqqoqjy3JpuXJ31rVTUovdy6+oISjdUaGywkodWiPGzdOkw9Br4JbUoqrRweXmPLWAQhqk/ltRW0y//Lly62M96effurRetHqpHRAu7pJ6iXqm9aZ0+7W39HJ/BQqdBLUoe1CXNC35o2/l9Jkfmcjrgct1wvhZjTQt02Gy0kwxjBt2jTBsWnTplGfgDNwVt+aRYPzxgEI/fv3h7+/P2JjY7FgwQLMnj0bq1evxk8//eRp1QjCrVCo0AkwxvDAAw9g//79mDp1KhYvXoxp06Zh6dKl2LlzJ7Kzs7X1xhoTY1owl1t7sK5gXtpGPADBso8L0K7npTaZn+Pnn392l0qEtxEdbfrLvfhxaVvQ0O4QZLiI2g/XwMy7ver69EHouXOCPi0ubBgaGqpJo0UQTkEcqXDHqGAXQIbLCeh0Ouzdu5f3spYuXQoAmDp1KsaPH6+dB6HFG1OQZbq276clXkQ0OxtJAJiFZ8UZL838VgShNbjnBDfdwoPPDerjchI6nQ6LFy8WHFu8eDE9CJ1Bw4amj1y6htAABA2hgQ5/wnsgj8tJyHX2jxs3zkMaScC9IcXEoLSsDEHe4mmZQ3yyaTVqycKimqaueO91GVHInfq4vBylzv7CwkKPz4GxYtcu1Pe0Du5EZmFR8VwlrU48rtVoqMOf8B7IcDkBnU6H0NBQyc7+qqoqehhqkCQARQkJtOeUo5DhITyAyw2X0WhEUlIS8vLy4O/vj+TkZLRq1UpwTmFhIWJjY/Htt98iICDA1Sq5hKSkJBiNRkFn/8KFC5GXl+dhzSywWGVBZ5murnafDp54sHH9YeYQB2vQAEXl5V635FOtxCJ8LUgT2kNikJOncPngDHetaM3hqRW/k5KSMH36dMGirdOnT9fWIrvi7UjMaW9bJd1uSkoE/Vq60lIsqq7mF4/18fERhHnJaBGEBKJ2ZJV2Iy43XO5Y0ZojKSlJsFoFt5qFq0M/lou2cqsvcG/wxcXF2jEEEnvxJOl07lkl3QUrvNuMhMHWGY14++23Bae9/fbbZLSchN0vQzt3krdF2IzLQ4XuWNG6vLwcJ06cwMaNG3H06FEUFhZi9uzZWLBgAdauXYvOnTtj+PDh/EPJFSskV1ZWIiIiQrBoa0REBAIDAz22oraY9uaHN3fdRoMBfwFYah5E8uqrryI1NRVr1qxBXFwcTpw4IXiQO1Jv4WVlAGCaPwag1Jy+ICErXjna0d9LSv4uAFct7kvAtIhtWFiY7BJKWl0F3KNIhPqSkpKo/7A2wrUXblShqP24E5cbLnetaN2+fXvExMTg6NGjWLt2LdauXcvnx8TEoEOHDrJv09zRmq6QzBjDoUOHrPqz8vLyEBAQgPbt27v0Tb6mo+N8ACwGoDOHzNasWQPA9lXS7ao3bnkps2cdZE5LSaqtHO3o72UAkA/TFh6BgYEoLi5GSEgIysvL8ddff+Gee+6RvEcdXdG6LuCtW8YQNccTo3NdHip014rW3ATgqVOnCo5zawe6uiJ79uwpebxz584uLdfRDRF1gHtWSedCgzdumD4enHDqC2AmgMDAQJSXl8PPzw/l5eUIDAzEzJkza/RiVSuIiUH4mDEOfQU3opb6D92ABiZtJ5WXe2RDVpcbLlrR2jZqMkBCqW+tqKjItu8AvG6VdGfgA2Ds2LGCY2PHjoWPDy0m4yjeuGVMjQkNRTuZl1aXk53t3pF9osEYrLgYRZWVDj1/agzzAg4ePKiYf+LECWY0GtnUqVMZTM9iwWfq1KnMaDTeFgCkPw4QHh7OAgMDBeUGBgayO++8U1U2MTGRTZkyhdfRaDSyKVOmsMTERFVZo9HI4uPjBeXGx8cLr5dDdL1GgMWLZLjvkvwOR+otOtr04WS4tAQnTpxwXrky1/0PifsEAPvHP/4hXXdSeolQu0+1gKSOEr+NoVcvwSkGg0H6CyVkjb17235P2ohi3SvcSy6nYUNWHRLi3jJtbEuSdeZIW5KQM5p/W3t/a0fbUq0yXD179hQYKs6Q9ezZ0zWGy3zDVFdXsyZNmkg+CBs1asSqq6tlv8JoNDJ/f38GgDdeU6ZMYQCYv7+/TY29urpaUKZseRLXnCi60TjjJWk0NWa4xHUjW1ciuSqABcoYrsDAQFZVVWWbXiJqi+GKbtiQdQ0K4o2VwWBgXbt2ZdFSv5dI1ti7N4tv0cL2lyHx98igWPd6venjTho2NH24+4pL20NNDa6NZbvDcHHt0LIN2fLMcrQt1Zpgvk6nQ2BgILp27SpYvSIjIwOBgYGuCVOY3XQfHx+0adNGci5ay5YtFcNPBoOB123ZsmVYtmwZn6fT6WAwGBT7XKKjo3HixAnBsebNm6Njx47YZcOWBUmAe1ZJd/LkxSQAf02bxvdfMvPUh0aNGqnG130BGAFejkOn09V48JBXY7Hqd1V1NfLq1cOVK1fQvXt3ZGVloXv37sjOzkbz5s1hNBoV72edTodQX1/3bBnD9e9wUx28bRLznj2e1sBhGKS7GlweGrbN1HoWResbHc1KzOEd8Zud7Jueox6XhPfQKiCAhYWFCd48wsLCbAoV9u7dW/Ltv3fv3opy1dXVzNfXlwFgTZo0EXh+vr6+1p6Xo9ftgrc1KdQ8LiPAekIYBpb1riXkDQDzkfG4fHx8ZMNitdbj4tDrmcHHh/f4xR/LcDaPo96wI2EvztPiZJ3geTlbb6fL2iFvj8dl03VLtEO7uhrUdLOgTnlc3Jud5Vwql4xmEnkLxkOHUFBRgbKKCsHxwsJCBAYGKr6lGo1G3JBZ7fzGjRuKsjqdDmFhYcjPz8e1a9cEnkJYWFjt7Aw30xPAfkCw/xkgP7rTEgNMHpcURqMRBoOhbg3SsPBcfAAc+fRTNPH1xTWLpcCaNGmCI0eO2HxPuWXLGJl5RYzVbHh2UlISioqKbJt/JhFBYIzBshTZcsWelr2el6PyIpJQs3U7dQCyYVpYYuHChfwyd7t27XL9ru+KZk0j2NqhzKKjmcFgELwlSr49O9njqnroIaaXeYPX6/WyfSaMmd5s/Pz8JGX9/PxU48UGg4F16tRJINepUyfXXLdGPC7ubU88GMdqEI6MPHlcIiz6TAwA85OpGz8/P+u6cZbXY3E/iMswGAzKdW8hm5iYKLgPOG9cbaCTXREbxqz6mRIDAli8RZ+0Yl+xo3Um0R4MorYkW2eOeE0a8rhqleGaGx7OunTpImhsXbp0YXPnzhXKOvkBbpRp6NxH6Qe8deuWouytW7cUVQkPD2d6vd7KWIaHh6vqXVcNVzXAfGXqWzLEKqeXCK81XBZ1U6lyL1dWVsrK1uieEj3EowHWBRAMDOnSpQu77777rGUlBob0DAkR3AuKIWTL72B2jtC10NsIsHidrsZGz+6BHaK6jgZYV1Gdde3aVbrOZNqSTdctITvX/IxVfeaKqLuGi8Nc8QaDgTVr1oyvOO6GB8CaNWsmfIuTaWyOjFKrqccl9hDFH9lhyIyxyspKpjM3GPFHp9PZ/JCp6XV7ynAZAdZCpr5atGih2uCqoexx1WXDVaVwLwKwvpdlflcpr0kSSxmANbN4+Fm24caNG1t/h8R9ESKjd0hIiPJLjZmqqirl65Upey5MBtemB7gTPS4DTEYLAOvatStvtACw9u3bq9aZ5e+l+txxRn2bcbQt1ZpAvk6nQ5l5/bsjR45Ar9fjyJEjAICysjLVeGsSwC9JAwCMMcTHx9s0A1wHU7+JFJajBqVgjMn2p/j4+PD6WBETA5++fWXzlb7XkiTAIzPfOYxGo2JaUgZAofn/xo0bo7q6Go0bNwZg6ldU+w5b+riImhMDICoqiv8djEYjoqKiECO1ykPDhvy2MzoAZebD4jZ869Yt1TZstJAXU1ZWJrwvxKtOxMSgVUAA6tWrJ5CrV6+e1TZMYhiAHwAcER0/cuQIfvjhB+s2KrNLQ03wAdAAQCBMKxPp9XpkZ2cjMDAQwcHBNj0DEmH6vSyJiopCYmKiopwRgNyGSNXV1Ta15RqjaNY0gqT1Fb21GHQ6/m1N/FHzuCzf4MVzqWx5g7c7vMKYYA6YkqzsnKzoaFb98MOKHpfaqEK3TkCWkIsGWGRkpCDEERkZaR3ikJD9O0zekeU1+/j4sL///e+qZRep/F5FRUWSl1AXPK5bKnVjFboWv7lbnNu5c2dmMBhY586d5d/kLWSrVcpWu58NKvJKzwBLT9PX15dVVVXxI3YBZU/TCGUP3tZ2JC7DFm/PALDmMmU3adJE1eOy7NMU/15WfZqO1LeIuhsqFFWiEcr9FkqdjJbutvjDud9KZdfIcJlly8vLFWXLy8uFchZxfSOUQ5S2NBilBqdW55wOltgaZrS86TnjFRkZadNDRqmxNm/eXPX3KlX5vUpLSyUvgQyXuuFypI/M0bILVeQLCwtlZR0pu0JFtqKiQlFvBrCGMC04wBmrqqoq5u/vzxpK9X2J2p9S2baEzZXkBS8LIlm7r9sCChWaqYay21qtsMuvD4DrMG25Yoler8f169dV3e1bKrrdumVxhihEUX7//Yqy5eXlsnmVUA5RVlZWKn53JZRDZmrySQCmAYIwo637n1nqffz4cej1esFebUrhOh0AuaCRTqdTDSnpFXOt74O6hPIvDtV7gqnIc/eKFH+pyP71l/IZaoEppdDVFRXZK1fkzyiUzTHnFyqfUQ3gBkx1GxQUhOrqagQFBaGyshI3btxQfHapbeNY4sKNHqvU8qvUzqg5tcZwOXLTGgBchPXD0mAw4OLFi6p9HmqzFQQPUm4jRTNVKitIWP34FvJqUXE1vR15SDEA2wAsBfjNO6dNm4alS5di27Ztig8owPHfK18mLz8/X/W65V8FzPkKLwu1HUcfhI4YLrnflM/PVz5D3MdklX9E/gxHDJej95Pl3VpZWQk/Pz9B21O6nwtUyi4oUD7DEeOjtme9s3a1l6LWTECuUMuvqEBAQIBkni0PUaW38CIV+aKiIoSEhEjm2dJYmzRpUuNy69evL5vv6BsT9wgSTwJWM1qAfCc6n19WJvt76aHsaap5TLZ4yA3NAwa0hNFoRFJSEvLy8uDv74/k5GTBwIH169dj3bp18PX1xYQJE9CnTx+7y3gMOySOrgfwHoB6eOwxf5jHwZjZgefxCZ7Hp7iGxhiCDRLy75m/oyX69dNDuKLWDszAOxiELZiBdgBWSsgnA/gJ06Z9AsYWCWQBIAVz8CD2YiUeAJAiIT8NwBEsX56HefP6CmQBYCXG4yucBDAQwAwJ+Th89dVXOH/+frz3HqzkB2MYTDGbMQCet5LeunUHJk16AStWAOvXC2UB4Etwv9MMsw6W3MLNmzdxxx13YP58wLSpxm3587gOYJg5lQLgAYH0Y48V4NSpNqZamAZki8pujpMAxptTKwEIt52aMKECq1cHAgBGYQ3+QEs+rxgAsBfAHPORDQBu3xxPPx2GIUOA1183pZ94AuCCT7frsWbUCsMVgx0Sbz23GxvwHR591B+BgVyesLENVWlsffv6QPgsvN3Y8tAO/6fQ2IAuGDq0kcVmocLGtlelse3YocfEicKyAVNjM6g0tqKiIuze/TfJxrYBw1Ch0NiAASgrK8MXXzQyNzah/A70gWkrSOvGduDAbdMg1dga4zreUGhswB+4dOkSGjVqJNnYWqs0tsmTq7F8uR8AYNQo4A+R/L0KjQ0AFi4Mwltvmf53ZmNzlPT0dFRWViItLQ3Z2dlITU3Fe2alCgoKsGbNGnz11VeoqKjAyJEj8dBDD8Hf39+uMtSjB64L0oSp5Yc1wvXr8vlXVeTz8/Mh907jSPjYkegBYAoTKubfuIE77rhDMk+hOgCoe1xqd4e/v59sntqd4OPjupUzaoXhAuT7t/j86iqYBo3WBGUPwpYbL1hmm+sTkkdvk5ubCyBCMu9HFdkff/wRzZt3lP9um8puIZlnS5hRzmMCgMMq8ocPH0ZkZKRknlpoxtSnKN/gHA2xeoqsrCz06tULgGmZHcs+waNHjyIqKgr+/v7w9/dHeHg4cnNz7d7IdC/6QH5D9lvYuzcAQUEWh3S3vbomuI6f0Qfyv/of+PlnIwS21EJ+GU5aeB/WrFgxHs2aScsCwNfYi8YK8t98MwVhnHUUyb4M4E1sAbBFUvbll1/GXXcBI0ZYy18B8AoA4FPzR8izz14GAEycaPqIy7797HrH/BFy112myMfrr5u9Fwv5UsDi95oDMZcu3Q7tLl4MYImwbAOANXxqPMSsWHFbu7WIE+S2Jw81AAAgAElEQVSVAQgSHBkmSO3aVQrLgM/339/+PyfHqii7cLnhckd4Yyf6YA2A0ZK5twD0QXz8Z4iLM1e8qLGtQx8LB1jMH1i37ipatLB4gFvIR+AkQhQaC3AEISGDsHPnIStZADiEvYCC/PHji7Fz52DJsucCgEJj++WXX5CWFi/Z2EyaAXKNDTD1CUyf3u+2x2chf57/T7qxXblyDq1atZJsbAAwgP/PurEBwBdfPIG4uDjJxnYAwDo+Zd3YXn55P4AeAIC1awH8z1r+tvMkbGwAMGLEbXlnNjZHKSkpEbwA6fV6VFdXw9fXFyUlJYJwdFBQkGx/VI7oQtqb/+pgehAqcfDgQTRt2lRSFlD3Pk6cOCF4obGUV/OYdu7cKTDE4rKPqsivX78e0dHRkrKXVWR37NiB+y0GUlnKq3VTZGdnC5554rLVXrqPHz/uUJ21adNGtmy1F9CjR48i0ByqEsuqeXuZmZlo2VL6yVpeXm51H9qDyw2XO8IbABCulh8uf4aaHxYYqHzGCCh7ECN4y2FNTwA7FWSVFo0dBFMwUzZ/0CCFXKCTYi7QqZP8GUGyOeb8IOUzWinmQnHSZyMV2UaNlM9orJgLfjKz1ggODkZp6W3TYrkFizivtLRUtl+1Q4cOwgNcn6ROh0MqOpSXlwvlLWQBoOrmTaBBA1n5tm3bCvWykJ+nUvaXX34pbEuisr+dPh1YuFBW/tSpU3j55ZclZb+ZNQt4801Z2by8PLzwwguSZatFPi5duoTHH39cVu/i/HzA4mVATIsWLYShQgv5/6qUvWXLFv55K1X2lTNnAAvDJiY0NBStW7eWlC3IygK6d5eVbdy4sfW9ZiYnJ0c2DzBFF5Rw+ahCW8MbISEhfHijJrRWy28tf0Y92Rxzfj3lM55UkX/ySfkz5E2aOV/B6MnfMuZ8hZsKAKQDcRb5MqE6wPTwlwtCduzYUfXhP0Wl7ClT5M+Q947N+TJveRyOGj5P0a1bN2RkZAAwvcW3a3e7b69z587IyspCRUUFiouLcfr0aUG+rYjfZNPT04X5KnuViaeOiL0+paklfxOlxffA3/4mPkNIUZFwuNLYsWMV8y0Rv5yKV41QenkV9+R89NFHwnyV6Rni3SFOnTqlmG+JeF14cZvdo7JyvHgQ0nVRJ6LSICXxy6n42a328uoILve43BHeANTngOzdu5d/IxXLqo2uO3bsGBpYvEWK5Q+oyH/22Wf821pNwhuWjcZS/mcV2eXLl2PSpEmSsgBwWkV+69at/EuHWL4S8v1zJ06cwJEjR2TDG4D60OW0tDTExsZKyqqFR3bt2qUYmlHrI8vOzsadd95pddzR8Iaj9O/fH5mZmYiNjQVjDCkpKVi9ejXCw8PRr18/xMXFYeTIkWDMtGyXUh+jJIyhRU4O0NH0SpKeno5+/fohPT0djzzyCACgX79+srKA0BMvKSnh2zT3DJB9mDGGpQCWmR/yU6ZM4UeqcpurWo5clSr7IwAff/wxAJPR4gwId0xsUCxlkwC88cYbAExGi5uLyB2TnZvIGB60qLOPPvqIN5gvvvgir4uS3ndbHDp16hTuvvtunDp1Cvfcc48p/+67JYRN8sdw2zBGRkbi2LFjuPfee3kH4dixY4plWw6IuX79OsLCwnD9+nX+xTMsTGLIjFnWsuc9NzcXERERyM3NRfv2phYXESHdN+8UFKcnO4GUlBS2detWPt2rVy/+//T0dMGy/xMnTmRHjx61+g5bFtldt26d4izudevWScoygF2+fFlR9vLly7JlM4D16tVLUd7ymsWyzzzzjKLsM888I1v2GJWZ62PGjFGss5UrVyrKr1y5Ulb+vErZ58+fV6yzIUOGKMoPGTJEVvbXX39VlP31118Vyz5x4oSivNys/lq7coYF3DWmp6cLjovTapSUlCimlZgyZYpVWq3uLRk7dqxiWgnxNiRq26EwdrvOPvroI8FxcVqNU6dOKaaViIyMtErbU2fXr19XTCuRm5urmJZC8ytnuCO8AVi/lRw8eFAx3xJxKFAcUlALFb700kuC9GeffaaYb4k4jMi9HcrlW/J/ovT27duF+f8nPkNIly5dBOl9+/Yp5lsifm8WTzZUCxOI49uzZ89WzLckNDRUkBaHVsT5avJ//vmnXfJ1AbFnJetpySD+/e0JG4k9K1lPSwaxZyXpackg9qzsWWxa7FnJeloyiJ9RSs8sMWLPStbTkkHsWUl6WjKIPSuXeloc6vbUMQwGA3v99dfZiBEj2PDhw9nvv//OPv74Y/4NLi0tjT399NNs6NChbNu2bZLfYetbIsxvzNz5Bw8e5I+pwZ3HLbBaVFRks6yl/GeffcYYY+yzzz6zu+yPP/6YMcbYxx9/bLfs9u3bGWOMbd++3SZZcZ3t27ePMcbYvn377C772rVrjDHGrl27VqM6mz17NmOMsdmzZ9tdNvdWeurUqRqV/eeffzLGGPvzzz9V5euSx6U1tKoXY9rVTat6MebGRXbPnTvHcnJybD3dqdjT2MTn2vMwEa8KLrdKuByc0bJM23rzcEZLLq0EZ7Tk0lJY6sUZLbm0EpzRkkurwRkty7StdeZIaIUxxhstubQYMlyeQ6t6MaZd3bSqF2OOtyUdY+rr83z00Uf4888/odPpUFBQgCVLljji5NlNVlaW4gg5taGVnkSrumlVL0C7utkyhFdtJKen8da2pFW9AO3qplW9AMfbkmwf16pVq/iFHs+fP48pU6Zg6tSpVv0BBEEQBOFOZIfDR0VFYebMmXj00UcxevRozJ8/H+Xl5YiPj3enfgRBEAQhQNZwde/eHd27d8e3336Ld999F3FxcZoPgxAEQRC1H9lQ4cmTJ/Gf//wHv//+O2bOnImsrCzMmTMHFy9edKd+BEEQBCFA1nDNnTsXzzzzDHr37o3Fixdj3LhxmDFjBj79VHpBVoIgCIJwB7KjCseMGYN+/fqhrKwM165dw2uvveZu3XjUFlwkCK2g9XA6tSXCW1BqS7KGq6ysDJmZmahfvz4efPBB1YUiCYIgCMId2DSPiyAIgiC0gsvXKiQIgiAIZ6K6rUlVVRX8/OS3QXcn7thNuSZUVVVhzpw5uHTpEiorKzFhwgTBoqSrV6/Ghg0b+IUr33jjDcGupK5myJAh/PYxLVu2xIIFC/g8T9XZ119/jY0bNwIAKioqkJOTg8zMTH77mOTkZBw6dIhfnHXFihWyGyM6kyNHjuDtt9/GmjVrcP78ebz66qvQ6XS45557kJiYKNhPqry8HDNnzsT169cRFBSEN998067FST2FVtsRoO22pMV2BGizLbm8HamtKTVw4ECWnJzM8vLybFuEyoX88MMPbNasWYwxxg4fPsxefvllPi8/P58NHDiQVVRUsJs3b/L/u4MNGzaw5ORkxhhjhYWFLDo6WpA/Y8YMduzYMbfoIqa8vJwNHjxYMs+TdWZJUlKS1bYzsbGxdm2t4Aw++OADNnDgQPbss88yxhgbP348v27j66+/brUG5Mcff8yWLl3KGGNsy5YtbP78+W7Vt6ZotR0xpt225A3tiDFttCV3tCPVUOE333yDhx9+mJ+E/OWXXwq2CHcn7tpN2V4ef/xxwYoier1ekP/bb7/hgw8+wHPPPYeVK1e6RSeO3Nxc3Lp1C2PHjsXo0aORnZ3N53myzjiOHTuG33//XbDTs9FoxPnz5zF37lzExsZiw4YNbtElPDyc37QQMP1uPXr0AAD07t0bv/zyi+B8y/uxd+/e2Lt3r1v0dBSttiNAu21J6+0I0E5bckc7Ug0V+vj4oHfv3gCADRs2YM2aNfjqq68wdOhQxW3lXYGzdlN2NpwLXlJSgqlTp2LatGmC/CeffBIjR45EcHAwJk+ejB07drgtlBAYGIgXX3wRzz77LM6dO4eXXnoJ27Zt83idcaxcuVKwSzNgGtE6atQovPDCCzAYDBg9ejQiIyP5nVVdxWOPPYY//viDTzPG+NG0QUFBKC4uFpxvWX9S+VpFq+2IK4/TUUttSevtCNBOW3JHO1L1uN566y08/vjjSE9Px0svvYTNmzfj888/xxdffGHXxTiD4OBggbdnNBrh6+srmVdaWuqWPhGOy5cvY/To0Rg8eDAGDRrEH2eMYcyYMQgLC4O/vz+io6Nx4oTcpvfOp3Xr1njqqaeg0+nQunVrhIaGoqCgAIDn6+zmzZs4c+YM7r//fsHxevXqYfTo0ahXrx6Cg4Nx//33e+QN1jIOX1payvcZcFjWn1S+VtFyOwK02Za03I4AbbclV7QjVcN11113YePGjZg/fz6/DL2Pjw/effddu5R3Bu7aTdlerl27hrFjx2LmzJkYNmyYIK+kpAQDBw5EaWkpGGPYv38/IiMj3aIXYPKSU1NTAQBXr15FSUkJ7rjjDgCerTMA+PXXX/Hggw9aHT937hxGjhwJg8GAqqoqHDp0CJ06dXKbXhwdO3bE/v37AQAZGRm47777BPndunXDrl27+HytTz7m0Go7ArTblrTcjgBttyVXtCPVeVxpaWk4ffo05syZg7Fjx+Kpp57CkCFDanoNDsGNhjp58iQYY0hJSUFGRgbCw8PRr18/rF+/HmlpaWCMYfz48XjsscfcoldycjK+//57weimZ599Frdu3cKIESOwadMmrFmzBv7+/njggQcwdepUt+gFAJWVlZg9eza/n9orr7yCI0eOeLzOANPWOb6+vnj++ecBmEaMcXp9+OGH2LZtG/z8/DB48GA899xzbtHpjz/+wPTp07F+/XqcPXsWr7/+OqqqqtCmTRskJydDr9dj7NixeP/992EwGDBr1iwUFBTAz88P77zzDv8w0zJabUeAdtuSltsRoL225Op2pGq4hg4dinXr1iEgIABVVVUYNWoU0tLSnHqRBEEQBGErqqFCHx8fBAQEAAD8/Pxo6SeCIAjCo6iOKuzXrx9GjhyJzp0747fffkPfvn3doRdBEARBSGLTWoU5OTk4e/Ys2rRp4/IhyQRBEAShhGqo8Pz588jIyMCZM2eQnp6OuXPnukOvOs/+/fsxcOBAt5b59ddfY/z48XbLff/993jqqacwaNAgjB49GufOnXO+cgRhJ3/88QciIiIwatQoq7xXX30VERERKCwslJVfsmQJNm3aVOPyly1bhvvvvx+DBw8WfN5++21FuX//+9/8JN3XXntNMEGcMKEaKpw1axb69OmDQ4cOoWnTpigrK3OHXoSXUFBQgMTERGzevBnNmzfH2rVrMX/+fHz00UeeVo0gEBAQgLNnz+LSpUto0aIFANOk3EOHDqnKWq7gUVMGDBhg98v+f/7zH/7/X375xe0LPXgDqh5XYGAgxo8fj2bNmiE1NRXXrl1zh14ETA1s6tSpGDx4MOLi4nD27Fm8+uqrePnll/Hkk0/iv//9L86ePYsXXngBw4cPR58+fTBhwgRUVFQAAO69914sW7YMsbGx6Nu3Lz7//HP+u1euXInHH38cAwcOxKRJk/jZ6gUFBRg3bhwGDRqEIUOG4PTp04o63nHHHcjMzETz5s1RXV2NS5cuITQ01HWVQhB2oNfr8cQTT+Dbb7/lj23fvp1fuJcxhuTkZDz77LMYMGAAnnjiCX6zzVdffZV/AVNqSzWhvLwcTz75JP73v/8BAL788ksMGjQIt27dQlxcHLZt24ZFixYhPz+fH3pP3EbVcDHGUFBQgLKyMpSVleHGjRvu0IuAaQWB559/Ht988w0GDhyIf/3rXwBMN/3WrVsxc+ZMrF+/HkOGDMH69euxfft2/PHHH9i5cycA09yTRo0aYd26dVi6dCkWLFiAiooK/PTTT/j666+RlpaGLVu2oGXLlli7di0A4OLFi/j3v/+Nb7/9Fvfdd59NnpOfnx+OHTuG6OhorF+/XjI0QxCeYsiQIfjmm2/49KZNmzB06FAAwNmzZ5Gfn4+0tDR89913GDp0KD788EOr75BrS2p89913VqHC3bt3IzAwEAsXLsTSpUuxa9cuLF68GEuWLEG9evV42YSEBDRt2hRvv/02unTp4oSaqD2ohgonT56M9PR0PPXUU+jXr5/HJh/XRSIiItCtWzcApvl0SUlJaNq0qWBm+cyZM5GZmYkPP/wQ586dQ35+viCcy71ZdurUCZWVlSgrK8PevXvx+OOPo2HDhgCA2bNnAzD1cXXu3Jnf4qJDhw748ccfbdL13nvvRWZmJjIyMjB+/Hikp6d7zRJIRO0mMjISer0ex48fR+PGjVFaWsqvbNGmTRtMmzYN69atw8WLF7F//35+vUQxUm2Jmyokh1KoMCIiApMnT8b48eORmprq1q2OvB1Vw3X06FG8+OKLACDYF4dwPZZrfAGATqeDr68v6tevzx+bPn06DAYDnnjiCcTExODy5cuwHCjKNSxu/h1jDHq9XjAf7+bNm7h58yYA8GvWcTJqg06vXr2KkydPClZ3Dg4OxoULF9y6tBVBKPHUU09h8+bNCAsLw+DBg/nju3btwooVK/DCCy+gX79+aNOmDTZv3iz5HVJtyVFOnTqFJk2a4MiRI+QU2IFqqHDXrl0wGAzu0IUQkZeXh5ycHACmpbe6d+8uCCUAwJ49ezBp0iQMGDAAgGkDN7Xf68EHH8SPP/7Ir2C9bNkyfPLJJzXSsbKyEtOnT8f58+cBAPv27UN1dTXatm1bo+8jCFcwePBgbNu2Dd99951gtO6xY8fQp08fjBw5EpGRkUhPT3fb82779u3Yv38/Nm/ejMzMTKSnp1udw63cTwhR9bj++usv9OrVCy1btoROp4NOp8O6devcoVudp02bNnj33Xdx8eJFNG7cGKmpqYJ9bgBTHHzSpEmoX78+goOD8Y9//AMXLlxQ/N7o6Gj8/vvv/Jpld999N+bPn4/t27fbrePf//53JCcnY8qUKdDpdGjQoAHef/99KwNLEJ6kWbNmaNu2LUJCQgSDhwYMGIDk5GQMGjQI1dXVeOihh7B9+3YYjUanlPvdd9/xgz047rzzTiQmJiIxMRHvv/8+wsLCkJqaikmTJllFKfr374+ZM2ciKSkJDz/8sFN0qg2oTkC+dOmS1TFuWClBEARBuBtVj2vjxo1WxyZPnuwSZQhtkpKSwm9LIGb27NlWewARRF1g3759WLBggWRez549MWfOHDdrVHdQ9bi4sCBjDCdOnIDRaBRMkCMIgiAId6LqccXGxgrS//znP12mDEEQBEGooWq4zp49y/9fUFCAy5cvu1QhKcSdmwShVbS+CzK1JcJbUGpLqoZr7ty5/HyewMBAfvUGd6N0ETk5OejQoYMbtbEdreqmVb0A7eqmppe3GAVvbEta1QvQrm5a1QtwvC2pGq5Vq1bh9OnT6NixI9LT0/Hggw/aryVBEARBOAnVCcgzZ87kF3jkFnklCIIgCE+hariuXr3KT1R96aWXkJ+f73KlCIIgCEIOVcMF3B6gceHCBdUZ5UeOHEFcXJzV8Z9//hnPPPMMRowYgfXr1wMwrXI+ZcoUjBw5Ei+99JLipm6EiJgY04cgCKKOodrHNWfOHEybNg3Xr19H06ZN8cYbb8ie++GHH2Lz5s1Wy/1UVVVhwYIF2LBhA+rVq4fnnnsOffr0wZYtW9CuXTtMmTIFW7duxYoVK/Daa685flUEQRBErUXV4+rQoQMWLFiAPXv2YOLEiWjfvr3sueHh4VZr6QHA6dOnER4ejoYNG8Lf3x/du3fHwYMHkZWVJVhVfO/evQ5cSh2B87R27TJ9yPMiCKKOoepxvfLKK3jggQfQsWNHnD17Ft9//z3eeecdyXMfe+wx/PHHH1bHS0pKEBISwqeDgoJQUlIiOB4UFMTvwisFt0q6FOXl5Yr5nsTZuoWb99ridgwqNacv2FlGXaozZ6FVvQiirqFquMSDM6T6r9QIDg5GaWkpny4tLUVISIjgeGlpqeLGg0pj/r15voLdHDhg+mv2soLMux3bW0KdqjMnUVvmcRGEt2PX4Izz58/XaLn/tm3b4vz58ygqKkJlZSUOHjyIqKgodOvWDbt27QIAZGRkaH7VAYIgCMLz2DU4IzAwEEOHDrX5y7/99luUlZVhxIgRePXVV/Hiiy+CMYZnnnkGzZo1w3PPPYdZs2bhueeeg5+fn2wIkpDA7GkRBEHUNVQNV5cuXTB//nysXbsWmZmZuH79uuL5LVu25Ie7Dxo0iD/et29f9O3bV3BuvXr1sHTp0proTRAEQdRRZA1XZWUltm7div/973/w9/dHSUkJfvrpJwQGBrpTP4IgCIIQINvH1bdvX+Tl5eHtt9/G559/jqZNm5LRIgiCIDyOrMc1evRobNmyBZcuXcKwYcOgst8kQRBOwmg0IikpCXl5efD390dycjJatWrF569fvx7r1q2Dr68vJkyYgD59+uDPP//EnDlzYDAYwBjDvHnz0KZNGw9eBUG4DlmPa9y4cdi8eTPi4uKwZcsWHD9+HP/9739x8uRJd+pHEHWO9PR0VFZWIi0tDTNmzEBqaiqfV1BQgDVr1mDdunX46KOPsHDhQlRWVmLJkiUYNWoU1qxZg/Hjx2PhwoUevAKCcC2qgzN69OiBHj164ObNm/jmm2/wr3/9C5s2bXKHboQS3GoZNLqw1mG5okzXrl1x/PhxPu/o0aOIioqCv78//P39ER4ejtzcXMyaNYufzG8wGBAQEOAR3QnCHagaLo4GDRogLi6uRhOQCYKwnZKSEgQHB/NpvV6P6upq+Pr6yq5CExYWBgA4c+YM3nzzTSxfvlz2+71xFRqt6gVoVzet6gU4rpvNhovQCJynZZ647XWeV2io6W9RkWf10DDilWaMRiN8fX0l87hVaABg3759eOONN/DWW28p9m954yo0WtUL0K5uWtULcHwVGptWziAIwn1069YNGRkZAIDs7Gy0a9eOz+vcuTOysrJQUVGB4uJinD59Gu3atcO+ffvwn//8B6tWrcK9997rKdUJwi2Qx+UInvB2uLI4z8XbPK0bN4Rp8rys6N+/PzIzMxEbGwvGGFJSUrB69WqEh4ejX79+iIuLw8iRI8EYQ0JCAgICApCSkoKqqip+h/LWrVtj3rx5Hr4SgnANZLgIQmP4+PhYGZ22bdvy/w8fPhzDhw8X5G/evNktuhGEFiDDVRM82c/ElcV5Lt7Sx8V5VuRpEQThINTHRRAEQXgV5HHVBE/2M3FleYunJYY8LYIgHIQMV03w1nAdR0yMaSdlblNKN5cNwHvqiiAIzUGGy1uhBz9BEHUUMlw1QQtD0mviuVgMKgmq6XfUFG+fOE0QhGZwquFSWtU6JycHKSkp/LnZ2dlYvnw5OnfujMcee4yfZPnII49gzJgxzlSLIAiCqEU41XBZrmqdnZ2N1NRUvPfeewBMy8ysWbMGAPD999+jadOm6N27N3755RcMHDgQr7/+ujNVcS2e7ONyxHOx8BQNRiP05O0QBOGFOHU4vNKq1hxlZWVYtmwZ/v3vfwMAjh8/jt9++w2jRo3C1KlTkZ+f70yVCIIgiFqGUz0upVWtOTZs2IDHH3+cX826TZs2iIyMxIMPPojNmzcjOTkZS5cutfpuLa1oHV5WBgCmfiIApeb0BQkdnK2bPWVbyZpDsEE3bkAPoLRHD5Psp586TT9ZzJ53u549AQAnzWm4oc6chVb1Ioi6hlMNl9Kq1hzffvutwDDdf//9qFevHgDTGm1SRgvQ2IrWos00g8xpKR2crlv9+sKyzWmbynBE1lG4kGZxsanMCRNMaYlwpVZXtXZ0RWuCIJyDU0OFSqtaA0BxcTEqKytx55138sdee+01/PDDDwCAvXv3olOnTs5Uqfaxc6fpEx1t+nBpgiCIOoJTPS61Va3Pnj2LFi1aCGRmzJiBOXPm4IsvvkC9evWQnJzsTJVcQ9eupr/cAAkurXWys5XTrkQLUwgIgqgVONVwqa1q3blzZ6xYsUKQ//e//50fbUi4mJIS5bQr8fbVRgiC0AzePwHZE8sXedJzIQiCqON4v+HyBJ4MFToyj4sb8cl5PRYjQF0Op59OJ0zbA3lpBEHAmw2XJ5cv8lY8GSrk+rbEaVotniAIO/Few+VJOG9HLk1Y44jRpHUOCYKwgAwX4R4eftj0lzM+XJogCMJOvNdw1dUBEo54e97ax+WszTPJUyOIWoH3Gi5P9tfo9aa/BoMwrXU8WWcc3lJXBEFoFu81XHUVxkx/Oc+FS9sCZ2jl0q6E83a4Mmvi/TjqaVEfGUHUCrzXcHky7OVJAyBa+5FPV1e7T4ea4IzQLhkcgiDgzYbL20OFNX0Ie9JoOoIn5745q4+MIAhN4L2Gq656XN6KIx4XhfoIgrDAew2Xt+LoQ9hbB4ZoATJ0BFEr8F7DpYURcp7Ak56mI3ArZHCDSuxZMYNCfQRBWOC9hstbqasPYe56xWl7rr+uzNUjCEIRMlzehrd6mrRMFkEQTsKpOyC7FYNBOChCnNY6Nd252NuvuybExJg+N26YPlyacBk3b95UTLtS/sqVK4ppNTZt2qSYVuLMmTOKaSWyRREBcVoNR+osPz9fMa1GiegFWJxWoqysTDHtCpzqcRmNRiQlJSEvLw/+/v5ITk5Gq1at+Pzk5GQcOnQIQUFBAIAVK1agqqoKr7zyCsrLy9G0aVMsWLAA9erVc6ZaBEfDhqa/XP8Yl9Z62c5a3stLwrNq7Wj9+vVYt24dfH19MWHCBPTp0weFhYWOtyOdDu0BmHshcePGDTRo0AA3b95EQ/PvxeQmvFtMiNeZ/6+JPFf25cuX0bx5c1y5cgV33nmn3WVv3LgRQ4YMwaZNmzB06FB5eQnZ06dPo02bNjhz5gy/Ea5S2ZZ1dvjwYXTt2hXZ2dmIioqyW29H6uzq1ato2rQp8vPz0axZM7vLLi4uRnBwMEpKShASEiIvLyFbWlqK+vXro6ysjH++y5btDJgT+eGHH9isWbMYY4wdPnyYvfzyy4L82NhYdv36dcGx+fPns6+++ooxxtjKlduUIX8AACAASURBVCvZ6tWrrb734MGD1oWZ1oyw/rgDO8o+ceKE9HdER5s+LizbCr3e9OFkuLQ7cKTOGjY0fTgZLm0vNa1zOb1ESN6nNUCpHeXn57OBAweyiooKdvPmTf5/W9qRqo4AKwIYLD43btywSsvJMpnzbZW/LCr78uXLVmmlsjdu3Cg4XyotJ3v69GnBuVJpubIPifQ+fPiwVdpVdXZVVPbVq1et0kplFxcXC86XSsvJlpaWCs6VSsvhaFtyqseVlZWFXr16AQC6du2K48eP83lGoxHnz5/H3Llzce3aNQwbNgzDhg1DVlYWxo8fDwDo3bs3Fi5ciOeff96ZahEcnpx/5sgwfkcnL3vZPDCldnT06FFERUXB398f/v7+CA8PR25urmPtiHuDBjAYO/AwgD3mdMOGhwBMAPAeLl++gaeeaiCU3bUTz+MTPA/gGhpjWMNDeBg7RPLDAazHb7/dxFNPhVjJz8A7GATgBtrhAazEXnPWnXfmAtgBIBk///xvXLnSHLGxQlkASMEDeBB70XToW+iEHfjNnG1ytHYAmIaNG5MQHDzkdoTZLAvswEqMR0TbtvgAAzEOMwAAbdteMMsCu3e3Rps2rZCWBrz3nrX8BgzDYVxHFMYAeB5RUUW8bPfu96FdO9PI3xUrgPXrhbIAsLNhQ9wA0BAzAAw015kp75FHHkaDBqbH9Pz5wE8/CeUb4zquYhiamWuiWbMTvGyXLl0xfXoo1q41nT1tGpC9RFh2u5DPUQzA9KusREjIQT7v4Yd74bXX9Fi82CQxSrcWf6Aln4+gA5iBFLyDOaZk0Pd8Xq9evTBggB79+gGvv246/YkngFu3TP/z9VhDnGq4SkpKEGwxPFuv16O6uhq+vr4oKyvDqFGj8MILL8BgMGD06NGIjIwUuKVBQUEoLi6W/O6cnBxBur35L9fkOKc0V3SeK7Cn7PLycoHu4WPGAACCfv0VAFDaowcA4MKnnzq9bGfKOkq7+vUBAHrz72swp0/aUGfcXd6+Y0cAQC5319uod7g55h5kTpea0xfsvG4rvVyEUjuybC+Aqc2UlJTY3I4A9bakB/AQgEyLc/bt24c///wTZWXCF476uH0f8foCiIrqhsOHDwnkr1y5grIyYbe6WN4fwAMAb7wA4L//fRuhoQxnzpxBWVkzgawlDEATAK1bt8bZs2f545MnT0FERAR++eU8ysqayMo2AxAZeS+OHz/GH4+MvBeVlYXIycnBpUshKCtrJCnfBUCS+cPRoUMH+PjokJubi3r1GK5caYSyshDJskMATJkyBcuW3dY7Kqqb+Z67CAAoKGiCsrL6VvJ3ALgCoLmlPl26wtdXjxs3biAn508AQGFhM4hhMLWLAwcOoEePw/zxbt26oaKiHIWFFcjJuWolx+EL4ODBg7jvvvssZLujvLzcrHMZcnKuAQBKSv6OigrTXeZwW1L0x+wkJSWFbd26lU/36tWL/7+6ulrgdr755pts48aNbMiQIezatWuMMcZycnLYuHHjrL63VoUKuXAVJ2Nv+MqRsJkn68wOvWXDCDXV10mhRneFCpXaUXp6OktMTOTTEydOZEePHrWpHanqCDCjRKiK+8iGrMyy3G/jiLw4PMh9fv75Z5vKFocHuY9kmFAkKw4Pch/ZMKFFnYnDg9xHNkzoxDoThwe5z86dO20qWxwe5D6SYUKRrDg8yH2UwoSMOd6WnDqqsFu3bsjIyABgGlHTrl07Pu/cuXMYOXIkDAYDqqqqcOjQIXTq1AndunXDLnMIJyMjA927d3emSrWPrl2FoTJxWqtwIwLl0kqEhpo+culahlI76ty5M7KyslBRUYHi4mKcPn0a7dq1c1o7ugnwgwIA02ABjoYNG6qOdLMcVGCv/BWAH4gBmAZocPTt21d1dKHlQAzANECDY+jQoYqjCy0HYgCmARocbdu2VRxdmA3wAzEA0wANjqioKNXRhY7UWT7AD8QATAM0OGJiYlRHF4o9eEtPPSQkRHF0oeVADMA0QIMjKCjItaMLFc2anRgMBvb666+zESNGsOHDh7Pff/+dffzxxyw9PZ0xxtgHH3zAnn76aTZixAj2+eefM8YYKygoYGPHjmUjRoxgL7/8sqSldpnH5YYBErYONDAajYLTxOmalO1UWUdxpM4cHVTiZR6XWjtKS0tjTz/9NBs6dCjbtm0bY8y2dmSLjidOnLB627f0BmzBEXnuPG4ghqUHZk/ZnIdl6YHZKst5WJYemBKWdcZ5WJYemD16O1Jn3EAMSw/MnrI5D8vSA7NVlrvfLD0wJRxtS256ajlGrTJcEqHCxFatWHx8PG+sjEYji4+PF4SEalK2U2Udxcl1Ztfv5qTRlO4yXK7EFsPFGLMKUSmGrCRwRF48evDy5cuqdW+JOCwoGyaUQBwWVAwTmuF0E4cFFcOEEjhSZ+LRg1evXrWrzsRhQdkwoQTilyS1MCFjGhtV6DV4cpSZKGzADh9GUXk5lixZAgBYtGgREhISsGTJEsTHx4Ox23MlANTNRXa534WrB3t/p4cfNv3lfm8uTcjSoEEDxbQr5Zs3b26V/uuvv2yWHzJkiGJaiTZt2iimlegqCtmL02o4UmdNmza1Sl+/ft1m+WDRmqfitBL169dXTLuCumm4NIROp8OiwEBgwgQsWbKEN2Dx8fFYtGiR0GgBtKVKTeAMHbfppkaHwRMEYRt103BxDy6ug9+dDzKJOUk6mDwtzmjBnLYyWnUVb931mSAIl+C9axV6K3v2mD4WabZ7NxISEgSnJSQkgDHmZuU0irPWZwwO9p5tYAiCkKVuGi5PLtoqeniyoCAk+PryfVpGoxHx8fFYsmQJGS+Ohg2FaxuK02pww+e537uWD6cniNpO3QwVehJRqFAXFYXQc+cQP2QIHx5ctGgRACA0NJTChYDjSz4RBFGrqJuGy9FRao4gsdJ5EgBm0afFGS8yWmYc/b243ZY5L8ue3ZcJgtAcddNwOWM3XicjNlJktCwQh/VqaoC8ZdNNgiAUqZuGy3JwhFTaldSGsJe7Db2zdn2m+VsEUSuom4aLGxzBrQnmzpFmngxTeiuOzl3zsm1NCIJQpm4aLi7ExBkPd/Z5OCvs5QnIABAEoQHqpuHyZB+XeEV0W1dIJ2qOxIAYgiC8l7ppuOhBVjM4w+5tnpaz+sgIgtAEddNwEd4FN9mY807tmXwM0CK7BFHLqJuGy5Mj+6KjhWVzaW/C3Z6Wox4TDYghiFpF3TRcnnyQ0UPUfpy1In5d2AKGIOoATjVcRqMRSUlJyMvLg7+/P5KTk9GqVSs+/5NPPsHWrVsBANHR0Zg8eTIYY+jduzfuuusuAKY9bGbMmOFMteTxxINMZqVzVlUlmHRstQ+XlvC2Pi5OX87geZv+BEEIcKrhSk9PR2VlJdLS0pCdnY3U1FS89957AICLFy9i8+bN+PLLL6HT6TBy5Eg88sgjqFevHjp16oT333/fmaooww1B5x5k7hySLuE9JAEoSkjgl3lijCEhIQGhoaFISkpyvU61HRqMQxC1CqeuDp+VlYVevXoBMHlOx48f5/OaN2+OVatWQa/Xw8fHB9XV1QgICMBvv/2Gq1evIi4uDi+99BLOnDnjTJW0h2hlc9agAYr8/QWrwXM7IBcVFdm0Orz4DJetKM+tor9rl+njrlX1HV0dvqRE2C8mThME4VU41eMqKSkRbPms1+tRXV0NX19f+Pn5ISwsDIwxvPXWW+jYsSNat26Na9euYdy4cXjiiSdw8OBBzJw5E1999ZXVd+fk5AjS7c1/uWAa96jOOXFCPeS2dy8AoF3PngCAk+Y0RGXIIVd2roR8eXm5QPfwdu0AAEG//goAKIuIQDJjuN6+vWAH5FGjRmHcuHHIzc1VLDsRQBGA8ebrZowhNTUVISEhmDx5co31liK8rMykuzldak5fsEHekTprbzYyvKw5bdNvDaC92cvl5c1pW69bTi+CIDyDUw1XcHAwSktL+bTRaISvRZ9ORUUF5syZg6CgICQmJgIAIiMjoTf3Nd133324evWq5AOoQ4cOimXrAFPI7YMPbA+5FRfb9N1qcJpKfU9OTo7wuNlgcQT9+iuSADR64AHB8UaNGmH9+vWKoUIG4AaApQB05utOSEjAmjVrEB8fj/bt2yv2kynpLcmBA6a/5tBqkDldk9qzq85E4VUdF16157e2sWwlrPQSkZWVZdf3EQRRM5waKuzWrRsyMjIAANnZ2Whn9i4A09vwxIkTERERgXnz5vHG6t1338Wnn34KAMjNzcXf/va3Gg1KYDB5Ho6E3DwBA7AKwLJlywTHly1bhlWrVinqrQOwCEA8TNft4+PDb0jpkm1RPLkBpwV2/9Z6vXAgjjhNEIRX4VSPq3///sjMzERsbCwYY0hJScHq1asRHh4Oo9GIAwcOoLKyErt37wYATJ8+HePGjcPMmTOxa9cu6PV6LFiwoEZlcw9xmHcP5kJukg9xmZF9qK6uUdkcYk/RFmNpBFAhk1dRUQGj0cgbeSm4615iccyb9vKqSZ3Z9VvbWba31BtB1GmYF3Dw4EHrg4Dkx2g0MpheyhnMaVtlbUZCNhFg8fHxfHlGo5HFx8eziRMnqsr2BlhgYKBA78DAQNa7d2/Vso0Ai7eQg0gPp153dLTpw8lxaQ/Umc2/tZ1lJyYmyl7CiRMnFC9R8j7VGGo6ql2jp9CqXoxpVzet6sWY423JqaFCT8MAJCQkCI5xoSRXlysXuiouLlYsn8HkcZWXlwuOl5eXo6KiQlU2ASZvKz4+HkajEfFmL8Ql171nj3DvMnHaDhypM07e5t9aFBpkPj4o0um8LqxMEIQZx22n67HF47L0PLg36fj4eGkPxAUel9FcDkSez2+//aYoawBYZ5HHxH06d+7MDAaDonyi+bpt8hwUPBdL7PFcbK43mTqbOnWq4JqnTp2qWmd2/daMMabXmz7cd+j1zOjjI1m27LUz8rg8iVb1Yky7umlVL8bI4+LRAQiFsJ9j0aJFiI+PR2hoqMv7LnQw9S1ZYmt/y1W541flcm6TBFN/D1cOd922TlxOAjBt2jTey2CMYdq0aW6Z+PyGzPHly5cLD4g8Jp1ej1CdzvbfOjhYuFlocDDe8POT1ukNOa0IgtAMzrOhrsPePi5L3NXHVVOPi8HUx9WkSROBbJMmTWzq4+LKVr1mCVkjwHqKvA3OC+nZs6f19zjR4zICbKqMpzlq1Chh2Q0bmj6cvDlts6co6psz9u7NprZoIVm2ktdFHpfn0KpejGlXN63qxRh5XFaI37bdMUqMwdzXZB6KbtnXlJqaalMf17Vr1wTHr127ptrHBZg8pgRA4DElJCTY7DH1NP9dunQpfHx8sHTpUtPxnj2tT/bUsHKZlS9s/q25lT44MjKAS5dcoChBEO6g1hkum3DyA1gHIBumZa4WLlwInU6HhQsX/n97Zx8VZZU/8M+AEASICpknld8Bdy3NRdJssXTtZd0sTdSjIqxjiZVaC3nsBTNRXLHSNF1zKXGXUsxNNF0zd1034+TapttPF1DE10rUTN6MaQZ5cZ77+2OYceZhnhlEGAZ+93OOR+69z3fu97nP3Oc7997v/V5iYmIoLi52aTwFYA2MFR0djdlsJjo6GoBjx465NXo/YnHOaI6TgQ5YDaSkpDjkp6SksHr16sZ6m82Om4HV6RtAB+wEwsLCHPLDwsLYt2+fY90tWK+17q5AcnKyQ35ycjJdu3aVLvESiZfT4QyX+mXt9OXdwi9CAcRg2XQ9d+5chBDMnTuX/Px8+vXr59KA6Lj+EAoLC/H19aWwsBAAHx8ft5EvPLoBuQVRsKxJVlRUOORXVFTQuXNnFEVp1foF2PYTWvnXv/7V5h6FNTU1JCcnk5iYyDPPPENlZWWja9auXcvEiROZMmWK7btSXFxMYmIier2eGTNmNBrBSyQdiQ5luNJxdIm+0Wmz5qIDQrGMuOwNSExMDCEhIW5HXFEaZVFRUU0aNb2tyrOO+twhgDlgmx60smbNGgeHDRstOFLVASM0yoYMGdI0/ZvyI8UJCvAJlh8aMTExmM1mYmJiyM/P55NPPml1o+mKv/zlL/Tt25fNmzczbtw4MjMzHcqLior4z3/+w9atW3n77bdtziRLly4lLS2NnJwcRo4cyfr169tCfYnEI3QYw2WbNmuDvTkCS8zAfNVxGfn5+W73JPlgMXo+Po6PwsfHh9DQ0Eb5ahYBg1R5gwYNssWCdIc1nHFycjKKotimz5wFOm7pqcICLMbenpiYGE6cOOHWcKXT/B8pPsDYhrry8/Px9fW1GbGxY8e6bfPWxP6EhV/96ld8ZQ0AbVc+bNgwdDodd9xxB2azmcrKSt5++21bHEWz2cwtt9zicd0lEk/RYU5AVo96rGGAYmJiCA0NbdVpM+uo54uGF6GVmJgYUlNTXdZtBoqg0a98RVEoKirCbDZrhnxSgHVY3OkHDhzIkSNHGDRoEAUFBfzwww8sWrTI7Uu4F3ARR3d6gF69ermUu1kEMBBY48TYT506FSG0wy/Z/0gBbMGFrdOkrmSt6IDhw4c7PK/hw4d7dHp169attjidVsLCwggJCQEgKCiInxoCQVsxGo10sZ4fZ3eN9cDWI0eOsGnTJj788EPNel1FuPfWCPjeqhd4r27eqhe0gG7N8GT0ODe6AVn9r7U3INu7lav/RUdHu6xbATFEQ3bIkCEuXdJd1dtUd3atTcBN3UbQXFd8M4jbNXQPCwtz3Hjdwnq7csVva3f4559/XhQUFAghhDAYDGL06NEO5Rs2bBBZWVm2dFxcnKioqBBCCLF7924xZswYUVJS0mwdvdWF2lv1EsJ7dfNWvYSQ7vA2rKMeq0eelejo6Cav+dwMRRr5p0+fdiknAK3DMA4fPuzWseMWIFyVHx4ezi233NKke9YBq1evdshz6lHohHSa74qvQ3u47+vr67Z+rW3C7X0D8aBBg/iiwXV///79DB48uFH5gQMHUBSF77//HkVR6NatGzt37mTTpk3k5OTQu3fvtlBdIvEYHcZwCSACbF5WVgoLC4mIiGjVNS4FCNAoCwgIcLnYLxrknX6uorjUWwFOAmr/sfLyck6ePNkkJwOBJdqIPdbpNndyN+uKn4Rzd/gJEya4dWi5gnOnkitXrjSp7p1YDLw94eHh7Ny5s029MRMSEjh9+jQJCQls2bLFdhjo8uXLKSwsZMCAAdx7773Ex8eTnJzMwoULMZvNLF26FJPJRHJyMnq9vlHbSCQdihYY9bU6TZkqNIMI0pj+CQoKcjv1dDNThQJEZxd1u5Kt15Cz/quvr9eUN7uRdRfnUAHRs+Ha5ORkoSiKSE5OFoDo2bOn26lGZ1NuTZ2uc/W8AgMDXT4vBcR9GrL33XefW73NIAZqyA8cOLBxuzUgI2e0Hd6qlxDeq5u36iWEnCp04Gda+T/TKmkZzIBBo8xkMmF24Xnn7gG4cq5wd3rYNTfniwmgtOHvL774AiGEbZqqtLTU7chlMY1Hi4qiNHm67qpGvjpSvjP+Vyv/f7VKruPKFX/EiBFevf9NIpF0EK/CB8kDLO7V18kF3gUCKShYzcMP2xuAPJ7iA55iA+WEMZFtAJiHX8PX19Iks2fDY48ZqKrqjF6vrjGPF1nJE3zKSfryLOucaJUB7AMG8vDDOq6/Cy26vs587ucr8hgKvO5Efg5QwN/+VsuKFYEOdQOsYyaRnALGAC86kbcovWULvPuuoyzANibSjQrqeRJ4isJC8PXdj/VIyvr6xwHIzITc3MbyeTzESsDIiw06WFi7Fnx961i0yOLZt2QJ7NvnKBtGBVuY2GD0XgeGOmguxAUURcHHx4c5cyDfThbgZ5xCYWZDah1w/aRtRYEXXhCsWWNp8KlT4YJKfihfsYb5tpaA69OVa9ZAeLiOtDRL+rHH4GqDhb3ejhKJpC1p0RGXoigsXLiQ+Ph49Ho9586dcyjPzc1lwoQJTJ48mbw8y8uksrKSpKQkEhMTmTNnDlevav0Od427FSwhXK/3fAEcOHAAs9kySqmuriY0NJSIiNZd6L7irvzKj5pl1W5kq6tdX6F18rKtvFb7CjNg1Cozm12OMkF7Xc9W7mJ9zt3OMXd1uxup1tfXu7lCIpG0KS03aynEP/7xD5GamiqEEOK///2vmDVrlq2stLRUjBkzRtTW1gqDwWD7e8mSJeLjjz8WQgixbt068f777zf63KascZW7We8pLy/XlK1SXVtVVdUo7aruEjd1O7gnq2QXuZFtdK6WneznbmQ///xzl3pfdCN/8eJFTfnzbmTPnz/vsu4zbuTPnDmjKWt0I2s0Gl3WXepGvrS0tPH3Tcg1rrbEW/USwnt181a9hPCyNS77Xf8xMTEcO3bMVlZYWMg999yDv78/ISEhREREcOLEiUaRAv797383q+5uNHYLtxIeHk63bt00ZTtjiXxhJTQ01PZ3VVUVnTt3dln3HYCrwEd33HGHZtlCl58MCxdqXzHMjeywYa6v6IHFE9MZERER9OjRQ1O2F1CiUVZSUuJ2A3Mk4K9R5uPjQ2RkpKbsrcDTGmVPP/00t956q8u6bwMGa5QNHjyY2267zaW8RCJpW1p0jctoNBJsd2Cfr68v165do1OnThiNRltEALDs+DcajQ75ziIFWFHvsr6r4X/7ZfRngDecyI4fP54TJ05oygogBDh48CCxsbG26w4ePMjFixe5qDoCQy1vxvX0VVFREX4NBxeqZd1N9x05coSgoCCndbubHj1+/Dj+/tfNg7P7LlULNfDDDz9QXFzs4ByirvstDdlXX32V+fPnOzg5qOtW0J4u1Ol0HD9+3BYxRC17DXhfQzY7O5uUlBQ6dbr+1XZ23/uBIBqTlZWluaPfmyMRSCT/n2hRwxUcHIzJZLKlFUWxvUDUZSaTiZCQEFt+QEAAJpNJc3RjjcNmw+rxptMhgPraWt7QiM+2fv161q5de/0lbicLoBMCg8FArN1ICyA2Ntb5iEsl71NfDxon6gL079//+otUJVt57hw0hOtxRvfu3YmIsBsX2cm7872LjIx01F3VZnXV1dRojE7q6uqIiooiMDDQqbwCvKNR74cffsiGDRscQ1Wp7lupreWaxvMym8307dtX83nVm0yYg5yZHct3rk+fPo6jLpV8TXU1QRr3PXjwYKqrqx3vu4Hi4uLG30M7Dh/W2koukUhakhadKhw0aBD79+8HLDHn+va97u0VHR3N4cOHqa2t5aeffuLs2bP07dvXbaSApmJvFIFGx0Goy+0xGAyNpgethIaGYjBoObtbuHTpkkO6pKTEZbk9H3zwgUNa7UquLrfniCptdXixlR9RX+HIlSuOriHqkaW63B71HZ0/f96x3MU9A1y4cMEhffbsWZfl9giVm77RaHRZrkbttKI+AsSdU4tEImljWm65TQiz2SzS0tJEfHy8mDx5sjhz5ozIzs4Wn332mRBCiC1btogJEyaI8ePHiz179gghhCgrKxNJSUkiPj5ezJo1S5hMphteqLMu9NGwuF5ZWSmEEKKystKW5w7rdVZHDHsHjaZgvdbqiFFSUnLDdS9evFgIIcTixYtvWDYvL08IIUReXl6TZNVtZnXEuHjx4g3XbXXEOH/+fLPa7OzZs0IIIc6ePXvDdVsdMYxGY7PqtjrtlJeXu5WXzhlth7fqJYT36uateglx832p/UbOsMO+EaxGSyvtCrX3YCNvQjeog5uWlJQ0+ctjNVpaaVdYjZZW2hn2eqm9Bxt5E7pA7T3YyJvQDVajZZ9uapupvQcbeRO6wcHT1ElajTRcbYe36iWE9+rmrXoJ4WVehd5A165dXaZdoV7LcudNqEYd3PRGgp2qvQddeROqefDBB12m3aH2enTlBalG7T14o8ehREVFuUy7Iki1zqVOu8NZnESJROL9dDjDJZFIJJKOjTRcEolEImlX6IRoxfM+WgjpZixpLzTXK9ZTyL4kaS+46kvtwnBJJBKJRGJFThVKJBKJpF0hDZdEIpFI2hXt6jwuRVFIT0/n5MmT+Pv7k5GRwf/YhUvKzc3lo48+olOnTsyePZuHHnrII3rV19czf/58Ll68SF1dHbNnz+aRRx6xlb///vts27bNFuh38eLFN+T2fbOMGzfOFg+yV69evPHG9YiObdVm27dvZ8eOHYDl+JTi4mK+/PJL2xaEjIwMhziNmZmZDrEuW4uCggJWrFhBTk4O586dY968eeh0On7+85+zaNEih9iNNTU1vPzyy1RUVBAUFMSyZctcBnP2Fry1H4F39yVv7EfgnX2p1ftRS20o8wTNOTbFE2zbtk1kZGQIISwbnkeMGOFQ/uKLL4qjR496RBc1NTU1Ii4uzmlZW7aZPenp6eKjjz5yyJsyZYqoqKjwqB5ZWVlizJgxYtKkSUIIIWbOnCkOHjwohBAiLS1N7N271+H67OxssWbNGiGEEJ9++qlYsmSJR/VtLt7aj4Tw3r7UHvqREN7RlzzRj9rVVGFzjk3xBKNGjeKFF16wpR2Cy2KJDp+VlUVCQgLr1jk7Lbn1OHHiBFevXiUpKYlp06aRn59vK2vLNrNy9OhRzpw5Q3x8vC1PURTOnTvHwoULmTJlCtu2bfOILhEREbzzzvXQwUVFRdx3332A8yN31EfyfPXVVx7R82bx1n4E3tuXvL0fgff0JU/0o3Y1VdicY1M8gXUIbjQaSUlJYc6cOQ7lo0ePJjExkeDgYH73u9+Rl5fnsamEgIAAZsyYwaRJk/juu+945pln2LNnT5u3mZV169bx/PPPO+RVV1czdepUpk+fjtlsZtq0aQwYMIC77rpL41NahkcffdQhuK8QwnY0i7Mjd5p6JI+34a39yFqfVUdv6kve3o/Ae/qSJ/pRuxpxNefYFE9x6dIlpk2bRlxcHE888YQtXwjBk08+Sbdu3fD392fEiBEcP37cY3pFRkYyduxYdDodkZGRdOnShbKyMqDtc+0SRAAABwpJREFU28xgMPDNN984nIEGEBgYyLRp0wgMDCQ4OJjY2Ng2+QVrPw/v7Mgd+/ZzdSSPt+HN/Qi8sy95cz8C7+5LrdGP2pXhas6xKZ6gvLycpKQkXn75ZSZOnOhQZjQaGTNmDCaTCSEEhw4dYsCAAR7RC2Dbtm28+eabAFy+fBmj0Wg74bct2wzg66+/5v7772+U/91335GYmIjZbKa+vp4jR45w9913e0wvK/379+fQoUOA5cide++916G8pY7k8TTe2o/Ae/uSN/cj8O6+1Br9qF1tQLZ6Q506dQohBK+//jr79+8nIiKCRx55hNzcXLZs2YIQgpkzZ/Loo496RK+MjAz+/ve/O3g3TZo0iatXrxIfH89f//pXcnJy8Pf3Z+jQoaSkpHhEL7AcCPnqq6/y/fffo9PpeOmllygoKGjzNgP405/+RKdOnXjqqacAi8eYVa/169ezZ88e/Pz8iIuLIyEhwSM6Xbhwgblz55Kbm8u3335LWloa9fX1REVFkZGRga+vL0lJSbz33nuYzWZSU1MpKyvDz8+PlStX2l5m3oy39iPw3r7kzf0IvK8vtXY/aleGSyKRSCSSdjVVKJFIJBKJNFwSiUQiaVdIwyWRSCSSdoU0XBKJRCJpV0jDJZFIJJJ2hTRckg7DhQsXmDx5MgAnT57k66+/btX6BgwYgF6vd/h3+fJlp9du376dffv2AbBp06Ym1/HPf/6z0WceOnSIoUOHotfrmTp1KlOmTOHs2bPNvxEN7NtTIvEm2lXIJ4mkqezdu5fw8HCGDBnSanWEhoaSk5PTpGsnTJhg+/vdd99l6tSpTZLbuHEj6enp3H777Q75sbGxrFq1CoADBw6wfPlyj8fBlEjaCmm4JB2Oy5cvs2PHDvz8/Lj77rupqalh1apV+Pr60rt3b37/+9+za9cu8vLyqKmpoaysjGnTprFv3z5Onz7NK6+8wq9//WvmzZtHSUkJtbW1zJgxg8cff7xJ9S9btgw/Pz/mzJnD9OnTmT59OkePHiU8PJwff/yRqqoq0tPTSU9Pt8mcOnWKN998E0VRMBgMLFiwAIPBQHFxMampqWzevBl/f3+n9RkMBnr27AmAXq+na9euGAwG3nnnHRYsWMBPP/3ElStXmDRpEomJiej1eu666y5Onz6N0WjkD3/4Az179iQzM5PPPvsMs9lMQkICw4YNo7Kykueee46ysjLuvPNOMjIybvr5SCQ3izRckg7H7bffzvjx4wkPD+cXv/gFo0aNYvPmzYSFhbF69Wp27NhBp06dMJlMZGdns3v3bj744ANyc3M5dOgQGzduJDY2lkOHDvHxxx8D8OWXXzaqp6qqCr1eb0t3796dlStXMnfuXH7729+SmppKdHQ0Dz74IEePHgVg9uzZbNq0ycFoAZw5c4bU1FTuvPNOdu3axfbt28nIyKBfv36kp6c3MloHDx5Er9dTV1fHyZMnHUZbTzzxBCNHjqSoqIjRo0fzm9/8hsuXL6PX60lMTAQsYYpee+01Vq1axe7duxk2bBj79+9n69at1NXVsXLlSh544AGMRiNvvPEGISEhjBw5koqKCsLCwlrkOUkkzUUaLkmHprKyktLSUluU8ZqaGh544AEiIiLo168fACEhIfTp0wedTkdoaCi1tbUEBweTlpZGWloaRqORsWPHNvpsralCPz8/nnzySVJTU8nLy2uSnt27dyczM5OAgABMJpND9HZn2E8VfvPNN0yZMsUWfzAyMhKA8PBwNmzYwN69ewkODubatWs2+f79+wPQo0cPysvL+fbbb4mOjsbX15fAwEAWLFjAhQsX6N27N6GhoQCEhYVx9erVJt2PRNKaSOcMSYdEp9OhKApdu3alR48eZGZmkpOTw6xZs/jlL39pu0aL0tJSioqK+OMf/0hWVhZvvfWWw4vfFVVVVbz33nvMmzePtLS0RuXOoqwtXbqUlJQUli1bRt++fW3X6HQ6p9fbEx4e7pC23ld2djYxMTGsWLGCUaNGufycqKgojh8/jqIo1NfXM336dOrq6ly2kUTSVsgRl6RDMmDAAJYvX06fPn147bXXePbZZxFCEBQUxPLly7l06ZJL+dtuu42ysjLGjRvHrbfeSlJSku3oDyvqqUKAuXPn8uc//5mnn36auLg4jh07xsaNGx2u6dOnDy+99BIrVqyw5Y0dO5bnnnuOsLAwevTowZUrVwC45557eOWVV8jOzqZLly62661ThT4+PphMJubNm0dAQIBDPQ899BDp6ens2rWLLl264OvrS11dndP77devH8OHDychIQFFUUhISNBcU5NI2hoZZFcikUgk7Qo5VSiRSCSSdoU0XBKJRCJpV0jDJZFIJJJ2hTRcEolEImlXSMMlkUgkknaFNFwSiUQiaVdIwyWRSCSSdsX/AWRYNfhsVMBiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Branch Name</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>E_Threshold</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>accepted_correct</th>\n",
       "      <th>accepted_incorrect</th>\n",
       "      <th>accepted_accuracy</th>\n",
       "      <th>overlap_adjusted_accuracy</th>\n",
       "      <th>M(T) B(F)</th>\n",
       "      <th>M(F) B(T)</th>\n",
       "      <th>M(F) B(F) overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>branch_1</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.7127</td>\n",
       "      <td>0.712700</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.327400</td>\n",
       "      <td>2992</td>\n",
       "      <td>282</td>\n",
       "      <td>0.913867</td>\n",
       "      <td>0.925473</td>\n",
       "      <td>244</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>branch_2</td>\n",
       "      <td>6726</td>\n",
       "      <td>0.7389</td>\n",
       "      <td>0.649569</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.251858</td>\n",
       "      <td>1401</td>\n",
       "      <td>293</td>\n",
       "      <td>0.827037</td>\n",
       "      <td>0.849469</td>\n",
       "      <td>255</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>branch_3</td>\n",
       "      <td>5032</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.888116</td>\n",
       "      <td>0.030746</td>\n",
       "      <td>0.547099</td>\n",
       "      <td>2693</td>\n",
       "      <td>60</td>\n",
       "      <td>0.978206</td>\n",
       "      <td>0.992372</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Main_Exit</td>\n",
       "      <td>2279</td>\n",
       "      <td>0.9574</td>\n",
       "      <td>0.880649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2007</td>\n",
       "      <td>272</td>\n",
       "      <td>0.880649</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Branch Name  Predictions  test_accuracy  Accuracy  E_Threshold  \\\n",
       "0    branch_1        10000         0.7127  0.712700     0.018753   \n",
       "1    branch_2         6726         0.7389  0.649569     0.006845   \n",
       "2    branch_3         5032         0.9250  0.888116     0.030746   \n",
       "3   Main_Exit         2279         0.9574  0.880649     0.000000   \n",
       "\n",
       "   acceptance_rate  accepted_correct  accepted_incorrect  accepted_accuracy  \\\n",
       "0         0.327400              2992                 282           0.913867   \n",
       "1         0.251858              1401                 293           0.827037   \n",
       "2         0.547099              2693                  60           0.978206   \n",
       "3         1.000000              2007                 272           0.880649   \n",
       "\n",
       "   overlap_adjusted_accuracy  M(T) B(F)  M(F) B(T)  M(F) B(F) overlap  \n",
       "0                   0.925473        244         14                 38  \n",
       "1                   0.849469        255         21                 38  \n",
       "2                   0.992372         21          4                 39  \n",
       "3                        inf          0          0                272  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displayEvidence_cascade(test_Outputs, thresholds=[0.018753,0.006845,0.030746],  Evidence = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''     overlap\n",
    "        if zero, both match, if else they don't match\n",
    "        TT 1-1 =0\n",
    "        TF 1-0 =1\n",
    "\n",
    "        FT 0-1 = -1\n",
    "        FF 0-0 =0\n",
    "        \n",
    "        '''\n",
    "# print(Outputs[1])\n",
    "displayEvidence(Outputs, Evidence = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 227,227 sigmoid evidence cifar10 v2\n",
    "# print(Outputs[0])\n",
    "\n",
    "displayEvidence(Outputs, Evidence = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 227,227 sigmoid evidence cifar10\n",
    "displayEvidence(Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 227,227 sigmoid cifar10\n",
    "displayEvidence(Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 32,32 crossEvidence cifar10\n",
    "displayEvidence(Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "displayEvidence(Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayEvidence(Predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.models.load_model('alexNetv6_evidence_test.hdf5',\n",
    "    custom_objects={\"CrossEntropyEndpoint\":CrossEntropyEndpoint,\"crossEntropy_loss\":loss_function()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_2 = collectEvidence(model_2,test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayEvidence(Predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectEntropy(model,test_ds):\n",
    "    num_outputs = len(model.outputs) # the number of output layers for the purpose of providing labels\n",
    "#     train_ds, test_ds, validation_ds = (dataset)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    iterator = iter(test_ds)\n",
    "    print(len(test_ds))\n",
    "    item = iterator.get_next()\n",
    "#     print(item)\n",
    "\n",
    "    pClass = []\n",
    "    predictions=[]\n",
    "    pEvidence = []\n",
    "    pUncertainty=[]\n",
    "    Outputs = pd.DataFrame()\n",
    "    output_names=[\"mainExit\"]\n",
    "    pAcc=[]\n",
    "    for i, (x,y) in enumerate(test_ds):\n",
    "#     for i in range(100):\n",
    "        print(\"prediction: {} of {}\".format(i,len(test_ds)),end='\\r')\n",
    "        result = model.predict(x)\n",
    "        pClass.append(tf.argmax(y,1).numpy()[0])\n",
    "        pred= (tf.nn.softmax(result)[0])\n",
    "\n",
    "        pEvidence.append(calcEntropy_Tensors(pred).numpy())\n",
    "        if np.argmax(pred) == np.argmax(y):\n",
    "            pAcc.append(1)       \n",
    "        else:\n",
    "            pAcc.append(0)\n",
    "    Predictions = pd.DataFrame({\"label\":pClass,\"evidence\":pEvidence,\"Acc\":pAcc,\"overlap\":0})\n",
    "    return Predictions\n",
    "\n",
    "def displayEntropy(Predictions):\n",
    "    output_names=[\"mainExit\"]\n",
    "    Outputs=pd.DataFrame()\n",
    "    Predictions[\"Acc\"]=Predictions[\"Acc\"].astype('bool')\n",
    "    acc = Predictions[\"Acc\"].value_counts()\n",
    "    print(acc)\n",
    "    print((acc.loc[True] , acc.loc[False]))\n",
    "    mean = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].mean().iloc[0]\n",
    "    std = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].std().iloc[0]\n",
    "    E_threshold = mean - std\n",
    "    correct_rows = Predictions.loc[Predictions['Acc'] == True]\n",
    "    incorrect_rows = Predictions.loc[Predictions['Acc'] == False]\n",
    "    # Incorrects_overlap = Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] > E_threshold) & (Predictions[\"overlap\"] == 0)].count().iloc[0]\n",
    "    print(acc)\n",
    "    for i,name in enumerate(output_names):\n",
    "        Outputs = Outputs.append(pd.DataFrame({\"Branch Name\":output_names[i],\n",
    "                \"Accuracy\":(acc.loc[True] /  (acc.loc[True] + acc.loc[False])),\n",
    "                \"E_Threshold\":E_threshold,\n",
    "                # \"Overlap_Threshold\":non_overlapping_incorrects_threshold,\n",
    "                \"acceptance_rate\":Predictions.loc[(Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0]/(Predictions.count().iloc[0]),\n",
    "                \"accepted_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                \"accepted_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                \"accepted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0])/ Predictions.loc[(Predictions[\"evidence\"] < E_threshold)].count()[0],\n",
    "                # \"overlap_adjusted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0] + Incorrects_overlap) / Predictions.loc[(Predictions[\"evidence\"] > E_threshold)].count()[0],\n",
    "                \"rejected_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                \"rejected_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                # \"Incorrects_overlap\":Incorrects_overlap,\n",
    "                },index=[i]))\n",
    "\n",
    "    # fig, axs = plt.subplots(1, 2)\n",
    "    plt.suptitle('Horizontally stacked subplots')\n",
    "    plt.scatter(correct_rows['label'],correct_rows['evidence'],c ='r',marker='+')\n",
    "    plt.scatter(incorrect_rows['label']+.3,incorrect_rows['evidence'],c ='k',marker='x')\n",
    "    plt.plot(np.repeat(E_threshold,11),'b--')\n",
    "    plt.title(\"evidence\")\n",
    "\n",
    "\n",
    "    # fig.set_size_inches(10, 10)\n",
    "    plt.show()\n",
    "    return Outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy_predictions = collectEntropy(model_2,test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayEntropy(Entropy_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "test_images = test_images.reshape(10000, 32,32,3).astype(\"float32\") / 255\n",
    "\n",
    "# print(y_train)\n",
    "K= 10 # number of classes\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels,10)\n",
    "shuffle_size = 22500\n",
    "batch_size=1\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "\n",
    "def augment_images(image, label,input_size=(227,227), channel_first = False):\n",
    "            image = tf.image.resize(image,input_size)\n",
    "            if channel_first:\n",
    "                image = tf.transpose(image, [2, 0, 1])\n",
    "            return image, label\n",
    "test_ds_size = len(list(test_ds))\n",
    "# test_ds = (test_ds.map(augment_images))\n",
    "t_target = tf.data.Dataset.from_tensor_slices((test_labels))\n",
    "test_ds = tf.data.Dataset.zip((test_ds,t_target))\n",
    "test_ds = (test_ds.batch(batch_size=1, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_mse = collectEvidence(model,test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayEvidence(Predictions_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sum entropy \n",
    "import pandas as pd\n",
    "def entropy(x):\n",
    "    return -(x * math.log(x))\n",
    "# Data for plotting\n",
    "t = np.arange(0.00001, 1, 0.01)\n",
    "print(t.shape)\n",
    "t_ = np.full((100,), .1)\n",
    "df = pd.DataFrame([t,t,t_,t,t])\n",
    "# print(df.transpose())\n",
    "p = df.apply(calcEntropy,axis=0)\n",
    "# print(p)\n",
    "# print(p)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, p)\n",
    "ax.set(xlabel='Probability of Outcome',ylabel='Entropy of event')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0]\n",
    "y_pred = [.99,.01, .01, .0, .01, .01]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.  \n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "crossE = scce(y_true, y_pred).numpy()\n",
    "print(\"crossE: \",crossE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0,1,0,0,0,0]\n",
    "y_pred = [.99,.01, .01, .0, .01, .01]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.  \n",
    "scce = tf.keras.losses.CategoricalCrossentropy()\n",
    "crossE = scce(y_true, y_pred).numpy()\n",
    "print(\"crossE: \",crossE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0]\n",
    "y_pred = [.0,.01, .9, .0, .0, .0]\n",
    "ent = calcEntropy(y_pred)\n",
    "print(\"Entropy: \",ent)\n",
    "loss = ent *1\n",
    "print(\"Loss: \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0],[0],[0]]\n",
    "y_pred = [[.9,.5, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.7,.5, .6, .5, .5, .5]]\n",
    "y_pred = [[.9,.0,.0,.0,.0,.0,],[.8,.5, .6, .5, .5, .5],[.7,.5, .6, .5, .5, .5]]\n",
    "# y_pred = [.1,.1, .1, .1, .1, .1]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.  \n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "crossE = scce(y_true, y_pred).numpy()\n",
    "print(\"crossE: \",crossE)\n",
    "\n",
    "''' When the answer is correct, CrossE goes down\n",
    "    When \n",
    "    When its wrong, Entropy High\n",
    "    When its right, Entropy Low\n",
    "    \n",
    "    so penalize being right with low entropy and reward being right with high entropy\n",
    "    \n",
    "    \n",
    "    OORRRR train a second model for a branch to determine if you are going to get it right or not?\n",
    "    Isn't that what ResNet Did? you calculate if the blocks will contribute, was it block drop?\n",
    "    Binary classification,\n",
    "    could be done at the branch end as a separate evaulator, using the entropy score and the input to the branch as inputs?\n",
    "'''\n",
    "\n",
    "\n",
    "ent = calcEntropy(y_pred)\n",
    "print(\"Entropy: \",ent)\n",
    "loss = crossE + ent\n",
    "print(\"combined Loss: \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[.1,.1, .675, .1091, .4311, .1875,.121,.143,.2,.5]]\n",
    "x = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],\n",
    "                 [14, 45, 23, 5, 27]])\n",
    "\n",
    "print(list(map(np.argmax,np.array(x))))\n",
    "def foo(y_pred):\n",
    "    y_pred = y_pred.numpy()\n",
    "    pred_label = list(map(np.argmax,np.array(y_pred)))\n",
    "    return pred_label\n",
    "%timeit foo(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.constant([[2],[2],[0]])\n",
    "A = tf.constant([.1,.1, .675, .1091, .4311, .1875,.121,.143,.2,.5])\n",
    "B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],\n",
    "                 [14, 45, 23, 5, 27]])\n",
    "\n",
    "y_pred = tf.constant([[.9,.5, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.7,.5, .6, .5, .5, .5]])\n",
    "\n",
    "# new_list = new_list = [list(range(10)) for _ in range(10)]\n",
    "\n",
    "print(tf.math.argmax(y_pred,1))\n",
    "pred_labels = tf.math.argmax(y_pred,1)\n",
    "print(tf.reshape(y_true,pred_labels.shape))\n",
    "indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "indexes = tf.reshape(indexes,[-1])\n",
    "# print(tf.gather(B,indexes))\n",
    "CorrectE = tf.gather(y_pred,indexes)\n",
    "print(CorrectE)\n",
    "# print(calcEntropy(CorrectE[0]))\n",
    "\n",
    "\n",
    "results = tf.map_fn(calcEntropy,tf.cast(CorrectE,'float'))\n",
    "print(\"results: \",results)\n",
    "\n",
    "\n",
    "\n",
    "%timeit tf.map_fn(calcEntropy,tf.cast(CorrectE,'float'))\n",
    "\n",
    "\n",
    "crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "crossE(y_true,y_pred)\n",
    "%timeit crossE(y_true,y_pred)\n",
    "# [\n",
    "#     [\n",
    "#         [ 2 20 30  3  6]\n",
    "#     ]\n",
    "#     [\n",
    "#         [ 3 11 16  1  8]\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "def entropyAddition_noCross(y_true, y_pred):\n",
    "    crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    #Entropy is added to the CrossE divided by the len of inputs\n",
    "    pred_labels = tf.math.argmax(y_pred,1)\n",
    "    indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "    indexes = tf.reshape(indexes,[-1])\n",
    "    entropies = tf.gather(y_pred,indexes)\n",
    "    if tf.equal(tf.size(entropies), 0):\n",
    "        correctEntropies = 0\n",
    "    else:\n",
    "        correctEntropies = tf.reduce_mean(tf.map_fn(calcEntropy_Tensors,tf.cast(entropies,'float')))\n",
    "    scce = crossE(y_true, y_pred)\n",
    "    loss = correctEntropies\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1],[2],[0]])\n",
    "y_pred = tf.constant([[0,0, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.9,.3, .3, .2, .1, .32]])\n",
    "crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "def entropyAddition(y_true, y_pred):\n",
    "    crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    #Entropy is added to the CrossE divided by the len of inputs\n",
    "    pred_labels = tf.math.argmax(y_pred,1)\n",
    "    indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "    indexes = tf.reshape(indexes,[-1])\n",
    "    entropies = tf.gather(y_pred,indexes)\n",
    "    if tf.equal(tf.size(entropies), 0):\n",
    "        correctEntropies = 0\n",
    "    else:\n",
    "        correctEntropies = tf.reduce_mean(tf.map_fn(calcEntropy_Tensors,tf.cast(entropies,'float')))\n",
    "    scce = crossE(y_true, y_pred)\n",
    "    loss = scce + (correctEntropies * scce)\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"normal CrossE: \",crossE(y_true ,y_pred))\n",
    "\n",
    "print(\"normal Entropy\",entropyAddition_noCross(y_true2,y_pred2))\n",
    "\n",
    "print(entropyAddition(y_true2, y_pred2))\n",
    "# %timeit entropyAddition(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([1])\n",
    "y_pred = tf.constant([0,1, 0, 0, 0, 0])\n",
    "# print(crossE(y_true,y_pred))\n",
    "\n",
    "print(tf.cast(1e-8,'float')+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1],[2],[0]])\n",
    "y_pred = tf.constant([[.9,.1, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.9,.3, .3, .2, .1, .32]])\n",
    "crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "def entropyMultiplication(y_true, y_pred):\n",
    "    crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    #Entropy is added to the CrossE divided by the len of inputs\n",
    "    pred_labels = tf.math.argmax(y_pred,1)\n",
    "    indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "    indexes = tf.reshape(indexes,[-1])\n",
    "    entropies = tf.gather(y_pred,indexes)\n",
    "    if tf.equal(tf.size(entropies), 0):\n",
    "        correctEntropies = 1\n",
    "    else:\n",
    "        correctEntropies = tf.reduce_mean(tf.map_fn(calcEntropy_Tensors,tf.cast(entropies,'float')))\n",
    "    scce = crossE(y_true, y_pred)\n",
    "    loss = correctEntropies * scce\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"normal CrossE: \",crossE(y_true,y_pred))\n",
    "print(entropyAddition(y_true, y_pred))\n",
    "# %timeit entropyAddition(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1],[2],[1]])\n",
    "y_pred = tf.constant([[.9,.1, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.9,.3, .3, .2, .1, .32]])\n",
    "\n",
    "def confidenceScore(y_true, y_pred):\n",
    "        # print(y_pred)\n",
    "        # print(tf.keras.backend.get_value(y_pred))\n",
    "        \n",
    "        # y_true =y_true.numpy()\n",
    "        # y_pred = y_pred.numpy()\n",
    "        # AvgConfidence = -1\n",
    "        pred_labels = tf.math.argmax(y_pred,1)\n",
    "        # countCorrect=0\n",
    "        indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "        indexes = tf.reshape(indexes,[-1])\n",
    "        entropies = tf.gather(y_pred,indexes)\n",
    "        if tf.equal(tf.size(entropies), 0):\n",
    "            correctEntropies = 0\n",
    "        else:\n",
    "            correctEntropies = tf.reduce_mean(tf.map_fn(calcEntropy,tf.cast(entropies,'float')))    \n",
    "        \n",
    "        return correctEntropies\n",
    "    \n",
    "print(confidenceScore(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[2],[2],[0]]\n",
    "y_pred = [[.9,0, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.7,.5, .6, .5, .5, .5]]\n",
    "\n",
    "def foo(x, y):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    crossE = scce(x, y).numpy()\n",
    "    return crossE\n",
    "\n",
    "print(foo(y_true,y_pred))\n",
    "%timeit foo(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python program explaining \n",
    "# where() function \n",
    "  \n",
    "import numpy as np\n",
    "  \n",
    "# a is an array of integers.\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "  \n",
    "print(a)\n",
    "  \n",
    "print ('Indices of elements <4')\n",
    "  \n",
    "b = np.where(a<5)\n",
    "print(b)\n",
    "  \n",
    "print(\"Elements which are <4\")\n",
    "print(a[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0],[0],[0]]\n",
    "y_pred = [[.5,.5, .6, .5, .5, .1],[.5,.5, .6, .5, .5, .2],[.5,.5, .6, .5, .5, .3]]\n",
    "# y_pred = [[1],[1],[1]]\n",
    "# print(np.array(y_pred))\n",
    "\n",
    "####\n",
    "# Numpy confidence metric version\n",
    "y_true =np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "def argmax(x):\n",
    "    return [np.argmax(x)]\n",
    "pred_labels = list(map(argmax,np.array(y_pred)))\n",
    "x = np.where(np.equal(y_true,pred_labels) ==True)\n",
    "y = y_pred[x[0]]\n",
    "results = calcEntropy(y)\n",
    "print(results)\n",
    "if not (results):\n",
    "    print(\"A\")\n",
    "print(np.median(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_true = [[2],[2],[0]]\n",
    "y_pred = [[.5,.5, .6, .5, .5, .5],[.5,.5, .6, .5, .5, .5],[.5,.5, .6, .5, .5, .5]]\n",
    "\n",
    "y_true = [[2]]\n",
    "y_pred = [[.1,.1, .15, .1, .1, .1]]\n",
    "def entropyAddition_loss():\n",
    "    #create a wrapper function that returns a function\n",
    "    crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    def entropyAddition(y_true, y_pred):\n",
    "        #Entropy is added to the CrossE divided by the len of inputs\n",
    "        pred_labels = tf.math.argmax(y_pred,1)\n",
    "        indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "        indexes = tf.reshape(indexes,[-1])\n",
    "        entropies = tf.gather(y_pred,indexes)\n",
    "        correctEntropies = tf.reduce_mean(tf.map_fn(calcEntropy,tf.cast(entropies,'float')))\n",
    "#         print(pred_label)\n",
    "        scce = crossE(y_true, y_pred)\n",
    "        sumEntropy = 0\n",
    "        loss = correctEntropies + scce\n",
    "        return loss\n",
    "    \n",
    "    return entropyAddition\n",
    "\n",
    "def custom_loss_multi(y_true, y_pred):\n",
    "    #CrossE is multiplied by the Entropy\n",
    "    pred_label = list(map(np.argmax,np.array(y_pred)))\n",
    "    crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    sumLoss = 0\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        loss = crossE(y_true[i], y_pred[i])\n",
    "#         print('crossE: ',loss)\n",
    "        if pred_label[i] == y_true[i]:\n",
    "#             print('calcEntropy ',calcEntropy(y_pred[i]))\n",
    "            loss = loss * calcEntropy(y_pred[i])\n",
    "        sumLoss += loss\n",
    "    sumLoss = sumLoss / len(y_pred)         \n",
    "    \n",
    "#     loss = crossE(y_true, y_pred)\n",
    "#     print(\"CrossE : \",loss.numpy())\n",
    "#     print(\"Loss : \",sumLoss)\n",
    "    return sumLoss\n",
    "    ### I want to reduce the entropy of correct answers\n",
    "    ### if label - pred = 0 (aka correct) then add entropy to crossE\n",
    "    \n",
    "    \n",
    "#     squared_difference = tf.square(np.array(y_true) - np.array(y_pred))\n",
    "#     return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`\n",
    "\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "crossE = scce(y_true, y_pred).numpy()\n",
    "print(\"crossEntropyLoss: \",crossE)\n",
    "\n",
    "\n",
    "crossE = custom_loss_addition(y_true, y_pred).numpy()\n",
    "print(\"customLoss_addition: \",crossE)\n",
    "\n",
    "\n",
    "crossE = custom_loss_multi(y_true, y_pred).numpy()\n",
    "print(\"customLoss_multi: \",crossE)\n",
    "\n",
    "  \n",
    "# model.compile(loss=custom_loss, optimizer='adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sub(x,y):\n",
    "    if x - y == 0:\n",
    "        return 1\n",
    "%timeit sub(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sub(x,y):\n",
    "    if x == y:\n",
    "        return 0\n",
    "    \n",
    "%timeit sub(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7483e188a462fd4248cd8d23b24bc727a5fe7a35e4044aa57ebcc92f8fe9e445"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
