{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.ndimage as nd\n",
    "from tensorflow import keras\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Activation, Concatenate\n",
    "from tensorflow.keras.layers import Flatten, Dropout\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# from tensorflow.examples.tutorials.mnist import input_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import branchingdnn as branching\r\n",
    "from branchingdnn.utils import * \r\n",
    "from branchingdnn.profiler import model_profiler as profiler\r\n",
    "from branchingdnn.dataset import prepare\r\n",
    "from branchingdnn.branches import branch\r\n",
    "from branchingdnn.initNeptune import Neptune"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    " dataset = prepare.dataset_distil(tf.keras.datasets.cifar10.load_data(),32,5000,22500,(227,227))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainSize 45000\n",
      "testSize 10000\n",
      "<BatchDataset shapes: (((32, 227, 227, 3), (32, 1)), (32, 1)), types: ((tf.float32, tf.uint8), tf.uint8)>\n",
      "<BatchDataset shapes: (((32, 227, 227, 3), (32, 1)), (32, 1)), types: ((tf.float32, tf.uint8), tf.uint8)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "modelName = \"alexNetv6.hdf5\"\r\n",
    "saveName = \"alexNetv6_distil_new\"\r\n",
    "transfer = True\r\n",
    "customOptions=\"CrossE\"\r\n",
    "numpEpocs = 10\r\n",
    "\r\n",
    "x = tf.keras.models.load_model('../models/{}'.format(modelName))\r\n",
    "# x.summary()\r\n",
    "if saveName ==\"\":\r\n",
    "    saveName = modelName\r\n",
    "# tf.keras.utils.plot_model(x, to_file=\"{}.png\".format(saveName), show_shapes=True, show_layer_names=True)\r\n",
    "# funcModel = models.Model([input_layer], [prev_layer])\r\n",
    "# funcModel = branch.add(x,[\"dense\",\"conv2d\",\"max_pooling2d\",\"batch_normalization\",\"dense\",\"dropout\"],branch.newBranch)\r\n",
    "# dataset = prepare.dataset_distil(tf.keras.datasets.cifar10.load_data(),32,5000,22500,(227,227))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class Notebook_EvidenceEndpoint(keras.layers.Layer):\r\n",
    "        def __init__(self, name=None, **kwargs):\r\n",
    "            super(Notebook_EvidenceEndpoint, self).__init__(name=name)\r\n",
    "            self.num_outputs = 10\r\n",
    "            self.loss_coefficient = 1\r\n",
    "            self.feature_loss_coefficient = 1\r\n",
    "            self.kl = tf.keras.losses.KLDivergence()\r\n",
    "            self.global_step = tf.Variable(initial_value=0.0, name='global_step', trainable=False)\r\n",
    "            self.crossE = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
    "            self.loss_fn = utils.evidence_loss(global_step=self.global_step)\r\n",
    "            self.temperature = 10\r\n",
    "            self.alpha = .1\r\n",
    "            self.lmb = 0.005\r\n",
    "        def build(self, input_shape):\r\n",
    "            self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]),\r\n",
    "                                                    self.num_outputs])\r\n",
    "\r\n",
    "        def call(self, inputs, labels):\r\n",
    "            outputs = tf.matmul(inputs,self.kernel)\r\n",
    "            # Compute the training-time loss value and add it\r\n",
    "            # to the layer using `self.add_loss()`.\r\n",
    "            #loss functions are (True, Prediction)\r\n",
    "            # softmax = tf.nn.softmax(inputs)\r\n",
    "            # print(\"inputs\",input)\r\n",
    "            #loss 1. normal loss, predictions vs labels\r\n",
    "            evidence, normal_loss = self.loss_fn(labels, outputs)\r\n",
    "            # print(self.weights, self.lmb)\r\n",
    "            # l2_loss = tf.nn.l2_loss(self.weights)# * self.lmb\r\n",
    "            # print(\"l2_loss\",l2_loss)\r\n",
    "            # print(evidence.shape)\r\n",
    "            # print(normal_loss)\r\n",
    "            total_loss =tf.reduce_mean(normal_loss)# + l2_loss\r\n",
    "            # print(total_loss)\r\n",
    "            total_evidence = tf.reduce_sum(evidence,1, keepdims=True) \r\n",
    "            # print(total_evidence)\r\n",
    "            pred = tf.argmax(outputs, 1)\r\n",
    "            # truth = tf.argmax(labels, 1)\r\n",
    "            # print(\"labels\",labels)\r\n",
    "            truth = tf.cast(labels,tf.int64)\r\n",
    "            # print(\"evid\", evidence)\r\n",
    "            # print(\"loss\", normal_loss)\r\n",
    "            # print(\"truth\",truth)\r\n",
    "            # print(\"pred\",pred)\r\n",
    "            match = (tf.equal(tf.reshape(truth,(1,32)),pred))\r\n",
    "            # match = tf.where(tf.math.equal(pred, tf.cast(tf.reshape(truth,(32,1)),'int64')))\r\n",
    "            match = tf.cast(match,tf.float32)\r\n",
    "            # match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\r\n",
    "            # print(\"match\",match)\r\n",
    "            mean_avg = tf.reduce_mean(total_evidence)\r\n",
    "            # print(\"mean_Avg\")\r\n",
    "            # print(\"match\",match)\r\n",
    "            mean_succ = tf.reduce_sum(tf.reduce_sum(evidence,1, keepdims=True)* match) / tf.reduce_sum(match+1e-20)\r\n",
    "            # print(\"mean_fail\")\r\n",
    "            # side_x = tf.reduce_sum(tf.reduce_sum(evidence,1, keepdims=True)*(1-match))\r\n",
    "            # print(\"side_x\",side_x)\r\n",
    "            # side_y = (tf.reduce_sum(tf.abs(1-match))+1e-20)\r\n",
    "            # print(\"side_y\",side_y)\r\n",
    "            # div = tf.reduce_sum(side_x/side_y)\r\n",
    "            # print(\"div\",div)\r\n",
    "            mean_fail = tf.reduce_sum(tf.reduce_sum(tf.reduce_sum(evidence,1, keepdims=True)*(1-match)) / (tf.reduce_sum(tf.abs(1-match))+1e-20) )\r\n",
    "            # print('end')\r\n",
    "            # self.add_metric(normal_loss, name=self.name+\"_normal_loss\")\r\n",
    "            # self.add_metric(evidence, name=self.name+\"_evidence\")\r\n",
    "            # self.add_metric(mean_avg, name=self.name+\"_mean_ev_avg\")\r\n",
    "            # self.add_metric(mean_succ, name=self.name+\"_mean_ev_succ\")\r\n",
    "            # self.add_metric(mean_fail, name=self.name+\"_mean_ev_fail\")\r\n",
    "            # print('metrics')\r\n",
    "            # self.add_loss(total_loss)\r\n",
    "            softmax = tf.nn.softmax(outputs)\r\n",
    "            CrossE_loss= self.crossE(labels,softmax)\r\n",
    "            self.add_loss(CrossE_loss)\r\n",
    "\r\n",
    "            # print(\"pred\",inputs)\r\n",
    "            # print(\"loss\")\r\n",
    "            #NOTE\r\n",
    "            # The total loss is different from parts_loss because it includes the regularization term.\r\n",
    "            # In other words, loss is computed as loss = parts_loss + k*R, where R is the regularization term \r\n",
    "            # (typically the L1 or L2 norm of the model's weights) and k a hyperparameter that controls the \r\n",
    "            # contribution of the regularization loss in the total loss.\r\n",
    "            return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def notebook_Branch_bottleneck(prevLayer, targets=None, teacher_sm=None, teaching_features=None):\r\n",
    "        print(\"targets::::\",targets)\r\n",
    "        print(\"teacher_sm::::\",teacher_sm)\r\n",
    "        print(\"teaching_features::::\",teaching_features)\r\n",
    "        if prevLayer.shape[1] == 4096:\r\n",
    "            #don't add a feature distil to the last branch\r\n",
    "            teaching_features = None\r\n",
    "        if teaching_features is not None:\r\n",
    "            branchLayer = branch.bottleneck(prevLayer,teaching_features)\r\n",
    "            branchLayer = branch.FeatureDistillation(name=tf.compat.v1.get_default_graph().unique_name(\"branch_teaching\"))(branchLayer,teaching_features)    \r\n",
    "            branchLayer = layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(branchLayer)\r\n",
    "        else:\r\n",
    "            # branchLayer = branch.bottleneck2(prevLayer)\r\n",
    "            print(\"no teaching feature Provided, bottleneck and teaching loss skipped\")\r\n",
    "            branchLayer = layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(prevLayer)\r\n",
    "\r\n",
    "        branchLayer = layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))(branchLayer)\r\n",
    "        branchLayer = layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))(branchLayer)\r\n",
    "        # branchLayer = layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer)\r\n",
    "        # if targets is None and teacher_sm is None:\r\n",
    "            # output = (layers.Softmax(name=tf.compat.v1.get_default_graph().unique_name(\"branch_softmax\"))(branchLayer))\r\n",
    "        # else:\r\n",
    "            # output = branch.BranchEndpoint(name=tf.compat.v1.get_default_graph().unique_name(\"branch_softmax\"))(branchLayer, targets, teacher_sm)\r\n",
    "        # output = (layers.Softmax(name=tf.compat.v1.get_default_graph().unique_name(\"branch_softmax\"))(branchLayer))\r\n",
    "        output = Notebook_EvidenceEndpoint(name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer, targets)\r\n",
    "\r\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "funcModel = branch.add_distil(x,[\"max_pooling2d\"],[notebook_Branch_bottleneck],exact=True)\r\n",
    "#so to self distil, I have to pipe the loss from the main exit back to the branches.\r\n",
    "# funcModel.summary()\r\n",
    "funcModel.save(\"models/{}\".format(saveName))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 227, 227, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 55, 55, 96)   34944       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 55, 55, 96)   384         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 27, 27, 96)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 27, 27, 256)  614656      max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 27, 27, 256)  1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 13, 13, 256)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 13, 13, 384)  885120      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 13, 13, 384)  1536        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 13, 13, 384)  147840      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 13, 13, 384)  1536        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 13, 13, 256)  98560       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 13, 13, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 6, 6, 256)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 9216)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4096)         37752832    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4096)         16781312    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "targets (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           40970       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 56,361,738\n",
      "Trainable params: 56,358,986\n",
      "Non-trainable params: 2,752\n",
      "__________________________________________________________________________________________________\n",
      "targets: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='targets'), name='targets', description=\"created by layer 'targets'\")\n",
      ">0\n",
      "abc\n",
      "input_1\n",
      "conv2d\n",
      "batch_normalization\n",
      "max_pooling2d\n",
      "add Branch\n",
      "targets:::: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='targets'), name='targets', description=\"created by layer 'targets'\")\n",
      "teacher_sm:::: [None]\n",
      "teaching_features:::: None\n",
      "no teaching feature Provided, bottleneck and teaching loss skipped\n",
      "conv2d_1\n",
      "batch_normalization_1\n",
      "max_pooling2d_1\n",
      "conv2d_2\n",
      "batch_normalization_2\n",
      "conv2d_3\n",
      "batch_normalization_3\n",
      "conv2d_4\n",
      "batch_normalization_4\n",
      "max_pooling2d_2\n",
      "flatten\n",
      "dense\n",
      "dropout\n",
      "dense_1\n",
      "dropout_1\n",
      "targets\n",
      "dense_2\n",
      "[<KerasTensor: shape=(None, 10) dtype=float32 (created by layer 'dense_2')>, <KerasTensor: shape=(None, 10) dtype=float32 (created by layer 'branch_output_2')>]\n",
      "[<KerasTensor: shape=(None, 227, 227, 3) dtype=float32 (created by layer 'input_1')>, <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'targets')>]\n",
      "INFO:tensorflow:Assets written to: models/alexNetv6_distil_new\\assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "model = funcModel\r\n",
    "\r\n",
    "epocs = 10\r\n",
    "save = False\r\n",
    "transfer = True\r\n",
    "saveName = saveName\r\n",
    "customOptions=\"CrossE\"\r\n",
    "tags =[\"v6\",\"drt\"]\r\n",
    "\r\n",
    "logs = []\r\n",
    "num_outputs = len(model.outputs) # the number of output layers for the purpose of providing labels\r\n",
    "train_ds, test_ds, validation_ds = dataset\r\n",
    "# train_ds, test_ds, validation_ds = prepare.prepareMnistDataset(dataset, batch_size=32)\r\n",
    "\r\n",
    "#Freeze main branch layers\r\n",
    "#how to iterate through layers and find main branch ones?\r\n",
    "#simple fix for now: all branch nodes get branch in name.\r\n",
    "if transfer: \r\n",
    "    for i in range(len(model.layers)):\r\n",
    "        # print(model.layers[i].name)\r\n",
    "        if \"branch\" in model.layers[i].name:\r\n",
    "            # print(\"setting branch layer training to true\")\r\n",
    "            model.layers[i].trainable = True\r\n",
    "        else: \r\n",
    "            # print(\"setting main layer training to false\")\r\n",
    "            model.layers[i].trainable = False               \r\n",
    "else:\r\n",
    "    for i in range(len(model.layers)):\r\n",
    "        # print(model.layers[i].name)\r\n",
    "        model.layers[i].trainable = True\r\n",
    "        # print(\"setting layer training to True\")\r\n",
    "        # model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.Adam(),metrics=[\"accuracy\"])\r\n",
    "\r\n",
    "\r\n",
    "if customOptions == \"CrossE\": \r\n",
    "    print(\"customOption: CrossE\")\r\n",
    "    model.compile(loss='SparseCategoricalCrossentropy',  optimizer=tf.optimizers.SGD(lr=0.01,momentum=0.9), metrics=['accuracy'],run_eagerly=True)\r\n",
    "elif customOptions == \"CrossE_Eadd\":\r\n",
    "    print(\"customOption: CrossE_Eadd\")\r\n",
    "    entropyAdd = entropyAddition_loss()\r\n",
    "    model.compile(optimizer=tf.optimizers.SGD(lr=0.01,momentum=0.9,clipvalue=0.5), loss=[keras.losses.SparseCategoricalCrossentropy(),entropyAdd,entropyAdd,entropyAdd], metrics=['accuracy',confidenceScore, unconfidence],run_eagerly=True)\r\n",
    "    # model.compile(optimizer=tf.optimizers.SGD(lr=0.001), loss=[crossE_test, entropyAdd, entropyAdd, entropyAdd], metrics=['accuracy',confidenceScore, unconfidence],run_eagerly=True)\r\n",
    "else:\r\n",
    "    print(\"customOption: Other\")\r\n",
    "# model.compile(loss=entropyAddition, optimizer=tf.optimizers.SGD(lr=0.001), metrics=['accuracy'],run_eagerly=True)\r\n",
    "    model.compile(loss='SparseCategoricalCrossentropy', optimizer=tf.optimizers.SGD(lr=0.001), metrics=['accuracy',confidenceDifference],run_eagerly=True)\r\n",
    "\r\n",
    "run_logdir = get_run_logdir(model.name)\r\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\r\n",
    "# print(\"after reset:\")\r\n",
    "# test_scores = model.evaluate(test_ds, verbose=2)\r\n",
    "# print(\"finish eval\")\r\n",
    "# printTestScores(test_scores,num_outputs)\r\n",
    "\r\n",
    "if saveName ==\"\":\r\n",
    "    newModelName = \"{}_branched.hdf5\".format(model.name )\r\n",
    "else:\r\n",
    "    newModelName = saveName\r\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"models/{}.hdf5\".format(newModelName), monitor='val_acc', verbose=1, mode='max')\r\n",
    "\r\n",
    "# neptune_cbk = Neptune.getcallback(name = newModelName, tags =tags)\r\n",
    "# print(\"epoc: {}\".format(j))\r\n",
    "# results = [j]           \r\n",
    "# history =model.fit(train_ds,\r\n",
    "#         epochs=epocs,\r\n",
    "#         validation_data=validation_ds,\r\n",
    "#         validation_freq=1,\r\n",
    "#         # batch_size=1,\r\n",
    "#         callbacks=[tensorboard_cb,checkpoint])\r\n",
    "#                     # callbacks=[tensorboard_cb,checkpoint])\r\n",
    "# print(history)\r\n",
    "# test_scores = model.evaluate(test_ds, verbose=2)\r\n",
    "# print(\"overall loss: {}\".format(test_scores[0]))\r\n",
    "# if num_outputs > 1:\r\n",
    "#     for i in range(num_outputs):\r\n",
    "#         print(\"Output {}: Test loss: {}, Test accuracy {}\".format(i, test_scores[i+1], test_scores[i+1+num_outputs]))\r\n",
    "#         results.append(\"Output {}: Test loss: {}, Test accuracy {}\".format(i, test_scores[i+1], test_scores[i+1+num_outputs]))\r\n",
    "# else:\r\n",
    "#     print(\"Test loss:\", test_scores[0])\r\n",
    "#     print(\"Test accuracy:\", test_scores[1])\r\n",
    "# logs.append(results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "customOption: CrossE\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "model.fit(train_ds,\r\n",
    "        epochs=epocs,\r\n",
    "        validation_data=validation_ds,\r\n",
    "        validation_freq=1,\r\n",
    "        # batch_size=1,\r\n",
    "        verbose=2)\r\n",
    "                    # callbacks=[tensorboard_cb,checkpoint])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(history)\r\n",
    "test_scores = model.evaluate(test_ds, verbose=2)\r\n",
    "print(\"overall loss: {}\".format(test_scores[0]))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7483e188a462fd4248cd8d23b24bc727a5fe7a35e4044aa57ebcc92f8fe9e445"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}