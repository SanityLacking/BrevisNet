{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "        \n",
    "#     get the three losses:\n",
    "#     1. normal loss, predictions vs labels\n",
    "#     2. KL divergence loss, aka the difference between the student and teacher's softmax\n",
    "#     3. L2 loss from hints.\n",
    "    def call(self, inputs, labels, sample_weights=None):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        normal_loss = self.loss_fn(inputs, labels, sample_weights)\n",
    "        self.add_loss(normal_loss)\n",
    "#         self.add_metric(normal_loss, name=self.name+\"_KL\")\n",
    "#         self.add_metric(normal_loss, name=self.name+\"loss\")\n",
    "\n",
    "        # Log accuracy as a metric and add it\n",
    "        # to the layer using `self.add_metric()`.\n",
    "#         acc = self.accuracy_fn(inputs, labels, sample_weights)\n",
    "#         self.add_metric(acc, name=\"accuracy\")\n",
    "\n",
    "        # Return the inference-time prediction tensor (for `.predict()`).\n",
    "        return tf.nn.softmax(inputs)\n",
    "    \n",
    "class BranchEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(BranchEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.loss_coefficient = 1\n",
    "        self.feature_loss_coefficient = 1\n",
    "        self.kl = tf.keras.losses.KLDivergence()\n",
    "#         self.loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "    def call(self, inputs, labels, teacher_sm=None, sample_weights=None):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        #loss functions are (True, Prediction)\n",
    "        softmax = tf.nn.softmax(inputs)\n",
    "        \n",
    "        #loss 1. normal loss, predictions vs labels\n",
    "#         normal_loss = self.loss_fn(inputs, labels, sample_weights)\n",
    "#         self.add_loss(normal_loss)\n",
    "        \n",
    "        #loss 2. KL divergence loss, aka the difference between the student and teacher's softmax\n",
    "        if teacher_sm is not None:\n",
    "            kl_loss = self.kl(softmax,teacher_sm)\n",
    "            self.add_loss(kl_loss)\n",
    "            self.add_metric(kl_loss, name=self.name+\"_KL\")\n",
    "        return softmax\n",
    "    \n",
    "\n",
    "class FeatureDistillation(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(FeatureDistillation, self).__init__(name=name)\n",
    "        self.loss_coefficient = 1\n",
    "        self.feature_loss_coefficient = 0.3\n",
    "    def call(self, inputs, teaching_features, sample_weights=None):\n",
    "        #loss 3. Feature distillation of the difference between features of teaching layer and student layer.\n",
    "        l2_loss = self.feature_loss_coefficient * tf.reduce_sum(tf.square(inputs - teaching_features))\n",
    "        self.add_loss(l2_loss)\n",
    "        self.add_metric(l2_loss, name=self.name+\"_distill\") # metric so this loss value can be monitored.\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(num, count =1,classes=10):\n",
    "    output = np.zeros(classes) \n",
    "    pos = max(0,num-1)\n",
    "    output[pos] = 1\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "targets =[]\n",
    "\n",
    "for i in range(30):\n",
    "    target = logit(np.random.randint(0,10))\n",
    "    targets.append(target)\n",
    "targets = np.array(targets)\n",
    "data = {\n",
    "    \"inputs\": np.random.random((30, 3)),\n",
    "    \"targets\": targets,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000001955D816048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7855 - endpoint_loss: 0.7343 - branch1_predictions_loss: 0.7344 - branch1_teaching_distill: 2.5633 - binary_accuracy: 0.0000e+00 - branch1_predictions_KL: 0.0015\n"
     ]
    }
   ],
   "source": [
    "#process for self distilation\n",
    "#add y_true as an input for the model, here called 'targets'. targets is not linked to the main model path\n",
    "#targets is added as input at the model define call\n",
    "#targets is used as an additional input to the endpoint layers\n",
    "#in endpoint layers, perform the loss function using the prev_layer input and the 'targets'\n",
    "\n",
    "#determine if the additional loss is precomputed or computed at the endpoint layer.\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np \n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "x = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "x= layers.Dropout(0.2)(x)\n",
    "\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "x= layers.Dropout(0.2)(x)\n",
    "\n",
    "branch1_256 = keras.layers.Dense(256,activation=\"relu\")(x)\n",
    "# print(branch1_256.shape)\n",
    "\n",
    "\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "x= layers.Dropout(0.2)(x)\n",
    "\n",
    "# branch2_256 = keras.layers.Dense(256,activation=\"relu\")(x)\n",
    "\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "x= layers.Dropout(0.2)(x)\n",
    "\n",
    "teaching_feat = layers.Dense(256, activation=\"relu\")(x)\n",
    "# teacher_feat = featureDistil(x)\n",
    "# x= layers.Dropout(0.2)(teaching_feat)\n",
    "\n",
    "output = layers.Dense(10, name=\"output\")(teaching_feat)\n",
    "\n",
    "# softmax = layers.Softmax()(output)\n",
    "endpoint = LogisticEndpoint(name=\"endpoint\")(output,targets)\n",
    "\n",
    "#rest of branches\n",
    "branch1_teaching = FeatureDistillation(name=\"branch1_teaching\")(branch1_256,teaching_feat)\n",
    "branch1_dense = keras.layers.Dense(10,name=\"branch1_dense\")(branch1_teaching)\n",
    "branch1_predictions = BranchEndpoint(name=\"branch1_predictions\")(branch1_dense, targets, endpoint)\n",
    "\n",
    "\n",
    "# branch2_teaching = FeatureDistillation(name=\"branch2_teaching\")(branch2_256,teaching_feat)\n",
    "# branch2_dense = keras.layers.Dense(10)(branch2_teaching)\n",
    "# branch2_predictions = BranchEndpoint(name=\"branch2_predictions\")(branch2_dense, targets, softmax, [branch2_256], teaching_feat)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=[endpoint,branch1_predictions])\n",
    "model.compile(optimizer=\"adam\", loss =keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "\n",
    "targets =[]\n",
    "\n",
    "for i in range(30):\n",
    "    target = logit(np.random.randint(0,10))\n",
    "    targets.append(target)\n",
    "targets = np.array(targets)\n",
    "data = {\n",
    "    \"inputs\": np.random.random((30, 3)),\n",
    "    \"targets\": targets,\n",
    "}\n",
    "\n",
    "# print(data['targets'])\n",
    "hist = model.fit(data,targets)\n",
    "\n",
    "\n",
    "# The total loss is different from parts_loss because it includes the regularization term.\n",
    "# In other words, loss is computed as loss = parts_loss + k*R, where R is the regularization term \n",
    "# (typically the L1 or L2 norm of the model's weights) and k a hyperparameter that controls the \n",
    "# contribution of the regularization loss in the total loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1s 1s/step - loss: 5.3907 - endpoint_loss: 0.7344 - branch1_predictions_loss: 0.7345 - binary_accuracy: 0.0000e+00\n",
    " 1s 1s/step - loss: 2.9807 - endpoint_loss: 0.7344 - branch1_predictions_loss: 0.7345 - binary_accuracy: 0.0000e+00\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "y = 2 \n",
    "z = 3\n",
    "data = (x,y,z)\n",
    "data\n",
    "a, b, c = data\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"alexnet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 227, 227, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 55, 55, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 27, 27, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 13, 13, 384)       147840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 13, 13, 256)       98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_735 (Dense)            (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout_524 (Dropout)        (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_736 (Dense)            (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_525 (Dropout)        (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_737 (Dense)            (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 56,361,738\n",
      "Trainable params: 56,358,986\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(227,227,3))\n",
    "x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[x], name=\"alexnet\")\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 10 //2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 55, 55, 96)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 227, 227, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
