{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# an updated version of the uncertainty code that uses the different distribution \r\n",
    "# actually using keras models and trying to match the use of custom layers and losses."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#import necessary libraries\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import scipy.ndimage as nd\r\n",
    "from tensorflow import keras\r\n",
    "%matplotlib inline\r\n",
    "import pylab as pl\r\n",
    "from IPython import display\r\n",
    "\r\n",
    "# from tensorflow.examples.tutorials.mnist import input_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Download MNIST dataset\r\n",
    "# mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\r\n",
    "# print(y_train)\r\n",
    "K= 10 # number of classes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# digit_one = mnist.train.images[4].copy()\r\n",
    "# y_train = (y_train.map(tf.keras.to_categorical()))\r\n",
    "# print(type(train_labels[0]))\r\n",
    "# if type(train_labels[0]) is not type(np.ndarray(None,None)):\r\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels,10)\r\n",
    "# print(type(np.ndarray(None,None)))\r\n",
    "# if type(test_labels[0]) is not type(np.ndarray(None,None)):\r\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels,10)\r\n",
    "\r\n",
    "    \r\n",
    "print(train_labels[0])\r\n",
    "print((test_labels[0]))\r\n",
    "# plt.imshow(train_images[1].reshape(228,228)) \r\n",
    "# plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "validation_size = 1000\r\n",
    "shuffle_size = 1000\r\n",
    "batch_size=32\r\n",
    "validation_images, validation_labels = train_images[:validation_size], train_labels[:validation_size] #get the first 5k training samples as validation set\r\n",
    "train_images, train_labels = train_images[validation_size:], train_labels[validation_size:] # now remove the validation set from the training set.\r\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\r\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\r\n",
    "\r\n",
    "\r\n",
    "def augment_images(image, label):\r\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\r\n",
    "    # image = tf.image.per_image_standardization(image)\r\n",
    "    # Resize images from 32x32 to 277x277\r\n",
    "    image = tf.image.resize(image, (227,227))\r\n",
    "    return image, label\r\n",
    "train_ds_size = len(list(train_ds))\r\n",
    "test_ds_size = len(list(test_ds))\r\n",
    "validation_ds_size = len(list(validation_ds))\r\n",
    "train_ds = (train_ds.map(augment_images))\r\n",
    "validation_ds = (validation_ds.map(augment_images))\r\n",
    "test_ds = (test_ds.map(augment_images))\r\n",
    "\r\n",
    "target = tf.data.Dataset.from_tensor_slices((train_labels))\r\n",
    "train_ds = tf.data.Dataset.zip((train_ds,target))\r\n",
    "\r\n",
    "v_target = tf.data.Dataset.from_tensor_slices((validation_labels))\r\n",
    "validation_ds = tf.data.Dataset.zip((validation_ds,v_target))\r\n",
    "\r\n",
    "t_target = tf.data.Dataset.from_tensor_slices((test_labels))\r\n",
    "test_ds = tf.data.Dataset.zip((test_ds,t_target))\r\n",
    "\r\n",
    "\r\n",
    "print(\"trainSize {}\".format(train_ds_size))\r\n",
    "print(\"testSize {}\".format(test_ds_size))\r\n",
    "train_ds = (train_ds\r\n",
    "                \r\n",
    "                .shuffle(buffer_size=tf.cast(shuffle_size,'int64'))\r\n",
    "                .batch(batch_size=batch_size, drop_remainder=True))\r\n",
    "\r\n",
    "test_ds = (test_ds\r\n",
    "               \r\n",
    "                #   .shuffle(buffer_size=train_ds_size)\r\n",
    "                .batch(batch_size=1, drop_remainder=True))\r\n",
    "\r\n",
    "validation_ds = (validation_ds\r\n",
    "               \r\n",
    "                #   .shuffle(buffer_size=validation_ds_size)\r\n",
    "                .batch(batch_size=batch_size, drop_remainder=True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainSize 48000\n",
      "testSize 10000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Create a LeNet network with softmax cross entropy loss function\r\n",
    "def LeNet_softmax(lmb=0.005):\r\n",
    "    inputs = keras.Input(shape=(227,227,3))\r\n",
    "    x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\r\n",
    "    x = keras.layers.BatchNormalization()(x)\r\n",
    "    x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\r\n",
    "    x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\r\n",
    "    x = keras.layers.BatchNormalization()(x)\r\n",
    "    x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\r\n",
    "    x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\r\n",
    "    x = keras.layers.BatchNormalization()(x)\r\n",
    "    x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\r\n",
    "    x = keras.layers.BatchNormalization()(x)\r\n",
    "    x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\r\n",
    "    x = keras.layers.BatchNormalization()(x)\r\n",
    "    x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\r\n",
    "    x = keras.layers.Flatten()(x)\r\n",
    "    x = keras.layers.Dense(4096, activation='relu')(x)\r\n",
    "    x = keras.layers.Dropout(0.5)(x)\r\n",
    "    x = keras.layers.Dense(4096, activation='relu')(x)\r\n",
    "    x = keras.layers.Dropout(0.5)(x)\r\n",
    "    x = keras.layers.Dense(10, activation='softmax')(x)\r\n",
    "    \r\n",
    "    # model = keras.Model(inputs=inputs, outputs=[x,branchLayer,branchLayer2], name=\"alexnet\")\r\n",
    "    model = keras.Model(inputs=inputs, outputs=[x], name=\"alexnet\")\r\n",
    "\r\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\r\n",
    "#     model.summary()\r\n",
    "    return model\r\n",
    "model = LeNet_softmax()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.fit(train_ds,\r\n",
    "          epochs=3,\r\n",
    "          validation_data=validation_ds,\r\n",
    "          validation_freq=1)\r\n",
    "          \r\n",
    "\r\n",
    "\r\n",
    "model.evaluate(test_ds)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "1468/1468 [==============================] - 98s 65ms/step - loss: 0.7524 - accuracy: 0.7364 - val_loss: 0.8411 - val_accuracy: 0.7107\n",
      "Epoch 2/3\n",
      "1468/1468 [==============================] - 105s 71ms/step - loss: 0.6783 - accuracy: 0.7624 - val_loss: 0.6536 - val_accuracy: 0.7792\n",
      "Epoch 3/3\n",
      "1468/1468 [==============================] - 92s 62ms/step - loss: 0.6053 - accuracy: 0.7883 - val_loss: 0.6515 - val_accuracy: 0.7893\n",
      "312/312 [==============================] - 5s 15ms/step - loss: 0.6853 - accuracy: 0.7647\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.6852789521217346, 0.764723539352417]"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# This function to generate evidence is used for the first example\r\n",
    "def relu_evidence(logits):\r\n",
    "    return tf.nn.relu(logits)\r\n",
    "\r\n",
    "# This one usually works better and used for the second and third examples\r\n",
    "# For general settings and different datasets, you may try this one first\r\n",
    "def exp_evidence(logits): \r\n",
    "    return tf.exp(tf.clip_by_value(logits,-10,10))\r\n",
    "\r\n",
    "# This one is another alternative and \r\n",
    "# usually behaves better than the relu_evidence \r\n",
    "def softplus_evidence(logits):\r\n",
    "    return tf.nn.softplus(logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def KL(alpha):\r\n",
    "    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\r\n",
    "    S_alpha = tf.reduce_sum(alpha,axis=1,keepdims=True)\r\n",
    "    S_beta = tf.reduce_sum(beta,axis=1,keepdims=True)\r\n",
    "    lnB = tf.compat.v1.lgamma(S_alpha) - tf.reduce_sum(tf.compat.v1.lgamma(alpha),axis=1,keepdims=True)\r\n",
    "    lnB_uni = tf.reduce_sum(tf.compat.v1.lgamma(beta),axis=1,keepdims=True) - tf.compat.v1.lgamma(S_beta)\r\n",
    "    \r\n",
    "    dg0 = tf.compat.v1.digamma(S_alpha)\r\n",
    "    dg1 = tf.compat.v1.digamma(alpha)\r\n",
    "    \r\n",
    "    kl = tf.reduce_sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\r\n",
    "    return kl\r\n",
    "\r\n",
    "def mse_loss(p, alpha, global_step, annealing_step): \r\n",
    "    S = tf.reduce_sum(alpha, axis=1, keepdims=True) \r\n",
    "    E = alpha - 1\r\n",
    "    m = alpha / S\r\n",
    "    \r\n",
    "    A = tf.reduce_sum((p-m)**2, axis=1, keepdims=True) \r\n",
    "    B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True) \r\n",
    "\r\n",
    "    annealing_coef = tf.minimum(1.0,tf.cast(global_step/annealing_step,tf.float32))\r\n",
    "    \r\n",
    "    alp = E*(1-p) + 1 \r\n",
    "    C =  annealing_coef * KL(alp)\r\n",
    "    return (A + B) + C\r\n",
    "\r\n",
    "def loss_function():\r\n",
    "    #create a wrapper function that returns a function\r\n",
    "    kl = tf.keras.losses.KLDivergence()\r\n",
    "    temperature = 1\r\n",
    "    Classes = 10\r\n",
    "    crossE = tf.keras.losses.CategoricalCrossentropy()\r\n",
    "    mse =mse_loss\r\n",
    "    global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\r\n",
    "    annealing_step = 320\r\n",
    "    def EMSE_Loss(y_true, y_pred):\r\n",
    "#         softmax = tf.nn.softmax(y_pred)\r\n",
    "        evidence = exp_evidence(y_pred)\r\n",
    "        alpha  = evidence + 1\r\n",
    "        \r\n",
    "        u = Classes / tf.reduce_sum(alpha, axis=1) #uncertainty\r\n",
    "        \r\n",
    "        prob = alpha/tf.reduce_sum(alpha, 1,keepdims=True) \r\n",
    "        \r\n",
    "        loss = tf.reduce_mean(mse(y_true, alpha,global_step,annealing_step))\r\n",
    "#         l2_loss = (tf.nn.l2_loss(W3)+tf.nn.l2_loss(W4)) * lmb\r\n",
    "#         l2_loss = tf.reduce_sum(tf.square(inputs - teaching_features))\r\n",
    "        \r\n",
    "        final_loss = loss #+ l2_loss\r\n",
    "        \r\n",
    "#         kl_loss = kl( tf.nn.softmax(softmax / self.temperature, axis = 1 ),\r\n",
    "        \r\n",
    "        return evidence, final_loss\r\n",
    "    return  EMSE_Loss\r\n",
    "\r\n",
    "def loss_function2():\r\n",
    "#create a wrapper function that returns a function\r\n",
    "    kl = tf.keras.losses.KLDivergence()\r\n",
    "    temperature = 1\r\n",
    "    Classes = 10\r\n",
    "    crossE = tf.keras.losses.CategoricalCrossentropy()\r\n",
    "    mse =tf.keras.losses.MeanSquaredError()\r\n",
    "    def EMSE_Loss(y_true, y_pred):\r\n",
    "#         softmax = tf.nn.softmax(y_pred)\r\n",
    "        evidence = exp_evidence(y_pred)\r\n",
    "        alpha  = evidence + 1\r\n",
    "        u = Classes / tf.reduce_sum(alpha, axis=1) #uncertainty\r\n",
    "        \r\n",
    "        prob = alpha/tf.reduce_sum(alpha, 1,keepdims=True) \r\n",
    "        \r\n",
    "        loss = tf.reduce_mean(mse(y_true, alpha))\r\n",
    "#         l2_loss = (tf.nn.l2_loss(W3)+tf.nn.l2_loss(W4)) * lmb\r\n",
    "#         l2_loss = tf.reduce_sum(tf.square(inputs - teaching_features))\r\n",
    "        \r\n",
    "        final_loss = loss #+ l2_loss\r\n",
    "        \r\n",
    "#         kl_loss = kl( tf.nn.softmax(softmax / self.temperature, axis = 1 ),\r\n",
    "        \r\n",
    "        return final_loss\r\n",
    "    return  EMSE_Loss\r\n",
    "    \r\n",
    "    # return entropyAddition\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "class EvidenceEndpoint(keras.layers.Layer):\r\n",
    "        def __init__(self, num_outputs, name=None, **kwargs):\r\n",
    "            super(EvidenceEndpoint, self).__init__(name=name)\r\n",
    "            self.num_outputs = num_outputs\r\n",
    "            self.loss_coefficient = 1\r\n",
    "            self.feature_loss_coefficient = 1\r\n",
    "            self.kl = tf.keras.losses.KLDivergence()\r\n",
    "            self.loss_fn = loss_function()\r\n",
    "            self.temperature = 10\r\n",
    "            self.alpha = .1\r\n",
    "            self.lmb = 0.005\r\n",
    "        def build(self, input_shape):\r\n",
    "            self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]),\r\n",
    "                                                    self.num_outputs])\r\n",
    "\r\n",
    "        def call(self, inputs, labels):\r\n",
    "            outputs = tf.matmul(inputs,self.kernel)\r\n",
    "            # Compute the training-time loss value and add it\r\n",
    "            # to the layer using `self.add_loss()`.\r\n",
    "            #loss functions are (True, Prediction)\r\n",
    "            # softmax = tf.nn.softmax(inputs)\r\n",
    "            # print(\"inputs\",input)\r\n",
    "            #loss 1. normal loss, predictions vs labels\r\n",
    "            evidence, normal_loss = self.loss_fn(labels, outputs)\r\n",
    "            l2_loss = tf.nn.l2_loss(self.weights) * self.lmb\r\n",
    "            # print(evidence.shape)\r\n",
    "            # print(normal_loss)\r\n",
    "            total_loss =tf.reduce_mean(normal_loss) + l2_loss\r\n",
    "            # print(total_loss)\r\n",
    "            total_evidence = tf.reduce_sum(evidence,1, keepdims=True) \r\n",
    "            # print(total_evidence)\r\n",
    "            pred = tf.argmax(outputs, 1)\r\n",
    "            truth = tf.argmax(labels, 1)\r\n",
    "            match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\r\n",
    "            # print(\"match\",match)\r\n",
    "            mean_avg = tf.reduce_mean(total_evidence)\r\n",
    "            # print(\"mean_Avg\")\r\n",
    "            \r\n",
    "            mean_succ = tf.reduce_sum(tf.reduce_sum(evidence,1, keepdims=True)*match) / tf.reduce_sum(match+1e-20)\r\n",
    "            # print(\"mean_fail\")\r\n",
    "            # side_x = tf.reduce_sum(tf.reduce_sum(evidence,1, keepdims=True)*(1-match))\r\n",
    "            # print(\"side_x\",side_x)\r\n",
    "            # side_y = (tf.reduce_sum(tf.abs(1-match))+1e-20)\r\n",
    "            # print(\"side_y\",side_y)\r\n",
    "            # div = tf.reduce_sum(side_x/side_y)\r\n",
    "            # print(\"div\",div)\r\n",
    "            mean_fail = tf.reduce_sum(tf.reduce_sum(tf.reduce_sum(evidence,1, keepdims=True)*(1-match)) / (tf.reduce_sum(tf.abs(1-match))+1e-20) )\r\n",
    "            # print('end')\r\n",
    "            self.add_metric(normal_loss, name=\"normal_loss\")\r\n",
    "            self.add_metric(evidence, name=\"evidence\")\r\n",
    "            self.add_metric(mean_avg, name=\"mean_ev_avg\")\r\n",
    "            self.add_metric(mean_succ, name=\"mean_ev_succ\")\r\n",
    "            self.add_metric(mean_fail, name=\"mean_ev_fail\")\r\n",
    "            # print('metrics')\r\n",
    "            self.add_loss(total_loss)\r\n",
    "            # print(\"pred\",inputs)\r\n",
    "            # print(\"loss\")\r\n",
    "            #NOTE\r\n",
    "            # The total loss is different from parts_loss because it includes the regularization term.\r\n",
    "            # In other words, loss is computed as loss = parts_loss + k*R, where R is the regularization term \r\n",
    "            # (typically the L1 or L2 norm of the model's weights) and k a hyperparameter that controls the \r\n",
    "            # contribution of the regularization loss in the total loss.\r\n",
    "            return outputs\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Create a LeNet network with softmax cross entropy loss function\r\n",
    "def LeNet_softmax2(lmb=0.005):\r\n",
    "    inputs = keras.Input(shape=(227,227,3))\r\n",
    "    targets = keras.Input(shape=(10,),name=\"targets\")\r\n",
    "    x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\r\n",
    "    x = keras.layers.BatchNormalization()(x)\r\n",
    "    x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\r\n",
    "    x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\r\n",
    "    x = keras.layers.BatchNormalization()(x)\r\n",
    "    x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\r\n",
    "    x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\r\n",
    "    x = keras.layers.BatchNormalization()(x)\r\n",
    "    x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\r\n",
    "    x = keras.layers.BatchNormalization()(x)\r\n",
    "    x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\r\n",
    "    x = keras.layers.BatchNormalization()(x)\r\n",
    "    x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\r\n",
    "    x = keras.layers.Flatten()(x)\r\n",
    "    x = keras.layers.Dense(4096, activation='relu')(x)\r\n",
    "    x = keras.layers.Dropout(0.5)(x)\r\n",
    "    x = keras.layers.Dense(4096, activation='relu')(x)\r\n",
    "    x = keras.layers.Dropout(0.5)(x)\r\n",
    "    # x = keras.layers.Dense(10, activation='relu')(x)\r\n",
    "    x = EvidenceEndpoint(10, activation='relu')(x,targets)\r\n",
    "    \r\n",
    "    # model = keras.Model(inputs=inputs, outputs=[x,branchLayer,branchLayer2], name=\"alexnet\")\r\n",
    "    model = keras.Model(inputs=[inputs,targets], outputs=[x], name=\"alexnet\")\r\n",
    "    EMSE_loss = loss_function2()\r\n",
    "    model.compile(  optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'],run_eagerly=True)\r\n",
    "#     model.summary()\r\n",
    "    return model\r\n",
    "model = LeNet_softmax2()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "model.fit(train_ds,\r\n",
    "          epochs=3,\r\n",
    "          validation_data=validation_ds,\r\n",
    "          validation_freq=1)\r\n",
    "          \r\n",
    "\r\n",
    "\r\n",
    "model.evaluate(test_ds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "1531/1531 [==============================] - 133s 86ms/step - loss: 1.0175 - accuracy: 0.1851 - normal_loss: 0.9203 - evidence: 10.1863 - mean_ev_avg: 101.8628 - mean_ev_succ: 258.2596 - mean_ev_fail: 40.1431 - val_loss: 0.8327 - val_accuracy: 0.3952 - val_normal_loss: 0.7907 - val_evidence: 17.4582 - val_mean_ev_avg: 174.5822 - val_mean_ev_succ: 405.9762 - val_mean_ev_fail: 22.1095\n",
      "Epoch 2/3\n",
      "1531/1531 [==============================] - 132s 86ms/step - loss: 0.8319 - accuracy: 0.4065 - normal_loss: 0.7558 - evidence: 36.8438 - mean_ev_avg: 368.4377 - mean_ev_succ: 718.4234 - mean_ev_fail: 103.4453 - val_loss: 0.6825 - val_accuracy: 0.5171 - val_normal_loss: 0.6477 - val_evidence: 39.5341 - val_mean_ev_avg: 395.3411 - val_mean_ev_succ: 689.6007 - val_mean_ev_fail: 79.8005\n",
      "Epoch 3/3\n",
      "1531/1531 [==============================] - 126s 82ms/step - loss: 0.6915 - accuracy: 0.5033 - normal_loss: 0.6326 - evidence: 105.2324 - mean_ev_avg: 1052.3241 - mean_ev_succ: 1716.5170 - mean_ev_fail: 320.8549 - val_loss: 0.5659 - val_accuracy: 0.5978 - val_normal_loss: 0.5362 - val_evidence: 104.6210 - val_mean_ev_avg: 1046.2098 - val_mean_ev_succ: 1533.3979 - val_mean_ev_fail: 238.9326\n",
      "312/312 [==============================] - 14s 43ms/step - loss: 0.5874 - accuracy: 0.5814 - normal_loss: 0.5577 - evidence: 96.1497 - mean_ev_avg: 961.4969 - mean_ev_succ: 1445.7648 - mean_ev_fail: 277.3134\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.5874406099319458,\n",
       " 0.5814303159713745,\n",
       " 0.5577133297920227,\n",
       " 96.14968872070312,\n",
       " 961.4968872070312,\n",
       " 1445.7647705078125,\n",
       " 277.3133544921875]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "model.evaluate(test_ds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  848/10000 [=>............................] - ETA: 3:27 - loss: 0.6040 - accuracy: 0.5625 - normal_loss: 0.5742 - evidence: 91.3220 - mean_ev_avg: 913.2200 - mean_ev_succ: 800.7679 - mean_ev_fail: 112.4521"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-cf7953a05327>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1387\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1389\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1390\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m   1231\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m         \u001b[1;34m\"\"\"Runs an evaluation execution with one step.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mstep_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1224\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1225\u001b[0m       outputs = reduce_per_replica(\n\u001b[0;32m   1226\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1257\u001b[0m       fn = autograph.tf_convert(\n\u001b[0;32m   1258\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m-> 1259\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2729\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2730\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2732\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3415\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3416\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3417\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3419\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;31m# Ensure counter is updated only if `test_step` succeeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_x_y_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m     \u001b[1;31m# Updates stateful loss metrics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     self.compiled_loss(\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \"\"\"\n\u001b[0;32m    424\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m--> 425\u001b[1;33m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m           outputs = nn.bias_add(\n\u001b[1;32m--> 267\u001b[1;33m               outputs, self.bias, data_format=self._tf_data_format)\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m   3377\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3378\u001b[0m       return gen_nn_ops.bias_add(\n\u001b[1;32m-> 3379\u001b[1;33m           value, bias, data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m   3380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m--> 674\u001b[1;33m         _ctx, \"BiasAdd\", name, value, bias, \"data_format\", data_format)\n\u001b[0m\u001b[0;32m    675\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def draw_EDL_results(train_acc1, train_ev_s, train_ev_f, test_acc1, test_ev_s, test_ev_f): \r\n",
    "    # calculate uncertainty for training and testing data for correctly and misclassified samples\r\n",
    "    train_u_succ = K / (K+np.array(train_ev_s))\r\n",
    "    train_u_fail = K / (K+np.array(train_ev_f))\r\n",
    "    test_u_succ  = K / (K+np.array(test_ev_s))\r\n",
    "    test_u_fail  = K / (K+np.array(test_ev_f))\r\n",
    "    \r\n",
    "    f, axs = pl.subplots(2, 2)\r\n",
    "    f.set_size_inches([10,10])\r\n",
    "    \r\n",
    "    axs[0,0].plot(train_ev_s,c='r',marker='+')\r\n",
    "    axs[0,0].plot(train_ev_f,c='k',marker='x')\r\n",
    "    axs[0,0].set_title('Train Data')\r\n",
    "    axs[0,0].set_xlabel('Epoch')\r\n",
    "    axs[0,0].set_ylabel('Estimated total evidence for classification') \r\n",
    "    axs[0,0].legend(['Correct Clasifications','Misclasifications'])\r\n",
    "    \r\n",
    "    \r\n",
    "    axs[0,1].plot(train_u_succ,c='r',marker='+')\r\n",
    "    axs[0,1].plot(train_u_fail,c='k',marker='x')\r\n",
    "    axs[0,1].plot(train_acc1,c='blue',marker='*')\r\n",
    "    axs[0,1].set_title('Train Data')\r\n",
    "    axs[0,1].set_xlabel('Epoch')\r\n",
    "    axs[0,1].set_ylabel('Estimated uncertainty for classification')\r\n",
    "    axs[0,1].legend(['Correct clasifications','Misclasifications', 'Accuracy'])\r\n",
    "    \r\n",
    "    axs[1,0].plot(test_ev_s,c='r',marker='+')\r\n",
    "    axs[1,0].plot(test_ev_f,c='k',marker='x')\r\n",
    "    axs[1,0].set_title('Test Data')\r\n",
    "    axs[1,0].set_xlabel('Epoch')\r\n",
    "    axs[1,0].set_ylabel('Estimated total evidence for classification') \r\n",
    "    axs[1,0].legend(['Correct Clasifications','Misclasifications'])\r\n",
    "    \r\n",
    "    \r\n",
    "    axs[1,1].plot(test_u_succ,c='r',marker='+')\r\n",
    "    axs[1,1].plot(test_u_fail,c='k',marker='x')\r\n",
    "    axs[1,1].plot(test_acc1,c='blue',marker='*')\r\n",
    "    axs[1,1].set_title('Test Data')\r\n",
    "    axs[1,1].set_xlabel('Epoch')\r\n",
    "    axs[1,1].set_ylabel('Estimated uncertainty for classification')\r\n",
    "    axs[1,1].legend(['Correct clasifications','Misclasifications', 'Accuracy'])\r\n",
    "    \r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('py36': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "interpreter": {
   "hash": "7483e188a462fd4248cd8d23b24bc727a5fe7a35e4044aa57ebcc92f8fe9e445"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}